The adage "the cloud is just someone else's computer" is true: that someone else is just insanely good at actually running that computer
With the advent of high-value services, run in a way that's beyond what any company can manage on its down, this image is rapidly changing
In the end, everything runs on a computer that has storage and networking
On top of this, however, we see several types of high-value services being built
We start out with those services that your IT department already considers as separate: storage, databases, load balancing, API gateways, the like
These are the things that show up in your network or infrastructure diagram
Then we have application-level services, such as machine learning, queueing services, identity management, or stream processing
These things usually show up in software architecture diagrams, and are typically embedded in some software stack you may have adopted
Using these services, the cloud providers have built offerings for business horizontals: mobile application support, internet of things, gaming backends
These serve as blueprints for well-thought-out solutions, working as building blocks that benefit from scale and experience
In traditional development, these would be represented by architecture patterns for typical solutions, or make-or-buy decisions for horizontal-specific components
Perpendicular to these, we have maturity services for monitoring and maintaining your applications
Do what you do best… and gain a few skills Each of these services is, in its own right, something that requires significant investment and skill to operate at the price point and quality that they’re offered
Also, they take some element of application construction, neatly define its boundaries (way neater than your own in-house architecture team would ever have the chance to), and build and run a service better, cheaper, and more reliable than you ever could–that is, if you’re willing to play by the rules
First rule is on functionality: you may have to re-architect your application to work with one of these offerings, and there may be key parts of functionality that are simply missing
Also, by picking these high value services, you are likely no longer “cloud agnostic”: the architecture of your solution will be highly influenced by the available services
Thirdly, consider separation of concerns: the cloud provider provides incredibly powerful building blocks, and it’s your job to compose them into something that’s valuable for your customer–if you’re just reselling the cloud provider’s service, there’s no use for you to be in the value chain
It’s now up to you to determine whether the opportunity cost of cloud lock-in outweighs the additional value provided by going all in with one provider
Cloud native ain’t your father’s cloud This new cloud status quo is no longer a good match for the existing IT skills that come from traditional deployment
As IT departments, we need to start thinking in heterogeneous vertical building blocks in stead of horizontal, well-known services, and get used to more complicated price models
On the development side, we’ll need to suppress the urge to start building software right away, and update our architecture process find a good balance between high value, off-the-shelf services and custom development
Finally, as consultancies, we must develop a stronger sense of how to compose solutions using these tools, and take a more central role in the solution design process, not only in the development or operations of solutions
Cloud native ain’t your father’s cloud
Act accordinglyFor years, performing HTTP calls from Java meant you would use an external dependency
That has changed with Java 11, where an all-new HttpClient API is introduced to the platform
When doing HTTP calls from Java code, you typically use an HTTP client library
Examples are the Apache HttpClient from the HttpComponents project, Square’s OkHttp or even more high-level libraries like the JAX-RS Client API
Even though these libraries are mostly fine, it does mean taking yet another dependency in your application
Can’t Java offer a basic functionality like making an HTTP request with its standard library
It turns out you can do so using the HttpURLConnection class, which is right there in the java.net package
However, this API (which was introduced in JDK 1.1 around 1997) is no joy to use, to say the least
Many have lamented its deficiencies
Remember, the API was created before generics, enums, lambdas, and other modern features were in the Java language
I think you can see the problem
Fortunately a new HttpClient API has been introduced in Java 11 (released September 2018)
It’s much more modern, and also supports HTTP/2 and WebSocket communication
So what does it look like
Here’s the simplest possible scenario, of performing a simple GET request: java HttpClient client = HttpClient.newHttpClient(); HttpRequest request = HttpRequest.newBuilder(URI.create("https://luminis.eu")) .build(); HttpResponse<String> response = client.send(request, HttpResponse.BodyHandlers.ofString()); After creating an HttpClient instance, we create an HttpRequest (using the builder API) and pass it to the the client’s send method
Couldn’t be simpler! In addition to the request, we must also tell the send method how to interpret the response body sent by the server
For this, several pre-defined BodyHandlers are available
Here we simple turn the server’s response into a String
The above example shows an HttpClient and request configured with their defaults
On both the client and the request builder APIs there are many possible configuration options, for example for timeouts and HTTP redirect policies
Once a client or request is built, it is immutable
These objects are thread-safe and can be safely shared throughout an application
In this example, a synchronous (and blocking) call is performed to the HTTP server
Once the response has been completed, it’s turned into a String and wrapped in an HttpResponse object
On this response we can check things like the HTTP status code, and of course get retrieve the body with the body() method
You can also perform asynchronous, non-blocking calls with the new HttpClient API: java HttpClient client = HttpClient.newHttpClient(); HttpRequest request = HttpRequest.newBuilder(URI.create("https://luminis.eu")) .build(); CompletableFuture<HttpResponse<String>> response = client.sendAsync(request, HttpResponse.BodyHandlers.ofString()); response.thenApply(r -> System.out.println(r.body())); The sendAsync method returns a CompletableFuture immediately
This CompletableFuture represents the response that will be returned at a later time by the server
The rest of the program keeps running, and the HTTP request is performed on a separate thread
Once the server returns a response, the CompletableFuture is completed
You typically use the result through methods like thenApply and other combinators that let you asynchronously work with the result
HttpClient is not just an updated version of the old HttpURLConnection API
It also supports newer standards, like HTTP/2 and WebSockets
It’s a much need upgrade to the HTTP functionalities that ship with the JDK
Although this post only scratches the surface, it should give you enough to get started and try it out yourself!I’m a designer of digital products
The past 20 years my work was primarily focussed on GUI - Graphical User Interfaces
Until now
With the rise of voice-controlled digital assistents from Amazon, Google and the others, a new way of communicating with digital products has become available: the VUI - the Voice User Interface
We all know how to create conversations And now I’m confused
I still like to think that designers like me should be the ones that decide how a VUI should behave; what it should say, and when
But are we even equipped for designing for voice
As it doesn’t involve any visual elements, it seems to boil down to designing pure conversations
And why would you need a designer for that
We all know how to create conversations, everybody has them all the time
So, what does it take to design a good VUI
Looking for anwers, I first stumbled on some dear memories from my own youth
The internet’s predecessor Many years ago, yet not so very long ago, people used telephones solely for talking to each other over distance
No apps, no displays, just that single simple purpose
You dialed a number and got connected to the person on the other side
And so you could reach virtually any person, anytime, from anywhere
To chat, share experiences, to place an order or to get information; The telephone was in many ways the internet’s predecessor
My grandmother worked at 008, which was not, as you might guess from the name, in any way related to the British Secret Intelligence Service
No, 008 once was the number you dialed in the Netherlands when you needed “information”
By information the Dutch telecom provider basically meant telephone numbers; you called this service when you needed to contact someone and didn’t have the number or address
There were other services too: you dialed 002 for the time, 003 for the weather forecast and so you dialed 008 to get someones phone number
These phone services were one of the very first to introduce Voice User Interfaces, or ‘talking computers’ as people usually called them those days
Before that, 008 employed human operators for the task of looking up the numbers
My grandmother was one of them
In her first years as an operator she needed to look up the requested numbers in phone books; the frequently requested numbers she knew by heart
In 1979 computers were introduced at 008 and from that time on the human operator used computers to retrieve the requested information
A human operator as a proxy for the computer system
My grandmother used to tell me stories about people, mostly men, calling for other purposes than getting a telephone number
I remember one story about a man who had called and asked her for the time
My grandmother kindly told the man he had probably dialed the wrong number, that 002 was the number to get the time
The man had replied he knew what time it was, but that he wanted to ask at what time my grandmother would be off from work, and if she would care to join him for diner
My grandmother proudly told us that these male callers always kept inquiring about her age, because over the phone she apparently sounded like a twenty year old girl
While in fact she was in her late fifties, she had always kept them in the dark and played hard to get, just for the fun of sharing her adventures with her colleagues over lunch
And, years later, with her own grandchildren
The 008 ladies finally got replaced by voice-controlled computers in the late nineties
Computers that we could call and talk to, that got us information faster and cheaper than my grandmother could
As long as the caller kept to the script and didn’t ask for anything out of the ordinary, it was great technology
But I’ll tell you these computers were a lousy flirt compared to my grandmother
Disfunctional relationships It’s been about 20 years since computers took over from the 008 ladies and you would think voice recognition technology would have improved dramatically by now… Let’s look at this typical conversation between me and my car: Does any of this sound familiar to you
Or maybe I should just get a new car
But no, I’ve got a similar disfunctional relationship with Siri, Apple’s smart voice assistant
But I don’t blame Siri
It is still a kid
I don’t blame her just the way I don’t blame my own kids if they misinterpret my words or behave in a way that is not quite socially accepted
Siri, just like my kids, needs to learn by making mistakes
Human conversation appears to be so simple, but if you take a moment to think about it you’ll see that it is absurdly complex
Siri is still a kid I do blame my car by the way
Because my car is basically not so very different from the voice-controlled computer of 008 from the late nineties
It only knows a very limited set of commands and asks for very specific input at specific points during the interaction
That is no conversation at all
Siri on the other hand listens to everything we say and tries its best to interpret our intention
That, I think, is a remarkable step forward in techology
Even if it still often fails to grasp our true intention, it is unbelievable what it is already capable off
Human communication is complex because our language is so very ambiguous
Words have different meanings in different contexts, what we say literally is often not the same as what we mean
Humans are complex beings
We have tempers, become emotional, are always seeking self-confirmation
Even with the advanced state of technology of today, we are still a long way from real human-like conversation with computers
Watch this fragment from the movie HER
Now that is a real conversation! It is interesting to see two types of VUI in this fragment: the first is the voice that guides Theodore through the setup of the OS, which is not so very different from the way we are used to interact with computers today
After initialisation this other VUI turns up: the very human-like and instantly intriguing Samantha
Until we have reached that level of conversational and emotional skill in voice user interfaces, let us restrict from using the term ‘conversational interface’
Today we are still in the infancy of the VUI, this is still the era of voice commands
Still, these VUIs need to be designed well in order for us humans to be able to interact with them confidently
This is the era of voice commands So I’m back to my original quest of seeking what it takes to design a good VUI
I have described what I think we can expect from a VUI today, how that is very different from 20 years ago, and in what direction we might expect this technology to evolve
But that doesn’t answer my question
So, in order to find out what skills you need to create a good VUI today, I will just need to create one myself
I have found a use case that seems perfectly fit for this: booking a meeting room at our office
I will call it Bookaroo (just add an m and you’ll see why)
I will start with a rudimentary prototype that I’m planning to install in one of our meeting rooms to test and evaluate
In the next couple of months I will keep you updated on Bookaroo’s progress and my learnings about designing a VUI by posting these blogs
Hope you will like it!I recently built a Cordova application with Angular
The application periodically calls a server to get information
One of the requirements was that the client server communication had to be over SSL
I didn’t know much about the SSL protocol but I thought it should be a piece of cake
With most hosting providers it is done in just a few clicks
But this case is slightly different
The application must function without internet connection
the app communicates with an on site server in a local network by IP address
In this post I will cover what the main reasons are to use SSL, what problems arise when you want to use SSL in a local network and how I fixed it for a Cordova application
Everything that is described in this post will also work (even better) for native applications
Why we want to use SSL The two reasons why we want the app to communicate with the server over SSL are Encryption and Identification
Encryption Encryption is basically hiding what is sent from one computer to another
If a you don’t use SSL, the data will be transmitted as is and any computer on any of the networks between the client and the server can intercept and read the data
The SSL protocol puts a barrier around the communication
All the data that is sent from the client to the server and vice versa will first be changed before it gets transmitted
The data can still be intercepted by any computer on the network but they can’t read the data
The data is encrypted
Identification There can be a secure connection, but that does not necessarily mean that the client can trust the computer that is on the other end
To validate the identity of a server the SSL Protocol uses a certificate
In the certificate are details of the receiver/owner
To make the certificate reliable, there is a whole process behind obtaining a certificate
The organization has to communicate with another organization called a Certificate Authority (CA)
The organization asks a certificate for a specific domain, the CA will check domain ownership and look up the details of the organization
For some versions the CA will check more information of the business/organization but domain ownership is always mandatory
If the CA approves the request, they create a certificate for the organization and sign it with their private key
The certificate can than be installed on the server to which the domain points to
If a client connects through HTTPS to the server, the server sends its public key with its SSL certificate to the client
Once the client gets the certificate it will check the signature to make sure it’s valid
This is done by e.g
checking if it is signed by a trusted CA and the current hostname matches the domain in the certificate
If all checks succeed, a secure connection will be established
If not, (depending on the client) the request will fail
Problems To get a valid certificate we need to pass a CA
A certificate belongs to a domain and the CA checks the domain ownership
In a local network we don’t have a domain and we communicate to the server by IP address
Because of this, the domain ownership check cannot take place
A CA will never issue a certificate for a local IP address
But can’t we just use the certificate of the domain for our organization
We have a valid certificate
We can just installed that one on the server… Fortunately it is not that easy
When setting up the SSL connection, the client also check if the domain matches the domain on the certificate this is called ‘Hostname verification’
We communicate by an IP address, this does not match the domain and the connection fails
Our solution In order for the hostname verification to succeed, our certificate must contain the IP address
We can’t count on a CA so we have to make a certificate ourselves
This is possible and is called a ‘Self signed certificate’
This is a certificate that is not validated and signed by a CA, but by the organization itself
The IP address can be added by adding it as a ‘subjectAltName’ to the certificate
That way the hostname verification will succeed Earlier we said that the validation of a CA is used in the SSL checks
If the certificate is not signed by a CA that is known by the client, the check will fail
To make sure the client trusts our self signed certificate we need to pin the certificate in the application
In Cordova certificate pinning is not possible
The server connections are handled by the webview
Therefore it is not possible to intercept SSL connections to perform the check of the server’s certificate
True certificate pinning is only possible in native code
That means that every request must happens through native code instead of traditional XHR/AJAX requests
To make all this work, we used a cordova plugin to pin the certificate and made sure the application makes all requests using this plugin
For convenience we created an Angular httpClient interceptor that stops the normal requests and delegates it to the pluginFrom one of our customers, we got a question about using compound words in queries
Compound words are essential in languages like Dutch and German
Some examples in Dutch are zomervakantie, kookboek and koffiekopje
When a user enters koffiekopje, we want to find documents containing koffie kop as well as koffiekop and of course koffiekopje
In Elasticsearch, and other Lucene based search engines, you can use the Compound Word Token Filter to accomplish this
There are two different versions, the Hyphenation decompounder, and the Dictionary decompounder
Reading the documentation, you’ll learn that you should always use the Hyphenation one
The hyphenation breaks up the terms in their hyphens
By combining adjacent hyphens, we create terms and check them with a provided dictionary
With a match, the term gets added to the query, just like a synonym
An example of hyphenation is: Koffiekopje -> kof – fie – kop – je These hyphens would potentially result in the terms: koffie, koffiekop, kop, kopje, koffiekopje
As with other things with the Dutch language, I was a bit skeptic about the results of the filter
Therefore I decided to have a look at the implementation and try it out before I started using it for real
I had to create a class and do some sub-classing to get access to protected parameters and methods
But I was able to distill the mechanism as used by the filter and the appropriate Lucene classes
The first step is the hyphenation part
For this part, you use the Lucene class HyphenationTree
The following piece of code shows the construction of the hyphenation tree using the mentioned XML filefrom the Objects For Formatting Objects project
code public TryHyphenation(String hyphenRulesPath) { HyphenationTree tree = new HyphenationTree(); try (InputStream hyphenRules = new FileSystemResource(hyphenRulesPath).getInputStream()) { InputSource source = new InputSource(hyphenRules); tree.loadPatterns(source); } catch (IOException e) { LOGGER.error("Problem while loading the hyphen file", e); } this.tree = tree; } The constructor gives us access to the HyphenationTree containing the rules
Now we can ask for the hyphens of a string we can choose our selves
The result is an array with numbers
Each number represents the start of a new hyphen
The following code block finds a list of strings containing the found hyphens
Printing the hyphens is just a matter of joining the strings with a separator
code public List<String> hyphenate(String sourceString) { Hyphenation hyphenator = this.tree.hyphenate(sourceString, 1, 1); int[] hyphenationPoints = hyphenator.getHyphenationPoints(); List<String> parts = new ArrayList<>(); for (int i = 1; i < hyphenationPoints.length; i++) { parts.add(sourceString.substring(hyphenationPoints[i-1], hyphenationPoints[i])); } return parts; } TryHyphenation hyphenation = new TryHyphenation(HYPHEN_CONFIG); String sourceString = "Koffiekopje"; System.out.println("*** Find Hyphens:"); List<String> hyphens = hyphenation.hyphenate(sourceString); String joinedHyphens = StringUtils.arrayToDelimitedString( hyphens.toArray(), " - "); System.out.println(joinedHyphens); Running the code results in the following hyphens (or output)
code *** Find Hyphens: Kof - fie - kop - je Next step is finding the terms we want to search for based on the provided compound word
The elasticsearch analyzer uses the Lucene class HyphenationCompoundWordTokenFilter to find terms out of compound words
We can use this class in our sample code as well; we have to extend it to get access to the protected tokens variable
Therefore we created this following sub-class
code private class AccessibleHyphenationCompoundWordTokenFilter extends HyphenationCompoundWordTokenFilter { public AccessibleHyphenationCompoundWordTokenFilter(TokenStream input, HyphenationTree hyphenator, CharArraySet dictionary) { super(input, hyphenator, dictionary); } public List<String> getTokens() { return tokens.stream().map(compoundToken -> compoundToken.txt.toString()) .collect(Collectors.toList()); } } With the following code, we can find the tokens that are available in our dictionary that are equal to found hyphens or combinations of hyphens
This class is not meant for our way of using it
Therefore the code looks a bit weird
But it does help us to understand what happens
We need a tokenizer; we use the Standard tokenizer from Lucene
We also need a reader with access to the string that needs to be tokenized
Next, we create the CharSetArray containing our dictionary of terms to find
With the HyphenationTree, the tokenizer and the dictionary we create the AccessibleHyphenationCompoundWordTokenFilter
After calling the internal methods of the filter, we can call our method with access to the internal variable tokens
code public static final List<string> DICTIONARY = Arrays.asList("koffie","kop", "kopje"); public List<String> findTokens(String sourceString) { StandardTokenizer tokenizer = new StandardTokenizer(); tokenizer.setReader(new StringReader(sourceString)); CharArraySet charArraySet = new CharArraySet(DICTIONARY, true); AccessibleHyphenationCompoundWordTokenFilter filter = new AccessibleHyphenationCompoundWordTokenFilter(tokenizer, tree, charArraySet); try { filter.reset(); filter.incrementToken(); filter.close(); } catch (IOException e) { LOGGER.error("Could not tokenize", e); } return filter.getTokens(); } Now we have the terms from the compound word that is also in our dictionary
code System.out.println("\n*** Find Tokens:"); List<String> tokens = hyphenation.findTokens(sourceString); String joinedTokens = StringUtils.arrayToDelimitedString(tokens.toArray(), ", "); System.out.println(joinedTokens); *** Find Tokens: Koffie, kop, kopje Using this test class is nice, but now we want to use it within elasticsearch
The following link is a reference to a gist containing the commands to try it out in Kibana Console
Using this sample, you can play around and investigate the effect of the HyphenationCompoundWordTokenFilter
Don’t forget to install the Dutch language file in the config folder of elasticsearch
Compound Word Token Filter Instalation Gist containing java class and Kibana Console exampleIn this post I’ll take you with me on my journey into Serverless with AWS
Head first, because I won’t dive into details like what is Serverless, why should you or should you not want Serverless, what those Amazon Web Services (AWS) are, nor into Kotlin
I will just work out a concrete case
At our office we regularly hold knowledge sessions
Mostly around lunch time a colleague will tell and show something about a topic
In the morning of an upcoming knowledge session someone sends out an announcement via email to all our colleagues
Since most of us also are on Slack, we could announce it there
And this is something we could easily automate
In this post I am going to build a Slack bot that announces knowledge sessions in the morning
What do we need
A schedule of planned sessions Piece of code that sends a message to a Slack channel For the sake of brevity I will focus on the absolute minimum of these requirements
Amazon Web Services The following AWS services will be used: S3 to store the scheduled sessions
Lambda to retrieve data from store and send a message to Slack when needed
CloudWatch for logging/monitoring and triggering a scheduled event every morning
IAM (Identity and Access Management) One service that’s missing in the list above list is IAM
This service is an inherent part of AWS
Before starting with the other services first we’ll create a role for our App
This role should have permissions to: AmazonS3FullAccess AWSLambdaExecute CloudWatchLogsFullAccess This single role will be used for all services I’ll be creating next
S3 (Simple Storage Service) First step is to create a S3 bucket
S3 is an object storage that can be used to store and retrieve files
Open S3 from the AWS console and create a bucket with the name ‘upcoming-sessions’ and use the default settings (click next, next, next)
In this bucket upload a file ‘next-session.json’ with the following contents: javascript { "date": "2019-01-20", "presenter": "Rachid", "topic": "Creating a Slack announcer using AWS", "type": "Chalk & Talk" } Lambda Next step is to create a lambda which will read this file and send a message to Slack when needed
In the AWS management console go to: Lambda -> functions -> Create function
As name choose: “session-announcer”
Since I want to write the lambda in Kotlin for runtime I choose Java 8
For Role select “choose an existing role” and select the Role we just created
Please note that everything can be done via scripts using the AWS CLI as well
Creating a new lambda function would look like: dos aws lambda create-function \ --function-name session-announcer \ --runtime java8 \ --role arn:aws:iam::123456789:role/sessionAnnouncerRole \ --handler eu.luminis.chefke.event.ScheduledEventAnnouncer::handleRequest \ --zip-file announcer.jar Since we don’t have any code or jar file yet I won’t use this now
Coding in Kotlin Now it’s time for the fun part; coding the logic to retrieve the sessions from S3 and send a message to Slack when needed
The handleRequest function is the entrypoint of our code
This function should be invoked to run our code
java import com.amazonaws.services.lambda.runtime.Context import com.amazonaws.services.lambda.runtime.RequestHandler import com.amazonaws.services.lambda.runtime.events.APIGatewayProxyRequestEvent import com.amazonaws.services.lambda.runtime.events.APIGatewayProxyResponseEvent import com.amazonaws.services.s3.AmazonS3ClientBuilder import com.beust.klaxon.Klaxon import java.io.InputStream import java.time.LocalDate import java.time.format.DateTimeFormatter data class SlackResponseMessage(val text: String) data class LuminisEvent(val topic: String, val type: String, val presenter: String, val date: String) const val filename = "next-session.json" class SessionAnnouncer : RequestHandler<APIGatewayProxyRequestEvent, APIGatewayProxyResponseEvent> { val s3client = AmazonS3ClientBuilder.defaultClient() val restClient = RestClient() override fun handleRequest(event: APIGatewayProxyRequestEvent, context: Context): APIGatewayProxyResponseEvent { val today = LocalDate.now() getEvent(today)?.let { e -> val message = "Today we have a ${e.type} session about '${e.topic}' presented by ${e.presenter}." println("Sending the event to Slack: $message") val slackJsonMessage = Klaxon().toJsonString(SlackResponseMessage(message)) // POST the announcement as a JSON payload to the Slack webhook URL
restClient.post(Config.slackWebhook, slackJsonMessage) return APIGatewayProxyResponseEvent().apply { statusCode = 200 headers = mapOf("Content-type" to "text/plain") body = "Message sent to Slack: $message" } } val notFoundMessage = "No event found for ${today} to post to Slack" println(notFoundMessage) return APIGatewayProxyResponseEvent().apply { statusCode = 404 headers = mapOf("Content-type" to "text/plain") body = notFoundMessage } } /** * Retrieves JSON file from S3, parse it and return the Object when it's today
*/ private fun getEvent(day: LocalDate): LuminisEvent
{ getEventFromBucket()?.let { val event = Klaxon().parse<LuminisEvent>(it) event?.let { nextEvent -> val eventDay = LocalDate.parse(nextEvent.date, DateTimeFormatter.ISO_DATE) if(eventDay.equals(day)) { return nextEvent } } } return null } fun getEventFromBucket(): InputStream
{ val s3Bucket = Config.s3Bucket if(s3client.doesObjectExist(s3Bucket, filename)) { return s3client.getObject(s3Bucket, filename).objectContent } println("'$filename' does not exists in the bucket: $s3Bucket") return null } } There are several ways to write this code and publish it to AWS
For any serious coding I personally prefer to use my favorite IDE on my local machine
To keep build/test/package related logic in a central place I use Gradle
Luckily there’s a Gradle plugin for uploading the code to an AWS lambda
My build.gradle file is as follows: java import com.amazonaws.services.lambda.model.InvocationType import jp.classmethod.aws.gradle.lambda.AWSLambdaInvokeTask import jp.classmethod.aws.gradle.lambda.AWSLambdaMigrateFunctionTask buildscript { ext.kotlin_version = '1.2.31' repositories { mavenCentral() maven { url "https://plugins.gradle.org/m2/" } } dependencies { classpath 'com.github.jengelman.gradle.plugins:shadow:2.0.2' classpath "org.jetbrains.kotlin:kotlin-gradle-plugin:$kotlin_version" classpath "jp.classmethod.aws:gradle-aws-plugin:0.30" } } version '1.0-SNAPSHOT' apply plugin: 'com.github.johnrengelman.shadow' // To create a fatjar which can be uploaded to AWS apply plugin: 'jp.classmethod.aws' // Gradle tasks for AWS stuff apply plugin: 'jp.classmethod.aws.lambda' // Gradle tasks for deploying and running lambda's apply plugin: 'kotlin' repositories { jcenter() mavenCentral() } dependencies { implementation "org.jetbrains.kotlin:kotlin-stdlib-jdk8:$kotlin_version" // AWS API implementation 'com.amazonaws:aws-lambda-java-core:1.2.0' implementation 'com.amazonaws:aws-lambda-java-events:2.1.0' implementation 'com.amazonaws:aws-java-sdk-s3:1.11.308' // JSON parser for Kotlin implementation 'com.beust:klaxon:3.0.1' } compileKotlin { kotlinOptions.jvmTarget = "1.8" } lambda { region = "eu-west-1" } // Task to deploy the code to AWS task deployFunction(type: AWSLambdaMigrateFunctionTask, dependsOn: [shadowJar, test]) { functionName = "session-announcer" runtime = com.amazonaws.services.lambda.model.Runtime.Java8 role = "arn:aws:iam::${aws.accountId}:role/scheduleCntRole" zipFile = shadowJar.archivePath handler = "eu.luminis.blog.SessionAnnouncer::handleRequest" memorySize = 512 timeout = 20 } // Task to directly invoke the lambda in AWS task invokeFunction(type: AWSLambdaInvokeTask) { functionName = "session-announcer" invocationType = InvocationType.RequestResponse payload = "" doLast { println "Lambda function result: " + new String(invokeResult.payload.array()) } } On top the plugins are applied and the dependencies are declared, at the bottom there are two custom tasks; deployFunction and invokeFunction
The first should be executed to upload the code to AWS, and the latter can be used for directly invoking the code running at AWS
Note that in deployFunction we’ve specified the handler function of our lambda
Now let’s upload the code by executing the task deployFunction
In IntelliJ this can be done by expanding the tasks in the Gradle view and double click deployFunction
Or just open a Terminal in the root of the project and run ./gradlew session-announcer:deployFunction
NB: When using this Gradle plugin or AWS CLI for the first time you may need to authenticate first
When the function is deployed successfully we can try to invoke it with: ./gradlew session-announcer:invokeFunction Oh noes, I got an error: {"errorMessage":"Missing env var 'S3_BUCKET'!","errorType":"java.lang.IllegalStateException"} Of course, the code uses Config.s3Bucket which reads the bucket name from an environment variable because we don’t want to have any configuration in code
java object Config { val s3Bucket by lazy { getRequiredEnv("S3_BUCKET") } val slackWebhook by lazy { getRequiredEnv("SLACK_WEBHOOK_URL") } private fun getRequiredEnv(name: String): String { println("Retrieving environment variable: $name") return System.getenv(name) ?: throw IllegalStateException("Missing env var '$name'!") } } We should add two required environment variables
Open the AWS console and go to lambda -> session-announcer and scroll down to the section “Environment variables”
Here we can add the key/value pairs
So add the key S3_BUCKET and for value the name of the bucket we created, e.g
‘upcoming-sessions’
Before we can add the second environment variable, first a bit more about Slack
Slack integration In order to send messages to a Slack Channel you need to create a so called ‘Slack App‘ and install it in the Slack workspace
After you’ve added the app to a channel, a webhook URL is generated and can be retrieved from the Slack console via: https://api.slack.com
This URL is the second environment variable we need to configure for the AWS lambda
So go back to the AWS console and add to the section “Environment variables” a new key: SLACK_WEBHOOK_URL with the slack webhook URL as value
Now try to invoke the lambda again with: ./gradlew session-announcer:invokeFunction Hopefully you will either see something like: {"statusCode":200,"headers":{"Content-type":"text/plain"},"body":"Message sent to Slack: Today we have a Chalk & Talk session about 'Creating a Slack announcer using AWS' presented by Rachid."} Or: {"statusCode":404,"headers":{"Content-type":"text/plain"},"body":"No event found for 2019-01-20 to post to Slack"} To get more insights, the logs can be viewed in CloudWatch
In the AWS console go to CloudWatch -> Logs
There should be a log with the name /aws/lambda/session-announcer containing all the logs of our lambda
Scheduling Now we have a piece of code running as a lambda in AWS that can announce the sessions of the current day
Next step is to trigger this lambda every morning
This can be done with ‘Events’ in CloudWatch
In the AWS console go to CloudWatch -> Events -> Rules and click “create rule”
Then select Schedule (instead of Event pattern)
To let it run every morning at 7:30 AM you can enter the following CRON expression: 30 7 * * 
*
At the right-hand side click ‘Add target’ and for function select the name of our lambda: session-announcer
Click configure details, choose a name and Save the rule
The next morning at 7:30 AM local time of the AWS region the lambda will be triggered
For testing purposes you could trigger it every minute with this CRON expression: */1 * * * 
*
Quite quickly we have automated a simple task
When you want to play around with this yourself, all the code from this post can be found in this Git repository
Becoming an AWS developer In the beginning playing around with AWS can be quite overwhelming
When you want to get some serious knowledge I recommend the Udemy course AWS Certified Solutions Architect – Associate (most of the time priced around $10,-)
A lot of information is scattered on the internet via blog posts and in the Amazon documentation
But what I like about this course is that it is kept up-to-date and gives you a single cohorent story which is easy to follow
Wind up In this post I focused on the bare minimum, but actually I also built: REST interface to add sessions to the store; using API gateway and another lambda REST interface to get session from store; called by a custom Slack command and backed by another lambda Now we have a small working app, some improvements and new features I have in mind: Store the session in a data store like Dynamo DB Make it possible to schedule multiple sessions Automatically provisioning of the services with CloudFormationIncluded in the Axon framework is a module that will collect metrics on the different Axon message types (commands, events and queries)
In this blog, I will explain how to use the Axon metrics module in combination with Prometheus and visualize the metrics in Grafana
Full disclaimer, I’m contributing to the Axon Metrics module myself, so I’m not completely impartial 😉 Intro Axon Axon is a Java framework for writing applications using the CQRS paradigm
One of the things CQRS focuses on more than the traditional CRUD/Active Record style of writing applications is business interactions
In a CRUD based system the focus lies more on the Creating/Read/Update/Deletion (hence CRUD) of records while CQRS focuses more the business interactions by modeling the explicit actions (Commands)
For example, instead of an update on the customer information record when the customer moves to a different address, CQRS also models the intent of the change
There will be an explicit MoveCustomer command
When using CQRS with event sourcing this command will also result in an explicit event that indicates that the customer has moved
Probably something like a CustomerMoved event
This explicit modeling of business interactions and state changes has great benefits in applications with complex business processes
In a traditional CRUD application this information is usually lost because it’s often impossible to deduce why, in this example, the customer’s address was updated (was it a misspelling or did the customer actually move to a new address)
When correctly applying event sourcing we can record and react to this information
Another advantage of having the business interactions modeled in explicit commands and events is that with Axon metrics we can measure the actual business itself
Visualizing metrics collected on these messages holds great value for businesses
Instead of measuring that certain (obscure) technical calls and updates are made to a system
Why not measure the amount of OrderPlaced events and put a thermometer in the core business process itself
When the number of OrderPlaced events suddenly go down there is a problem that affects the bottom line of the business, not just a technical problem of some obscure REST API call or internal service call that doesn’t work anymore where it’s unclear what the impact is on the business
Example application I’ve created an example application to demonstrate the capabilities of using Axon with the metrics module
The code can be found here: https://github.com/luminis-ams/axon-metrics-example The example application contains a flight ticket booking domain
There is a component that simulates user interaction by generating commands that automatically create flights and books seats on flights
I’ve instrumented the message processors in such a way that is possible to monitor individual message payload types (like BookSeatCommand and SeatBookedEvent for example) to individual message handlers (like the SlowEventListener component for example)
I’ve tried to put in some interesting problems in the application so that I can demonstrate how you can detect problems in your Axon application by monitoring it
The Axon Metric configuration code can be found here
Small side note, the current metrics module uses Dropwizard metrics
In order to expose the metrics using Prometheus I’ve configured an exporter for the Dropwizard metrics to Prometheus with this line: collectorRegistry.register(new DropwizardExports(metricRegistry)); (see MetricConfig line 33) Micrometer Metrics in Axon I’ve sent in a pull request to Axon to add support for Micrometer metrics
Micrometer is the new default metrics library which is used in Spring Boot 2.0
If you check out the axon-micrometer_beta branch on the GitHub repository you can see how to use the new module (for now you only can use it if you maven install my pull request branch) In the configuration class on the Micrometer branch, you can see it’s not necessary to export the Dropwizard metrics to Prometheus anymore
Micrometer provides an abstraction of Prometheus which we now use directly
Micrometer can be used with a lot of different metric implementations
For an overview see https://micrometer.io/docs Dashboard with Grafana Now that the metrics are exposed via Prometheus we can use Grafana to visualize them in a dashboard
(For more information about Prometheus and Grafana see the excellent blog of my colleague Jeroen Reijn about “Monitoring Spring Boot applications with Prometheus and Grafana”) Here are some screenshots from Grafana
CapacityMonitor on events This one displays the capacity of the event listeners As you can see in the above image there is 1 event listener that is close to full capacity
If you have a single thread processor this means that almost 100% of the time the event listener is busy
Probably events are queuing up for this event processor
The capacity is not always 1 because the capacity doesn’t measure the overhead of the Axon Framework itself, Axon has to retrieve event messages from a database and store them once their processed
MessageTimerMonitor on events This one displays the timings of the different event processors
You can see that two of the four event processors show a latency of sometimes more than 4 seconds for half of the requests
This is a problem and should be looked at
MessageCountingMonitor on BookSeatCommand This graph displays the success, failure and ingested counter of the BookSeatCommand
As you can see sometimes the command handler of the BookSeatCommand throws an Exception which will be counted as a failure in the graph
Run the example application I’ve created a docker setup if you want to run the full stack yourself
You can check out the code of example application here: https://github.com/luminis-ams/axon-metrics-example After that first do a build of the app project to create the application docker image
From the project root run: ./mvnw clean build After that run: docker-compose -f docker/docker-compose.yml up As told the metrics are exposed using Prometheus
On the Prometheus endpoint http://localhost:8080/actuator/prometheus you can see the metrics in raw form
In Prometheus, you can see the metrics under http://localhost:9090 You can import the example metrics dashboard in Grafana in the following way: Login http://localhost:3000 (login admin:password) On the left side of the screen click on the + icon and select import Copy the contents of grafana/dashboard.json into the bottom text field Select the Prometheus data source in the dropdown Conclusion Because of Axon’s strong focus on modeling business interactions, metrics on those business interactions contain a lot more useful information
Useful not only for troubleshooting but also for measuring the performance of the business itselfStarting January 1st of 2019, the Google Search Appliance (GSA) is set to be EOL
For one of my clients, we have chosen Elasticsearch as our alternative from 2019 onwards
However, there was one problem; the current GSA solution is in use by several API’s that can’t be changed for various reasons
Therefore, it was up to us to replicate the GSA’s behaviour with the new Elasticsearch implementation and come migration time, to swap out the GSA for Elasticsearch with as little functional changes as possible
The GSA provides a range of functionality, some of which is easily implemented with other technologies
In our case, this included of course search functionality, but also other things such as website crawling
The part that I found most interesting however, was the GSA’s ability to enable users to form queries based on two small domain specific languages (DSL)
Queries in these two DSL’s reach the GSA as query parameters on the GSA URL
The first DSL, specified with query parameter q, has three “modes” of functionality: Free text search, by simply putting some space separated terms allintext search, a search query much like the free text search, but excluding fields such as metadata, anchors and URL’s from the search inmeta search, which can potentially do a lot, but in our case was merely restricted to searches on metadata of the form key=value
The second DSL, specified with query parameter partialFields also provides searching on metadata
In this case, searches are of the form (key:value) and may be combined with three boolean operators: .: AND |: OR -: NOT An example query could then be (key1:value1)|(key2.value2)
In this blog, I will explain how to implement these two DSL’s using ANTLR and I will show you how ANTLR enables us to separate the parsing of the DSL from our other application logic
If this is your first time working with ANTLR, you may want to read two posts ([1], [2]) that have been posted on our blog earlier
If you are looking for the complete implementation, then please refer to the Github repository
Parsing the GSA DSL Let us start with parsing the q DSL
I have split the ANTLR grammar into a separate parser and lexer file for readability
The parser is as follows: apache parser grammar GsaQueryParser; options { tokenVocab=GsaQueryLexer; } query : pair (OR
pair)* #pairQuery | TEXT+ #freeTextQuery; pair : IN_META TEXT+ #inmetaPair | ALL_IN_TEXT TEXT (OR
TEXT)* #allintextPair; And the lexer is defined below: html lexer grammar GsaQueryLexer; ALL_IN_TEXT : 'allintext:'; IN_META : 'inmeta:'; OR : 'OR'; TEXT : ~(' '|'='|':'|'|'|'('|')')+; WHITESPACE : [ \t\r\n]+ -> skip; IGNORED : [=:|()]+ -> skip; Note that the parser grammar reflects the two different ways that the q DSL can be used; by specifying pairs or by simply putting a free text query
The pairs can be separated by an OR operator
Furthermore, the allintext keyword may separate terms with OR as well
The definition of the partialFields DSL is somewhat different because it allows for query nesting and more boolean operators
Both the parser and the lexer are shown below, again in two separate files
Parser: html parser grammar GsaPartialFieldsParser; options { tokenVocab=GsaPartialFieldsLexer; } query : pair | subQuery; subQuery : LEFTBRACKET subQuery RIGHTBRACKET | pair (AND pair)+ | pair (OR pair)+ | subQuery (AND subQuery)+ | subQuery (OR subQuery)+ | subQuery AND pair | subQuery OR pair | pair AND subQuery | pair OR subQuery; pair : LEFTBRACKET KEYWORD VALUE RIGHTBRACKET #inclusionPair | LEFTBRACKET NOT KEYWORD VALUE RIGHTBRACKET #exclusionPair | LEFTBRACKET pair RIGHTBRACKET #nestedPair; Lexer: html lexer grammar GsaPartialFieldsLexer; AND : '.'; OR : '|'; NOT : '-'; KEYWORD : [A-z0-9]([A-z0-9]|'-'|'.')*; VALUE : SEPARATOR~(')')+; SEPARATOR : [:]; LEFTBRACKET : [(]; RIGHTBRACKET: [)]; WHITESPACE : [\t\r\n]+ -> skip; Note the usage of labels in both grammars which, in the above case, allows me to easily distinguish different types of key-value pairs; nested, inclusive or exclusive
Furthermore, there is a gotcha in the matching of the VALUE token
To make a clear distinction between KEYWORD and VALUE tokens, I’ve included the : as part of a VALUE token
Creating the Elasticsearch query Now that we have our grammars ready, it’s time to use the parse tree generated by ANTLR to construct corresponding Elasticsearch queries
I will post some source code snippets, but make sure to refer to the complete implementation for all details
For both DSL’s, I have chosen to walk the tree using the visitor pattern
We will start be reviewing the qDSL
Creating queries from the q DSL The visitor of the q DSL extends a BaseVisitor generated by ANTLR and will eventually return a QueryBuilder, as indicated by the generic type: html public class QueryVisitor extends GsaQueryParserBaseVisitor<QueryBuilder> There are three cases that we can distinguish for this DSL: a free text query, an allintext query or an inmeta query
Implementing the free text and allintext query means extracting the TEXT token from the tree and then constructing a MultiMatchQueryBuilder, e.g.: html @Override public QueryBuilder visitFreeTextQuery(GsaQueryParser.FreeTextQueryContext ctx) { String text = concatenateValues(ctx.TEXT()); return new MultiMatchQueryBuilder(text, "album", "artist", "id", "information", "label", "year"); } private String concatenateValues(List<TerminalNode> textNodes) { return textNodes.stream().map(ParseTree::getText).collect(joining(" ")); } The fields that you use in this match query depend on the data that is in Elasticsearch – in my case some documents describing music albums
An inmeta query requires us to extract both the field and the value, which we then use to construct a MatchQueryBuilder, e.g.: apache @Override public QueryBuilder visitInmetaPair(GsaQueryParser.InmetaPairContext ctx) { List<TerminalNode> textNodes = ctx.TEXT(); String key = textNodes.get(0).getText().toLowerCase(); textNodes.remove(0); String value = concatenateValues(textNodes); return new MatchQueryBuilder(key, value); We can then combine multiple pairs by implementing the visitPairQuery method: html @Override public QueryBuilder visitPairQuery(GsaQueryParser.PairQueryContext ctx) { BoolQueryBuilder result = new BoolQueryBuilder(); ctx.pair().forEach(pair -> { QueryBuilder builder = visit(pair); if (hasOrClause(ctx, pair)) { result.should(builder); result.minimumShouldMatch(1); } else { result.must(builder); } }); return result; } Based on the presence of OR clauses we either create a should or must boolean clause for our Elasticsearch query
Creating queries from the partialFields DSL The visitor of the partialFields DSL also extends a BaseVisitor generated by ANTLR and also returns a QueryBuilder: html public class PartialFieldsVisitor extends GsaPartialFieldsParserBaseVisitor<QueryBuilder> There are three kinds of pairs that we can specify with this DSL (inclusion, exclusion or nested pair) and we can override a separate method for each option, because we labelled these alternatives in our grammar
A nested pair is simply unwrapped and then passed back to ANTLR for further processing: html @Override public QueryBuilder visitNestedPair(GsaPartialFieldsParser.NestedPairContext ctx) { return visit(ctx.pair()); } The inclusion and exclusion query implementations are quite similar to each other: html @Override public QueryBuilder visitInclusionPair(GsaPartialFieldsParser.InclusionPairContext ctx) { return createQuery(ctx.KEYWORD().getText(), ctx.VALUE().getText(), false); } @Override public QueryBuilder visitExclusionPair(GsaPartialFieldsParser.ExclusionPairContext ctx) { return createQuery(ctx.KEYWORD().getText(), ctx.VALUE().getText(), true); } private QueryBuilder createQuery(String key, String value, boolean isExcluded) { value = value.substring(1); if (isExcluded) { return new BoolQueryBuilder().mustNot(new MatchQueryBuilder(key, value)); } else { return new MatchQueryBuilder(key, value).operator(Operator.AND); } } Remember that we included the : to help our token recognition
The code above is where we need to handle this by taking the substring of the value
What remains is to implement a way to handle the combinations of pairs and boolean operators
This is done by implementing the visitSubQuerymethod and you can view the implementation here
Based on the presence of an AND or OR operator, we apply must or should clauses, respectively
Examples In my repository, I’ve included a REST controller that can be used to execute queries using the two DSL’s
Execute the following steps to follow along with the examples below: Start an Elasticsearch instance at http://localhost:9200 (the application assumes v6.4.2) Clone the repository: git clone https://github.com/markkrijgsman/migrate-gsa-to-elasticsearch.git && cd migrate-gsa-to-elasticsearch Compile the repository: mvn clean install Run the application: cd target && java -jar search-api.jar Fill the Elasticsearch instance with some documents: http://localhost:8080/load Start searching: http://localhost:8080/search You can also use the Swagger UI to execute some requests: http://localhost:8080/swagger-ui.html
For each example I will list the URL for the request and the resulting Elasticsearch query that is constructed by the application
Get all albums mentioning Elton John http://localhost:8080/search?q=Elton html GET rolling500/_search { "query": { "bool": { "must": [ { "multi_match": { "query": "Elton", "fields": [ "album", "artist", "id", "information", "label", "year" ], "type": "best_fields", "operator": "AND", "lenient": true, } } ] } } } Get all albums where Elton John or Frank Sinatra are mentioned http://localhost:8080/search?q=allintext:Elton%20OR%20Sinatra html GET rolling500/_search { "query": { "bool": { "must": [ { "bool": { "must": [ { "multi_match": { "query": "Elton Sinatra", "fields": [ "album", "artist", "id", "information", "label", "year" ], "type": "best_fields", "operator": "OR", "lenient": true } } ] } } ] } } } Note that the operator for the multi match query is now OR, where it was AND in the previous example
Get all albums where the artist is Elton John http://localhost:8080/search?partialFields=(artist:Elton) html GET rolling500/_search { "query": { "bool": { "must": [ { "match": { "artist": { "query": "Elton", "operator": "AND" } } } ] } } } Get all albums where Elton John is mentioned, but is not the artist http://localhost:8080/search?partialFields=(-artist:Elton)&q=Elton html GET rolling500/_search { "query": { "bool": { "must": [ { "bool": { "must_not": [ { "match": { "artist": { "query": "Elton", "operator": "OR" } } } ] } }, { "multi_match": { "query": "Elton", "fields": [ "album", "artist", "id", "information", "label", "year" ], "type": "best_fields", "operator": "AND", "lenient": true } } ] } } } Get all albums created by Elton John between 1972 and 1974 for the label MCA http://localhost:8080/search?partialFields=(artist:Elton).(label:MCA)&q=inmeta:year:1972..1974 html GET rolling500/_search { "query": { "bool": { "must": [ { "bool": { "must": [ { "match": { "artist": { "query": "Elton", "operator": "AND" } } }, { "match": { "label": { "query": "MCA", "operator": "AND" } } } ] } }, { "bool": { "must": [ { "range": { "year": { "from": "1972", "to": "1974", "include_lower": true, "include_upper": true } } } ] } } ] } } } Please refer to the unittests for a lot more examples: Unittest examples for DSL q Unittest examples for DSL partialFields Conclusions As you can see, the usage of ANTLR allows us to specify fairly complex DSL’s without compromising readability
We’ve cleanly separated the parsing of a user query from the actual construction of the resulting Elasticsearch query
All code is easily testable and makes little to no use of hard to understand regular expressions
A good addition would be to add some integration tests to your implementation, which you can learn more about here
If you have any questions or comments, let me know!In a lot of software that I’ve seen, the class model could be better
One of the problems I often see is that a class contains other “hidden” classes: a set of fields that really should be its own class
I will discuss where this originates from, and why it may cause problems
First, I will provide two common examples
The first is the address, a set of at least street, house number, and city, but often also containing ZIP code, state and country
In a lot of cases these will just be fields in another class such as customer
The second example is a date range; a period of time between a start date and an end date, for instance to indicate when a given rule is valid
Often, a date range is a start and end date in the class that it applies to
What is the problem anyway
So, why is it a problem if these are not represented as separate classes
First, it may lead to duplication of code
If you have several date ranges in your code, there will be several places where you are going to check if a given date is in that date range, or if another date range overlaps
Similarly, there may be validation and rendering methods for an address
All this code must also be tested, sou you get a lot of duplication in tests as well
Second, it makes the code less readable in many ways
If-statements comparing four dates from two date ranges is hard to understand, and tricky to debug
Quick, what does this do
java Java LocalDate dateRangeStart = ...; LocalDate dateRangeEnd = ...; LocalDate otherDateRangeStart = ...; LocalDate otherDateRangeStart = ...; if (dateRangeEnd.isAfter(otherDateRangeStart) && dateRangeStart.isBefore(otherDateRangeEnd)) { ..
} If a class contains multiple addresses, your field names will become less readable: java Java class Customer { private String firstName; private String surname; private String residenceAddressStreet; private String residenceAddressHouseNumber; private String residenceAddressCity; private String postalAddressStreet; private String postalAddressHouseNumber; private String postalAddressCity; ..
} 1 2 3 4 5 6 7 8 9 10 11 class Customer { private String firstName; private String surname; private String residenceAddressStreet; private String residenceAddressHouseNumber; private String residenceAddressCity; private String postalAddressStreet; private String postalAddressHouseNumber; private String postalAddressCity; ..
} This may be the right moment to consider why classes are modelled this way
There may be a lot of different reasons, but one of the most important ones I see is that an application blindly copies an external data model
If a database is provided, and the database has a table “customer” with street, house number and city fields, there will be a class Customer with those same fields
Similarly, if the interface to a front end specifies the address as fields in a larger object instead of a separate object, that model may easily become the back end model as well
Another reason may be that no suitable class is available
Java has great support for dates and durations, but not for a date range
So the easy way is to just add a start and end date to your class
Type safety Now, for a last warning: upon discovering the code duplication and readability problems, one might be tempted to create static helper methods in utility classes, because this has less impact on the code as a whole
Apart from all the reasons why utility classes should be avoided, I want to add one more specific to this case: since many fields of an address are Strings, any helper method will have a signature that does not check whether you pass the right fields: java class AddressHelper() { public static boolean isAddressValid(String street, String houseNumber, String city) { ..
} } // No compile error if (AddressHelper.isAddressValid(houseNumber, street, city) { ..
} Especially error-prone because different countries have a different order in which they present the address fields: compare the Dutch street – house number – ZIP code – city to the US house number – street – city – ZIP code
Using separate classes If you represent an address as a separate class, it is far easier to provide it to an external service in a type-safe manner
It also makes your code more readable
java class Address { private String street; private String houseNumber; private String city; } class Customer { private String firstName; private String surname; private Address residenceAddress; private Address postalAddress; } class AddressService() { public boolean isAddressValid(Address address) { ..
} } if (addressService.isAddressValid(customer.getResidenceAddress())) { ..
} To use date ranges, you probably want to have your own class (or use one from a library) to contain all the related logic
This is both far easier to test and makes your code a lot more readable
java class DateRange { private LocalDate start; private LocalDate end; ..
public boolean overlaps(DateRange other) { return end.isAfter(other.getStart()) && start.isBefore(other.getEnd()); } } if (dateRange.overlaps(otherDateRange)) { ..
} In closing Refactoring may incur significant cost to the development project, and the later you start the more it will cost
However, operational cost is significant over the lifetime of a software product, and solid, readable code will drive that cost down
Experience will make it easier to spot these problems earlier, but even if you run into the problems late in a project, it is worth considering refactoring your class modelAngularJS is a huge framework and already has many performance enhancements built in
Despite that, it’s very easy to write code in AngularJS that will slow down your application
To avoid this, it is important to understand what causes an AngularJS application to slow down and to be aware of trade-offs that are made in the development process
In this post I describe things that I have learned from developing AngularJS applications that will hopefully enable you to build faster applications
First of all, a quick win to boost your applications performance
By default AngularJS attaches information about binding and scopes to DOM nodes and adds CSS classes to data-bound elements
The application doesn’t need this information but it is added for debugging tools like Batarang to work properly
You can disable this by one simple rule in your application config
java app.config(['$compileProvider', function ($compileProvider) { $compileProvider.debugInfoEnabled(false); }]); If you want to temporarily enable the debug information just open the debug console and call this method directly in the console: java angular.reloadWithDebugInfo(); Watchers in AngularJS “With great power comes great responsibility” AngularJS provides the $watch API to observe changes in the application
To keep track of changes you need to set a watcher
Watchers are created on: $scope.$watch
{{ }} type bindings
Directives like ngShow, ngClass, ngBindHtml, etc
Isolated scope variables: scope: { foo: ’=’ }
filters
Basically: everything in AngularJS uses watchers
AngularJS uses dirty checking, this means it goes through every watcher to check if they need to be updated
This is called the digest loop
If a watcher relies on another watcher, the digest loop is called again, to make sure that all of the changes have propagated
It will continue to do so, until all of the watchers have been updated
The digest loop runs on: User actions (ngClick, ngModel, ngChange)
$http responses
promises are resolved
using $timeout/$interval
calling $scope.$apply() of $scope.$digest()
Basically: the digest loop is called a lot
The “Magic” of AngularJS works great but it is fairly easy to add so many watchers, that your app will slow down
Especially when watchers doing too much work
With this in mind we will go on and talk about some easy changes to make our applications faster! Avoid a deep watch By default, the $watch() function only checks object reference equality
This means that within each $digest, AngularJS will check to see if the new and old values pointing to the same object
A normal watch is really fast and preferred from a performance point of view
What if you want to perform some action when a modification happened to an object or array
You can switch to a deep watch
This means that within each $digest, AngularJS will check the whole tree to see if the structure or values are changed
This could be very expensive
A collection watch could be a better solution
It works like a deep watch, except it only checks the first level of object’s properties and checks for modifications to an array like adding, removing, replacing, and reordering items
Collection watches are used internally by Angular in the ngRepeat directive Another effective alternative that I recommend is using one-way dataflow
This can be accomplished by making a copy of the watched object, modify the copy and then change the variable to point to the copy
Than a normal (shallow) watch will do the job
Use track by in ng-repeat ngRepeat uses $watchCollection to detect changes in collections
When a change happens, ngRepeat makes the corresponding changes to the DOM
Let’s say you want to refresh a flight list with new flight data
The obvious implementation for this refresh would be something like this: apache $scope.flights = serverData.flights; This would cause ngRepeat to remove all li elements of existing tasks and create them again, which might be expensive if we have a lot of flights or a complex li template
This happens because AngularJS adds a $$hashkey property to every flight object
When you replace the flights with the exact same flights from the server the $$hashkey is missing and ngRepeat won’t know they represent the same elements
The solution Use ngRepeat’s track by clause
It allows you to specify your own key for ngRepeat to identify objects, instead of just generating a unique hashkey
The solution wil look like: apache <div ng-repeat=”flight in flights track by flight.id”> Now ngRepeat reuse DOM elements for objects with the same id
Bind once Use bind once where possible
If bind once is used, Angular will wait for a value to stabilize after it’s first series of digest cycles, and will use that value to render the DOM element
After that, Angular will remove the watcher and forget about that binding
This will minimize the watchers and thereby lightens the $digest loop
From AngularJS 1.3 there is the :: notation to allow one time binding
apache <p>{{::name}}</p> <ul> <li ng-repeat="user in ::users"></li> </ul> Use ng-if instead of ng-show/ng-hide Maybe this is obvious but i think it’s worth mentioning it
Use ng-if where possible
The ng-show/ng-hide directives just hide the element by setting it’s style property “display” to none
The element still exists in the DOM including every watcher inside it
Ng-if removes the element and watchers completely and generates them again when neededWith advanced tools available for search like Solr and Elasticsearch, companies are embedding search in almost all their products and websites
Search is becoming mainstream
Therefore we can focus on teaching the search engine tricks to return more relevant results
One new trick is called “learning to rank”
Learning to rank uses a trained model to come up with a better ranking of the search results
During the presentation you’ll learn what Learning To Rank is
To be able to understand the machine learning part, you get information about machine learning models, feature extraction and the training of models
You will also learn about when to apply learning to rank and of course you’ll get an example to show how it works using elasticsearch and a learning to rank plugin
After this presentation you have learned how and why to combine Machine Learning and SearchAs an engineer, you will probably have wondered “why didn’t they secure this just a little better” after you’re notified of yet another breach of your information
As professionals, we understand how security should work in our corner of the organization, but what about the rest
Outside IT, security and risk are often handled in a very different way
This talk will show you a peek of how the “real world” thinks about security
We’ll touch upon the models that are used, how risk is quantified, and why smart companies sometimes take dumb risks
We’ll show you the breadth of security, ranging from encryption, IDSs and firewalls to personnel security training, risks vs measures, legislation (like Europe’s GDPR and Dutch law) and terms like “residual risk.” To top it off, we will go into some of the main risks threatening organizations today
This talk will bring you a more complete picture of what “security” actually is, and helps you understand—and provide arguments against—decisions that don’t make sense from your point of viewTracing bugs, errors and the cause of lagging performance can be cumbersome, especially when functionality is distributed over multiple microservices
In order to keep track of these issues, the usage of an ELK stack (or any similar sytem) is already a big step forward in creating a clear overview of all the processes of a service and finding these issues
Often bugs can be traced by using ELK far more easily than just using a plain log file – if even available
Optimization in this approach can be preferred, as for example you may want to see the trace logging for one specific event only
Here Sleuth comes in handy
Depicted from systems as Dapper, Zipkin (we will come to that one later) and HTrace, Sleuth can ensure that every request event that is initiated in a Spring application, will start a new trace
Every internal event (that will possibly trigger an event to another application) will initiate a new span and closes after the event is finished
As the request event is finished, the trace will be closed and a new one will start eventually when there is a new request event initiated
Getting Sleuth to work in your Spring-Boot application, is as easy as putting the dependency in your Maven’s pom.xml, as described at https://cloud.spring.io/spring-cloud-sleuth/: html <dependencyManagement> <dependencies> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-sleuth</artifactId> <version>2.0.0.M5</version> <type>pom</type> <scope>import</scope> </dependency> </dependencies> </dependencyManagement> <dependencies> <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-sleuth</artifactId> </dependency> </dependencies><repositories> <repository> <id>spring-milestones</id> <name>Spring Milestones</name> <url>https://repo.spring.io/libs-milestone</url> <snapshots> <enabled>false</enabled> </snapshots> </repository> </repositories> As you integrate Sleuth in all the (micro-)services, it will automatically pass the trace ids to the next to be called service (if you are using RestTemplate; for other rest service implementations, you may need to add your own configuration), so that it inherits the value
This ensures that all logs behind one starting event can be grouped under one trace
By itself this already comes in handy, as you can filter in your logging by traceid and thus easily oversee the escalating flow of one event over the services
All four services can be filtered by the identical traceid, but the services will have different spanids
(Only service A is set to debug level, other services to info.) When coming to investigating performance issues, we can already analyse the logs – with the help of the traceids in place and take a look at the timing of the services
But here Zipkin can come in handy
Zipkin is – as the authors say – a distributed tracing system
It runs as Spring Boot application and its default storage is in-memory, but you may as well choose one of the database options available to store your data, with help of their spark job
You can start using Zipkin locally by using the docker image available: https://hub.docker.com/r/openzipkin/zipkin/
The server is default available on http://localhost:9411/zipkin
Implementing Zipkin in your (micro-)services is also adding a dependency to your Maven structure: html <dependency> <groupId>org.springframework.cloud</groupId> <artifactId>spring-cloud-starter-zipkin</artifactId> </dependency> Of course, you may want to change configuration such as the base-url and those properties are available by spring.zipkin.* 
As Sleuth already opens and closes spans by events that are opened and closed (namely, requests done with RestTemplate), Zipkin can be integrated to visualize those spans correctly
Each trace can be found separately and you can also search per available service
So, let’s look at an example call done to service a: Searching in service a to find our trace
When you have found the trace of interest, you can click on it and an overview of the underlying spans is shown: View of same call as previous
As seen in this example, we see the trace with spans from service a, to b to d & a to c
Also, we can see the time it took to perform each call (or span)
When a trace has an error, you may view the trace and find out it is red instead of blue
A trace that has gone wrong
Inside this trace, you can still find the called services with their spans and find out were it could have went wrong
Spans of the errored trace
You may click on a span and a popup with extra information will open
When you click again on “more info”, you will find the traceid & spanid
This traceid can be used to look into the logs in ELK again to find the exception
Extra information of a span
Finding the error in the logs
Expanding the message will give the information needed to trace the exception that is thrown
Of course, finding bugs or exceptions can be perfectly optimized without the use of Zipkin and as suggested by https://peter.bourgon.org/blog/2016/02/07/logging-v-instrumentation.html, it is important to consider the difference between logging and instrumentation tooling
But, when it comes to creating an overview of multiple called microservices over time and tracing any timing issues, Zipkin can really increase the ability to gain insight of performance issues
So let’s look into an example: A call to service a is taking more than 30 seconds
There are complaints that a call to some endpoint of service a is taking around 30 seconds – which is unexpected
As seen in the logs, we can already confirm the call is taking from 18:16:05 to 18:16:35
The cause cannot be fully confirmed by looking at the logs, although an estimated guess can be made
In Zipkin, all we need to do is select service a (the root service), filter the traces by datetime and endpoint call and the culprit can be viewed in an eyeblink: The culprit
Here we can see which value Zipkin in combination with Sleuth may add to the use of microservices
One last thing to add, is something we need to customize: the possibility to group by conversation, with several follow-up calls that need to be grouped, as they form a cascade of events
When a user wants to start such a conversation and wants to trace that whole conversation, it will be convenient if the user can use a level higher id than traceid – which will only return one call of the conversation
With Sleuth we can add baggage items & tags: Baggage items are items you can add to a trace and will be propagated over the trace context; thus also passed over the services
But, they are not passed to Zipkin
Tags, on the contrary, are normally used for spans to tag them and are passed to Zipkin
By adding a webfilter (or any other interceptor you prefer), we can create our own id system, which can provide a grouping over multiple calls – a conversation: java @Configuration public class SleuthConversationConfiguration { public static final String TAG_NAME = "X-B3-CONVID"; @Bean @Order(TraceFilter.ORDER + 5) public GenericFilterBean customTraceFilter(final Tracer tracer) { return new GenericFilterBean() { @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { if (servletRequest instanceof RequestFacade) { //Get header conversation id IF available -> overrules any other one String existingConversationId = ((RequestFacade) servletRequest) .getHeader(TAG_NAME); Span span = tracer.getCurrentSpan(); if (existingConversationId != null && span.getBaggage().get(TAG_NAME) == null) { span.setBaggageItem(TAG_NAME, existingConversationId); //IF the conversation id is not available in header of request, we try to get it from baggage item } else if (existingConversationId == null) { existingConversationId = span.getBaggage().get(TAG_NAME); //If not available, generate one for us if (existingConversationId == null) { existingConversationId = UUID.randomUUID().toString(); span.setBaggageItem(TAG_NAME, existingConversationId); } } //Add conversation id as tag in order to make it available in Zipkin tracer.addTag(TAG_NAME, existingConversationId); //Add to MDC in order to make it visible in logging MDC.put(TAG_NAME, existingConversationId); //Will continue span tracer.continueSpan(span); //Will ensure the conversation id is also available in the response HttpServletResponse response = (HttpServletResponse) servletResponse; if (response != null) { response.addHeader(TAG_NAME, existingConversationId); response.addHeader("Access-Control-Expose-Headers", TAG_NAME); } } filterChain.doFilter(servletRequest, servletResponse); } }; } } The part with the headers is optional; for service a it is convenient, as the user can pass its own id to the service, in order to ensure that the conversation id will take its place in all the calls
But, for service b, c and d, Sleuth will already ensure that the id is passed as baggage item
Therefore, the part in which we try to get it from the header is unnecessary, and the code could be as follows: java @Configuration public class SleuthConversationConfiguration { public static final String TAG_NAME = "X-B3-CONVID"; @Bean @Order(TraceFilter.ORDER + 5) public GenericFilterBean customTraceFilter(final Tracer tracer) { return new GenericFilterBean() { @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { Span span = tracer.getCurrentSpan(); String existingConversationId = span.getBaggage().get(TAG_NAME.toLowerCase()); if(existingConversationId == null){ existingConversationId = UUID.randomUUID().toString(); span.setBaggageItem(TAG_NAME, existingConversationId); } tracer.addTag(TAG_NAME, existingConversationId); MDC.put(TAG_NAME, existingConversationId); tracer.continueSpan(span); filterChain.doFilter(servletRequest, servletResponse); } }; } } Further, you need to ensure that you add this extra field in your logback configuration (or any log configuration you may use) of all the services that use it by using %X{X-B3-CONVID:-} as property value; else you will not see it in your logs! If you configured all correctly and do some calls with a header “X-B3-CONVID: OUR-UNIQUE-IDENTIFIER”, you may see the following in the logs: Using our own unique identifier will provide a way to get all logging of the conversation – thus, of multiple calls that are given in a cascade of calls
In Zipkin, we can provide the name of the tag we created to filter all the traces
This way, all traces will be shown and we can look into them if we want to gather more info of their performance and/or error handling
Hence, Sleuth & Zipkin are helpful tools to create a more concrete overview of the distribution of calls over (micro-)services and can enhance the tracing of bugs or performance issues
With some additional tweaking, we can even create overcoupling ids to trace conversations
It will not replace ELK or similar services, but will be a helpful tool for monitoring your applications
Useful resources: https://hub.docker.com/r/openzipkin/zipkin/ https://peter.bourgon.org/blog/2016/02/07/logging-v-instrumentation.html https://cloud.spring.io/spring-cloud-sleuth/ https://spring.io/blog/2016/02/15/distributed-tracing-with-spring-cloud-sleuth-and-spring-cloud-zipkinAfter reading the Java 9 modularity book by Sander Mak and Paul Bakker (excellent book btw), I was wondering how many Java libraries already had been migrated to Java 9 modules
I couldn’t find much about it, except for Stephen Colebourne’s Module names for Java SE 9
Even though it’s more focussed on module naming, it contains a markdown table that mentions a number of migrated libraries (40, with 30 from one project)
But it didn’t completely answer my question, so I wondered if I shouldn’t do some research myself and (“how hard can it be?”) started writing a script that gets jars from maven central and checks their Java 9 module status
Because I didn’t fancy the idea of entering lots of maven coordinates of libraries I knew, I started with a few well-known ones and extracted the dependencies from the pom file to discover more
It took some coding effort (which made me realise again that when something isn’t hard, that doesn’t necessarily mean it’s not a lot of work ;-)), but it worked out pretty well
Even though I only started with a list of 15 to 20 libraries, the script resulted in more than 4000 (!) libraries being checked
59 Java Modules The results were somewhat disappointing: i found only 59 Java modules
On a total of 4527, that’s only 1,3%! It seems we must conclude that adoption of the Java 9 Module system is still very limited
Automatic modules But maybe that is a premature or unfair conclusion
First of all, in this list there are 534 that have explicitly set the “Automatic Module Name” in their manifest
So you might argue that at least the library maintainers put some effort in making their library ready for use in a modular Java application
Taking these into account leads to an adaption figure of 13% – 10 times better then my first conclusion
Dead libraries Next, even though the script always obtains the latest version of a library, there is of course a number of libraries that is too old, that is not maintained anymore, for example because they are superseded by something new (jboss -> wildfly) or just got new maven coordinates (tomcat -> org.apache.tomcat)
It wouldn’t be fair to count them as “still not migrated”, as they probably never will
I should only check libraries that are “new enough”; using the date that Java 9 was released seems the logical thing to do
Filtering my list of libraries on being updated after Sept 22, 2017, the figures are: – modules: 53 – defined automatic modulde name: 530 – total number of libraries: 2756 So 1,9% is a real Java module, and 21% is “module ready”
The latter is much better than the 13% we got before, but the 1,9% for real modules is still not very impressive
(B.t.w., did you notice the number of modules is less then before
Apparently, some libraries where already migrated before Java 9 was released; I guess there are people who are enthusiastic about the Java 9 module system ;-)) Libraries or artifacts The last thing to consider is that while I was talking about libraries, I should have used the term “artifacts”
I don’t think there is a formal definition of “library”, but I think most people would agree with me that a library can (and often will) consist of several artifacts
So an interesting question is what the figures will be when we take this into account
Unfortunately, there is no easy way to regcognize which artifacts belong to which library
But what we can do, is measure against maven group id’s; it’s likely that artifacts within one group somehow form a project and are update / migrated together
Counting maven group-id’s, the figures are: – modules: 22 – defined automatic modulde name: 130 – total number of libraries: 684 resulting in: 3,2% real modules, 22% “module ready”
Even though the percentage of real modules has increased with 50%, it’s still not much, is it
Modularity is key May be I should have known better after I’d discovered Stephen Colebourne’s list, but I think the results are disappointing
And it makes me a bit sad, because I think that creating modular software is important in order for it to be and stay maintainable
At Luminis we have always believed (and still do) that OSGi is an excellent framework for writing modular code and I think that Java 9 modules is a nice addition in the field of modularity that fits nicely for systems that don’t require the dynamic nature that OSGi supports so nicely
I wonder whether the low adoption rate indicates indifference, or that the library maintainers are just to busy with other priorities
But let’s end positive: there are quite some library maintainers that did put effort in support Java 9 modules, and we should appreciate that
Let’s send them some kudos: pick some from the list and tweet that you’re glad they made it a module!In product development advanced tooling and so-called ‘proven' development methods can sometimes cause more problems than they solve
For instance when they distract the development team from what they should really be focussing on: the end result, the product itself
Although Scrum provides a healthy framework that minimises risk and maximises agility of the development process, I’ve seen Scrum teams completely go astray at the expense of the product
In this blog I will outline a recurring pattern I have noticed in Scrum development teams and I will share some of my ideas on how teams can keep the right focus within Scrum
When I think about product development and Scrum, I see that the product and the development team often live in separate worlds
The world of the product is ruled by business and user goals, while the world of the development team is ruled by sprint goals
These are not the same kind of goals
For instance, a user goal could be to easily order a coffee-to-go, while a sprint goal could be to implement single sign-on
Like I said: different worlds
During this whole process the development team is often ‘trapped’ inside their own sprint bubble
So the development process goes something like this: During a sprint new functionality is added to the product and gets released to the market after it’s considered done
This new functionality triggers some kind of response of the business and/or the users
This response (or the lack of it) might then be translated back into new product requirements that are fed back to the development team
During this whole process the development team is often ‘trapped’ inside their own sprint bubble
They are lulled to sleep by what I call ‘the Scrum Pendulum’; the constant rhythm of the sprints and the hyper-focus on the sprint goals
Most of the time they are unaware of the effect of their work on the actual product, unaware also of the effect the product has on the business or the users
The Scrum Pendulum creates a ‘detached’ development team, that might very well be meeting its sprint goals, but has no idea whether it is moving the product in the right direction
This means you could also regard the PO as a SPOF: a Single Point Of Failure
Most of the time the only link between the development team and the product is the product owner (PO)
The PO has a pretty crucial role in the development process; she (or he) is responsible for translating business and user goals to product requirements and also for deciding whether new functionality that has been developed is fit te be released to the market
So you can imagine that if the PO fails, the whole process will fail
This means you could also regard the PO as a SPOF: a Single Point Of Failure
And that is something I think you need to avoid in product development
As a team you should define what ‘success’ means
So what can a development team do to prevent the Scum Pendulum from happening
I think it is key to take a shared responsibility for the end result, the product
Even if this was not explicitly asked by your PO, manager or client
As a professional engineer, designer or tester I believe you have a responsibility that goes beyond the development team
As a team you should define what ‘success’ means: when do you think your team has delivered a good job
You might not all agree on this, which is even more reason to discuss this with your team members and create a common understanding of ‘success’
For example, for a team ‘success’ might mean a minimum of 4 stars in the App Store rating (if it is an app you are developing)
It could also be something like acceptable response times during heavy traffic or a good overall product usability
It doesn’t matter what you as a team define as ‘success’, the point is that you start thinking about product quality beyond the sprint boundaries
Defining success is a useful exercise in itself, but it becomes even more valuable when the development team measures it’s success using their own defined standard
This doesn’t need to take a lot of time or be very complex
Checking the app store rating and comments will often give a lot of insight
Also, a quick-and-dirty user test is really simple to conduct and doesn’t need a lot of preparation
The results might not be 100% representative, they do give some level of insight
Which is a lot more to go on then when the development team would forever remain inside their sprint bubble
An important last detail is of course to use the insights to learn and improve the work inside the sprints, and everybody will benefit
Don’t let Scrum (or any other development method or tool) replace common sense
This team’s definition and constant evaluation of it’s own success creates an ‘outer loop’ around the Scrum process that provides a backup for failing (or even non-existing) product owners
But more importantly, it prevents the team from getting too detached from the product and the world of the business and the users
Don’t let Scrum (or any other method or tool) replace common sense
And don’t hide behind your sprint commitment; product quality and success are a shared responsibility
Want to learn more about how to avoid the pitfalls of tooling and methods in product development
I’ll be sharing my thoughts on this at the DevCon 2018 (April 12, Ede) and CodeMotion Amsterdam 2018 (May 8, Amsterdam) conferences
Or sign up for the training Product Thinking at the Luminis AcademyOk, I have to admit, I am not (yet) a Solr expert
However, I have been working now with elasticsearch for years and the fundamentals of obtaining relevant results for Solr and Elasticsearch are still the same
For a customer, I am working on fine-tuning their search results using an outdated Solr version
They are using Solr 4.8.1, yes an upgrade is planned in the future
Still, they want to try to improve their search results
Using my search knowledge I started getting into Solr and, I liked what I saw
I saw a query that had matching algorithms, filters to limit documents that need to be considered and on top of that lots of boosting options
So many boosting options that I had to experiment a lot to get to the right results
In this blog post, I am going to explain what I did with Solr coming from an elasticsearch background
I do not intend to create a complete guideline on how to use Solr
I’ll focus on the bits that surprised me and the stuff that deals with tuning the edismax type of query
A little bit of context Imagine you a running a successful eCommerce website
Or even better, you have created a superb online shopping experience
With an excellent marketing strategy, you are generating lots of traffic to your website
But, yes there is a but, sales are not as expected
Maybe it is even a bit disappointing
You start evaluating the visits to your site using analytics
When going through your search analytics you notice that most of the executed searches do not result in a click and therefore not into sales
It looks like the results shown to your visitors for their searches are far from optimal
So we need better search results
Getting to know your data Before we start writing queries, we need to have an idea about our data
We need to create inverted indexes per field, or for combinations of fields, using different analyzers
We need to be able to search for parts of sentences, but also be able to boost matching sentences
We want to be able to find matches for combinations of fields
Imagine you sell books, you want people to be able to look for the latest book by Dan Brown called Origin
Users might enter a search query like Dan Brown Origin
If might become a challenge if you have structured data like: Being relevant with Solr html { "author": "Dan Brown", "title": "Origin" } How would you do it if people want to have the latest Dan Brown
What if you want to help people choose by using the popularity of books using ratings or sales
Or how to act if people want to look at all the books in the Harry Potter series
Of course, we need to have the right data to be able to facilitate our visitors with these new requirements
We also need a media_type field later on
With the media type, we can filter on all eBooks for example
So the data becomes something like the following block
html { "id": "9781400079162", "author": "Dan Brown", "title": "Origin", "release_date": "2017-10-08T00:00:00Z", "rating": 4.5, "sales_pastyear": 239, "media_type": "ebook" } Ranking requirements Based on analysis and domain knowledge we have the following thoughts translated into requirements for the ranking of search results: Recent books are more important than older books Books with a higher rating are more important than lower rated books Unrated books are more important than low rated books Books that are sold more often in the past year are more important than unsold books Normal text matching rules should be applied Mapping data to our index In Solr, you create a schema.xml to map the expected data to specific types
You can also use the copy_to functionality to create new fields that are analyzed differently or are a combination of the provided fields
An example could be to add a field that contains all searchable other fields
In our case, we could create a field containing the author as well as the title
This field is analyzed in the most optimal way to do matching
We add a tokenizer, but also filters for lowercasing, stop words, diacritics, and compound words
We also have fields that are more for boosting using phrases and numbers or dates
We want fields like title and author to support phrases but also full matches
With this, we got a few extra search requirements Documents of which the exact author or title matches the query should be more important Documents of which the title contains the words in the query in the same order are more important With these rules, we can start to create a query and apply our matching and boosting requirements
The Query Creating the query was my biggest surprise when going for Solr
Another configuration mechanism is the Solrconfig.xml
This file configures the Solr node
It gives you the option to create your own endpoint for a query that comes with lots of defaults
One thing we can do for instance is to create an endpoint that automatically filters for only ebooks
We can call this endpoint to search for ebooks only
Below you’ll find a sample of the config that does just this
html <requesthandler name="/ebook" class="solr.SearchHandler"> <lst name="defaults"> <str name="wt">json</str> <!-- Return response as json --> <str name="fq">media_type:ebook</str> <!-- Filter on all items of media_type ebook --> <str name="qf">combined_author_title</str> <!-- Search in the field combined_author_title --> </lst> </requesthandler> For our own query, we’ll need other options that Solr provides
This is called the edismax query
This comes by default with options to boost your results using phrases, but also for boosting using ratings, release dates, etc
Below an image giving you an idea of what the query should do
Next, I’ll show you how this translates into the Solr configuration html <requesthandler name="/select" class="solr.SearchHandler"> <lst name="defaults"> <str name="wt">json</str> <str name="ps">2</str> <str name="mm">3</str> <str name="pf">author^4 title^2</str> <str name="pf2">author^4 title^2</str> <str name="pf3">author^4 title^2</str> <str name="bq">author_full^5 title_full^5</str> <str name="boost">product(div(def(rating,4),4),recip(ms(NOW/DAY,releasedate),3.16e-11,1,1),log(product(5,sales_pastyear)))</str> <str name="qf">combined_author_title</str> <str name="defType">edismax</str> <str name="lowercaseOperators">false</str> </lst> </requesthandler> I am not going over all the different parameters
For multi-term queries we use phrases
These are configured with pf, pf2, and pf3
Also, mm is used for multi-term queries
This has to do with the number of terms that have to match
So if you use three terms, they all have to match
The edismax query also supports using AND/OR when you need more control over what terms to match
With lowercaseOperators we prevent that and/or in lowercase are also used to create your own boolean query
With respect to boosting there is the bq, these numbers are added to the score
With the field boost, we do a multiplication
Look also at the diagram
Also notice the bq has text related scores, while the boost has numeric scores
That is about it for now
I think it is good to look at the differences between Solr en Elasticsearch
I like the idea of creating a query with Solr
Of course, you can do the same with Elasticsearch
The json API for creating a query is really flexible, but you have to create the constructs used in Solr yourselfHere is an easy Step by Step guide to installing PySpark and Apache Spark on MacOS
Step 1: Get Homebrew Homebrew makes installing applications and languages on a Mac OS a lot easier
You can get Homebrew by following the instructions on its website
In short you can install Homebrew in the terminal using this command: apache /usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)" Step 2: Installing xcode-select Xcode is a large suite of software development tools and libraries from Apple
In order to install Java, and Spark through the command line we will probably need to install xcode-select
Use the blow command in your terminal to install Xcode-select: xcode-select –install You usually get a prompt that looks something like this to go further with installation: You need to click “install” to go further with the installation
Step 3: DO NOT use Homebrew to install Java! The latest version of Java (at time of writing this article), is Java 10
And Apache spark has not officially supported Java 10! Homebrew will install the latest version of Java and that imposes many issues! To install Java 8, please go to the official website: https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html Then From “Java SE Development Kit 8u191” Choose: Mac OS X x64 245.92 MB jdk-8u191-macosx-x64.dmg To download Java
Once Java is downloaded please go ahead and install it locally
Step 3: Use Homebrew to install Apache Spark To do so, please go to your terminal and type: brew install apache-spark Homebrew will now download and install Apache Spark, it may take some time depending on your internet connection
You can check the version of spark using the below command in your terminal: pyspark –version You should then see some stuff like below: Step 4: Install PySpark and FindSpark in Python To be able to use PyPark locally on your machine you need to install findspark and pyspark If you use anaconda use the below commands: apache #Find Spark Option 1: conda install -c conda-forge findspark #Find Spark Option 2: conda install -c conda-forge/label/gcc7 findspark #PySpark: conda install -c conda-forge pyspark If you use regular python use pip install as: pip install findspark pip install pyspark Step 5: Your first code in Python After the installation is completed you can write your first helloworld script: apache import findspark from pyspark import SparkContext from pyspark.sql import SparkSession findspark.init() sc = SparkContext(appName="MyFirstApp") spark = SparkSession(sc) print("Hello World!") sc.close() #closing the spark sessionIn recent years there is much attention to bringing machine learning models in production whereas up to a few years ago the results of machine learning came into slides or some dashboards
Bringing machine learning in production is important to integrate the outputs of machine learning with other systems
What does “bring in production” mean
To bring a machine learning in production means to run the machine learning model more often and integrate and use the model’s output in other systems
There are some ways that you can bring your machine learning models in production, such as: To build a web-service around it and use it in real time (API calls, microservice architecture) Schedule your code to be run regularly (such as Oozie and Airflow) Stream analytics (such as Spark Streams) for lambda/kappa architect The focus of this article is to discuss the first way of going on production
This method is usually used in web-based environments and microservices architect
Python and modeling In this article, we build up a sample machine learning model for Online Shoppers’ Purchasing Intention Dataset Data Set, available and discussed in https://bit.ly/2UnSeRX
In the below you can find the code for the data preparation and modeling: python import pandas as pd # to use dataframes and etc
import numpy as np #for arithmatic operationms from time import strptime #to convert month abbrivations to numeric values from sklearn.model_selection import train_test_split #to split up the samples from sklearn.tree import DecisionTreeRegressor #ression tree model from sklearn.metrics import confusion_matrix # to check the confusion matrix and evaluate the accuracy #reading the data dataset=pd.read_csv(".../online_shoppers_intention.csv") #preparing for split df=dataset.drop("Revenue",axis=1) y=dataset["Revenue"].map(lambda x: int(x)) X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2) #making a copy (to be able to change the values) X_train=X_train.copy() X_test=X_test.copy() #data prep phase def dataPrep(localData): #The problem is "June"is the full month name and the rest abbriviation --> we turn all in abbr
localData["Month"]= localData["Month"].map(lambda x: str.replace(x,"June","Jun")) # Our model doesn't ingest text, we transform it to int localData["Month"]= localData["Month"].map(lambda x: strptime(x,"%b").tm_mon) # The weeknd should also be turned into int localData["Weekend"]= localData["Weekend"].map(lambda x: int(x)) # turning string to int localData["VisitorType"] = localData["VisitorType"].astype('category').cat.codes return localData #Sending the data through data prep phase X_train=dataPrep(X_train) X_test=dataPrep(X_test) #define the regression tree regr_tree = DecisionTreeRegressor(max_depth=200) #fiting the tree with the training data regr_tree.fit(X_train, y_train) # running the predictions pred=regr_tree.predict(X_test) # looking at the confusion matrix confusion_matrix(y_test,pred) For data scientists, the above code should be very familiar
We read the data, do very few data wranglings and model it with decision trees
Save the model The next step which does not appear in data scientists workflow regularly is to save the model on hard-drive
This step is necessary if you bring your python code in production
In below you can see how “joblib” can be of assistant to do this: python from sklearn.externals import joblib joblib.dump(regr_tree, '.../model3.pkl') Build your Flask back-end If you are not familiar with building back-end programs and RESTful APIs, I highly recommend reading https://bit.ly/2AVTwxW and other related materials
But in short Web Services, and RESTful APIs are servers provide functions (on the server)
The application can remotely call those function and get the outputs back
In our example, we call our machine learning model from anywhere through internet and TCPIP protocol
Once the model is called with the data, the result of classification is back to the client or the computer which has already called the machine learning model
Discussing details about web-services and web APIs are beyond the scope of this article but you can find many interesting articles on this by some internet search
In below we use Flask to build the webservice around the machine learning model
python from flask import Flask, request from flask_restful import Resource, Api from sklearn.externals import joblib import pandas as pd app = Flask(__name__) api = Api(app) class Classify(Resource): def get(self): # get is the right http verb because it caches the outputs and is faster in general data = request.get_json() # greading the data data1 = pd.DataFrame.from_dict(data,orient='index') # converting data into DataFrame (as our technique does not ingest json) data1=data1.transpose() # once Json is converted to DataFrame is not columnar, we need to convert it to columnar model = joblib.load('../model3.pkl') # loading the model from disk result = list(model.predict(data1)) # conversion to list because numpy.ndarray cannot be jsonified return result # returning the result of classification api.add_resource(Classify, '/classify') app.run(port=5001) Test your API You can use various techniques to test if the back-end works
I use Postman software to test if the API is working
You need to consider that we made a GET request in our Flask application
The motivation behind choosing GET request is the ability of the webservers to cash the results helping with the speed of the webservice
Another consideration is we send the data in JSON format (in the format after data preparation phase) for the call and the results are back also in JSON
python { "Administrative":0, "Administrative_Duration": 0.0, "Informational": 0, "Informational_Duration": 0.0, "ProductRelated": 1, "ProductRelated_Duration": 0.0, "BounceRates": 0.2, "ExitRates": 0.2, "PageValues": 0.0, "SpecialDay": 0.0, "Month": 5, "OperatingSystems": 2, "Browser": 10, "Region": 5, "TrafficType": 1, "VisitorType": 87093223, "Weekend":0 } Microservices architect: I personally like to bring machine learning in production using RESTful APIs and the motivation behind it is because of microservices architect
The microservices architect lets developers to build up loosely coupled services and enables continuous delivery and deployment
Scaling up To scale up your webservice there are many choices of which I would recommend load balancing using KubernetesAt my current project, we’ve been building three different applications
All three applications are based on Spring Boot, but have very different workloads
They’ve all reached their way to the production environment and have been running steadily for quite some time now
We do regular (weekly basis) deployments of our applications to production with bug fixes, new features, and technical improvements
The organisation has a traditional infrastructure workflow in the sense that deployments to the VM instances on acceptance and production happen via the (remote) hosting provider
The hosting provider is responsible for the uptime of the applications and therefore they keep an eye on system metrics through the usage of their own monitoring system
As a team, we are able to look in the system, but it doesn’t say much about the internals of our application
In the past, we’ve asked to add some additional metrics to their system, but the system isn’t that easy to configure with additional metrics
To us as a team runtime statistics about our applications and the impact our changes have on the overall health are crucial to understanding the impact of our work
The rest of this post will give a short description of our journey and the reasons why we chose the resulting setup
Spring Boot Actuator and Micrometer If you’ve used Spring Boot before you’ve probably heard of Spring Boot Actuator
Actuator is a set of features that help you monitor and manage your application when it moves away from your local development environment and onto a test, staging or production environment
It helps expose operational information about the running application – health, metrics, audit entries, scheduled task, env settings, etc
You can query the information via either several HTTP endpoints or JMX beans
Being able to view the information is useful, but it’s hard to spot trends or see the behaviour over a period of time
When we recently upgraded our projects to Spring Boot 2 my team was pretty excited that we were able to start using micrometer a (new) instrumentation library powering the delivery of application metrics
Micrometer is now the default metrics library in Spring Boot 2 and it doesn’t just give you metrics from your Spring application, but can also deliver JVM metrics (garbage collection and memory pools, etc) and also metrics from the application container
Micrometer has several different libraries that can be included to ship metrics to different backends and has support for Prometheus, Netflix Atlas, CloudWatch, Datadog, Graphite, Ganglia, JMX, Influx/Telegraf, New Relic, StatsD, SignalFx, and Wavefront
Because we didn’t have a lot of control over the way our applications were deployed we looked at the several different backends supported by micrometer
Most of the above backends work by pushing data out to a remote (cloud) service
Since the organisation we work for doesn’t allow us to push this ‘sensitive’ data to a remote party we looked at self-hosted solutions
We did a quick scan and started with looking into Prometheus (and Grafana) and soon learned that it was really easy to get a monitoring system up and we had a running system within an hour
To be able to use Spring Boot Actuator and Prometheus together you need to add two dependencies to your project: html <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-actuator</artifactId> </dependency> <dependency> <groupId>io.micrometer</groupId> <artifactId>micrometer-registry-prometheus</artifactId> </dependency> Actuator has an endpoint available for prometheus to scrape but it’s not exposed by default, so you will need to enable the endpoint by means of configuration
In this case, I’ll do so via the application.properties
html management.endpoint.prometheus.enabled=true management.endpoints.web.exposure.include=prometheus,info,health Now if you browse to http(s)://host(:8080)/actuator/prometheus you will see the output that prometheus will scrape to get the information from your application
A small snippet of the information provided by the endpoint is shown below, but there is a lot more information that the prometheus endpoint will expose
html # HELP tomcat_global_sent_bytes_total # TYPE tomcat_global_sent_bytes_total counter tomcat_global_sent_bytes_total{name="http-nio-8080",} 75776.0 tomcat_global_sent_bytes_total{name="http-nio-8443",} 1.0182049E8 # HELP tomcat_servlet_request_max_seconds # TYPE tomcat_servlet_request_max_seconds gauge tomcat_servlet_request_max_seconds{name="default",} 0.0 tomcat_servlet_request_max_seconds{name="jsp",} 0.0 # HELP process_files_open The open file descriptor count # TYPE process_files_open gauge process_files_open 91.0 # HELP system_cpu_usage The "recent cpu usage" for the whole system # TYPE system_cpu_usage gauge system_cpu_usage 0.00427715996578272 # HELP jvm_memory_max_bytes The maximum amount of memory in bytes that can be used for memory management # TYPE jvm_memory_max_bytes gauge jvm_memory_max_bytes{area="nonheap",id="Code Cache",} 2.5165824E8 jvm_memory_max_bytes{area="nonheap",id="Metaspace",} -1.0 jvm_memory_max_bytes{area="nonheap",id="Compressed Class Space",} 1.073741824E9 jvm_memory_max_bytes{area="heap",id="PS Eden Space",} 1.77733632E8 jvm_memory_max_bytes{area="heap",id="PS Survivor Space",} 524288.0 jvm_memory_max_bytes{area="heap",id="PS Old Gen",} 3.58088704E8 Now that everything is configured from the application perspective, let’s move on to Prometheus itself
Prometheus Prometheus is an open-source systems monitoring and alerting toolkit originally built at SoundCloudand now part of the Cloud Native Computing Foundation
To get a better understanding of what prometheus really is let us take a look at an architectural diagram
(Source: https://prometheus.io/docs/introduction/overview/) The prometheus server contains of a set of 3 features: A time series database A retrieval component which scrapes its targets for information An HTTP server which you can use to query information stored inside the time series database To make it even more powerful there are some additional components which you can use if you want: An alert manager, which you can use to send alerts via Pagerduty, Slack, etc
A push gateway in case you need to push information to prometheus instead of using the default pull mechanism Grafana for visualizing data and creating dashboards When looking at Prometheus the most appealing features for us were: no reliance on distributed storage; single server nodes are autonomous time series collection happens via a pull model over HTTP targets are discovered via service discovery or static configuration multiple modes of graphing and dashboarding support To get up and running quickly you can configure prometheus to scrape some (existing) Spring Boot applications
For scraping targets, you will need to specify them within the prometheus configuration
Prometheus uses a file called prometheus.yml as its main configuration file
Within the configuration file, you can specify where it can find the targets it needs to monitor, specify recording rules and alerting rules
The following example shows a configuration with a set of static targets for both prometheus itself and our spring boot application
html global: scrape_interval: 15s # By default, scrape targets every 15 seconds
# Attach these labels to any time series or alerts when communicating with # external systems (federation, remote storage, Alertmanager)
external_labels: monitor: 'bootifull-monitoring' scrape_configs: - job_name: 'monitoring-demo' # Override the global default and scrape targets from this job every 10 seconds
scrape_interval: 10s metrics_path: '/actuator/prometheus' static_configs: - targets: ['monitoring-demo:8080'] labels: application: 'monitoring-demo' - job_name: 'prometheus' scrape_interval: 5s static_configs: - targets: ['localhost:9090'] As you can see the configuration is pretty simple
You can add specific labels to the targets which can, later on, be used for querying, filtering and creating a dashboard based upon the information stored within prometheus
If you want to get started quickly with Prometheus and have docker on your environment you can use the official docker prometheus image by running the following command and provide a custom configuration from your host machine by running: html docker run -p 9090:9090 -v /tmp/prometheus.yml:/etc/prometheus/prometheus.yml \ prom/prometheus:v2.4.3 Create, explore, and share dashboards with your team and foster a data driven culture
In the above example we bind-mount the main prometheus configuration file from the host system, so you can, for instance, use the above configuration
Prometheus itself has some basic graphing capabilities (as you can see in the following image), but they are more meant to be used when doing some ad-hoc queries
For creating an application monitoring dashboard Grafana is much more suited
Grafana So what is Grafana and what role does it play in our monitoring stack
Grafana allows you to query, visualize, alert on and understand your metrics no matter where they are stored
Create, explore, and share dashboards with your team and foster a data driven culture
The cool thing about Grafana is (next to the beautiful UI) that it’s not tied to Prometheus as its single data source like for instance Kibana is tied to Elasticsearch
Grafana can have many different data sources like AWS Cloudwatch, Elasticsearch, InfluxDB, Prometheus, etc
This makes it a very good option for creating a monitoring dashboard
Grafana talks to prometheus by using the PromQL query language
For Grafana there is also an official Docker image available for you to use
You can get Grafana up and running with a simple command
html docker run -p 3000:3000 grafana/grafana:5.2.4 Now if we connect Grafana with Prometheus as the datasource and install this excellent JVM Micrometer dashboard into Grafana we can instantly start monitoring our Spring Boot application
You will end up with a pretty mature dashboard that lets you switch between different instances of your application
If you want to start everything all at once you can easily use docker-compose
html version: "3" services: app: image: monitoring-demo:latest container_name: 'monitoring-demo' build: context: ./ dockerfile: Dockerfile ports: - '8080:8080' prometheus: image: prom/prometheus:v2.4.3 container_name: 'prometheus' volumes: - ./monitoring/prometheus/:/etc/prometheus/ ports: - '9090:9090' grafana: image: grafana/grafana:5.2.4 container_name: 'grafana' ports: - '3000:3000' volumes: - ./monitoring/grafana/provisioning/:/etc/grafana/provisioning/ env_file: - ./monitoring/grafana/config.monitoring depends_on: - prometheus I’ve put together a small demo project, containing a simple Spring Boot application and the above prometheus configuration, in a github repository for demo and experimentation purposes
Now if you want to generate some statistics run a small load test with JMeter or Apache Bench
Feel free to use/fork it!Are you still using Docker in production
Get over it! Serverless is the NEW future of the Cloud
But since the Cloud is still someone else’s computer, that needs to be managed too
And if it is sitting idle, you probably have to pay for it whether you like it or not
No server can be more easily managed than no server
Therefore: meet Serverless, a new paradigm that truly approaches the Pay-as-You-Go philosophy once promised by the Cloud
This talk explores Serverless, its impact on existing architectures, and assesses it’s usability for Mobile Back-ends as a Service (MBaaS), Functions-as-a-Service (FaaS) and also for Microservices based architectures hosted in the cloud
Internet connectivity permitting, there will be demos tooRedux is a data store container for JavaScript apps
I’m currently on a team, that’s building an application for the government in three months
We use React and Redux in the front-end
This combination works well for us
In this blog I’ll explain a little bit about what we have done so far and how we implemented Redux
We’re building a Content Management System (CMS) for editors (users) to enter information and later disclose it through an API
In order to show and edit the data in a clear way for the users in the front-end, we want to have the display of the data (rendered UI components) linked to the data itself
The displayed data should be re-rendered when the data changes
Redux controlled React components come in handy for this! We store the data retrieved from the server through asynchronous calls in the data store container of Redux
When the user uses the user interface by going to different pages, tabs or saving forms, a call is triggered to fetch the latest data from the server
This data (partially) overwrites the old data in the store and brings the store to a new state.
When data is changed in the store the parts (components) of the user interface that display the data are re-rendered
The actions of the user trigger (actually dispatch) Redux Actions
Responses from the server also dispatch Redux Actions
These Actions contain a type and a payload
The type specifies which kind of action is triggered
The payload contains the new data, which can be the response (body) of the server
This data needs to be put in the data store
This is done by a Reducer
The Reducer is responsible of changing the state of the store
Actually it takes (part of) the previous state and switches over the type of Action, manipulates the data from it accordingly and returns the new state
It uses the payload of the Action to create a new state
It can do anything more you want too, like sorting arrays or execute other functions on the state
This way you have a time line of the state and its changes due to Actions
javascript // actions.js export const createResource = formData =&gt; ({ type: 'CREATE_RESOURCE_REQUEST_ACTION', payload: formData });</p> <p>export const createResourceSuccess = response =&gt; ({ type: 'CREATE_RESOURCE_RESPONSE_SUCCESS_ACTION', payload: response });</p> <p>export const createResourceFailed = err =&gt; ({ type: 'CREATE_RESOURCE_RESPONSE_FAILED_ACTION', payload: err }); javascript // reducer.js const initialState = { isFetching: false, createdResources: [], };</p> <p>export const reducer = (state = initialState, action) =&gt; { switch (action.type) { case 'CREATE_RESOURCE_REQUEST_ACTION': return { ...state, // copies the previous state isFetching: true // overwrites this field in the copied state }; case 'CREATE_RESOURCE_RESPONSE_SUCCESS_ACTION': return { isFetching: false, createdResources: [...state.createdResources, action.payload] }; default: return state; } }; In a complex, big application all these can be split, for example by a feature
So you can make Actions, a Reducer and React components for a particular feature
This way you can easily order and maintain your application
There are also Redux DevTools and extensions for multiple browsers to look into the data store and debug while you code
In our application we use Saga’s
When a user triggers an Action that needs to make a request to the server this is handled by a worker Saga
The worker Saga sends an asynchronous request to the server
Depending on the response of the server, i.e
a success or a failure, the Saga dispatches a new Action specifically for a success or failure response of the specific request
Then the Reducer catches this to handle the data from the response
javascript // saga.js import { call, put, takeLatest } from 'redux-saga/effects'</p> <p>export function* createResource(action) { const { formData } = action.payload; try { const response = yield call(axios.post, <code>https://amsterdam.luminis.eu/api/resources</code>, formData); yield put(actions.createResourceSuccess(response.data)); } catch (e) { yield put(actions.createresourceFailed(e)); } }</p> <p>export function* saga() { yield takeLatest('CREATE_RESOURCE_REQUEST_ACTION', createResource); } The great advantage of this is that all the data is easily accessible from a single data store
Also the data can be reused elsewhere in the user interface
For example, a user fills in a form field, then this data can be dynamically used to show data elsewhere (after calculations)
All this is done automatically as long the React components are connected to the Redux storeAny webdeveloper who has worked with Angular as their front-end framework lately has probably encountered the RxJS (reactive extensions for javascript) library
This is because the angular http module is dependent on RxJS and this modules’ various methods for http put, post and patch requests return a RxJS ‘Observable’, which is the key component and concept of RxJS
I was, until recently, used to working with promises to handle asynchronous Javascript operations
Although I was familiar with the observer design pattern in general, the way RxJS works and how to work with it confused me at first
I decided to write down my recent experience with RxJS in the form of this blog
This is partly to benefit my own learning process and partly hoping that it may aid that of the reader
Alternatively, I hope it will peak the readers’ interest in making use of RxJS when writing code
RxJS turns out to be an elegant and useful tool when structuring javascript applications that need to handle asynchronous processes
As such, I feel it is useful for any javascript developer to take note of
So what is a RxJS observable
Let’s start off with the most basic example of handling a http get request may look like in an Angular app, taken straight from the angular.io documentation: javascript getConfigResponse(): Observable<HttpResponse<Config>> { return this.http.get<Config>( this.configUrl, { observe: 'response' }); } Let’s try to put into words what this code is for: This is a piece of typescript that shows a method getConfigResponse(), that returns an observable of a HttpResponse , that maps to a Config object’
In this piece of code, this.http is the angular http client
If we look at the get() methods of this ‘class’ we can see that they return an observable object from the rxjs library
And, courtesy of typescript, we can check out the typescript definitions of Observable ourselves
At the very top of the code in the Observable.d.ts interface it says the following about the Observable<T> class: javascript /** * A representation of any set of values over any amount of time
This is the most basic building block * of RxJS
* @class Observable<T> */ The official RxJS documentation describes an observable like this: “An observable represents the idea of an invokable collection of future values or events”
So we can infer that the Observable class is at the heart of RxJS and that this building block always functions as a representation, or wrapper, of something else
Moreover, it represents the things it wraps as values or events, that are bound to become available at some unknown time in the future
Let’s explore some examples of RxJS observables used in code from an actual application! In the interface definition of Observable we see the definitions of some methods: subscribe() and pipe() are the most interesting ones and will be discussed shortly
Apparently, a RxJS observable can be interacted with via a limited set of methods defined on the Observable class
And how should I use this thing
javascript import { Observable } from 'rxjs/Rx'; @Injectable() export class AuthService { ...//other AuthService code private userSettings: UserSettings; ...//other AuthService code public getProfile(): Observable<UserSettings> { if (!this.userSettings) { return this.http.get<UserSettings>(this.config.userServiceUrl + '/users/usersettings', { withCredentials: true }).do(userSettings=> { this.userSettings= userSettings; }); } return Observable.create(this.userSettings); } } This code fragment is already illustrative of the powerful things we can do with RxJS observables
Here, we return an observable of some user’s userprofile settings (note that userId is not handled via url in this specific case)
If there are no user settings in memory yet, we assign the object returned from the http response to the userSettings variable of the class, then return an observable of the user settings for further interaction
If instead there are user settings in memory already, we simply instantiate a new observable of the existing userSettings with Observable.create()
The latter is a very handy aspect of RxJS observables: You can wrap anything in the observable and subsequently program against that observable in a reactive manner
Thus easily mixing synchronous with asynchronous code
The .do method is the other new concept in the above piece of code
This method is not present on Observable (that returns from this.http.get<UserSettings>()) itself
Rather this ‘operator’ gets imported from ‘rxjs/operators’
The official RxJS documentation describes pretty clearly what this specific operator is for: “Perform a side effect for every emission on the source Observable, but return an Observable that is identical to the source”
RxJS has a large number of operators similar to .do, that can filter, buffer, delay, map (etc., etc.) your observables and always create a new observable as output
Observable.pipe() that I mentioned before is present on the Observable object itself, but otherwise functions more or less the same as an operator in that it returns a new observable based on the input of an existing one
The new Observable in the case of .pipe () is projected to from the old one, by passing through the functions defined in the pipe
I’ll refer to this external article for readers who want more info on piping observables
Now let’s cover how to actually observe a RxJs Observable
This is where, gasp, observable.subscribe() comes into play
I’ll give a hypothetical example based on the code above for getProfile(): javascript @Component export class SomeViewComponent { constructor (private authService: AuthService){ } private userSettingsOnScreen(): void { this.authService.getProfile().subscribe(userSettings => { this.backGroundColor = userSettings.preferences.backGroundColor; }); } The subscribe method in the above example takes a function as method parameter
This function kind of acts like an observer in a classic observer design pattern and in fact the RxJS documentation actually calls this function the ‘observer’ as well
This makes a lot of sense given that: You can subscribe multiple of these functions to the same Observable You can unsubscribe these methods from the Observable The ‘observer’ function(s) can listen to only 3 kind of ‘events’ emitted by the Observable and those are: ‘onnext’: The Observable delivering it’s wrapped value (as in the above example)
‘onerror’: The Observable delivering an error result due to an unhandled exception
‘oncompleted’: All onnext calls have been completed and the observer function has finished
The onnext callback confused me for a while, but it makes more sense knowing you can do this (example taken from here): javascript public Listener(name: string): void { return { onNext: function (v) { console.log(name + ".onNext:", v) }, onError: function (err) { console.log(name + ".onError:", err.message) }, onCompleted: function () { console.log(name + ".onCompleted!") }, } let obs = Observable.of(1, 2) obs.subscribe(Listener("obs")); //output obs.onNext: 1 obs.onNext: 2 obs.onCompleted! Not only can we subscribe multiple times to a single Observable but, as the example above should clarify, we can also observe multiple things in sequence with the same Observable
Pretty nice! I'll round up this write-up with showing error handling in RxJS: javascript // Try three times with the retry operator, to get the data and then give up var observableOfResponse = this.http.get('someurl').retry(2); var subscription = observableOfResponse.subscribe( data => console.log(data), err => console.log(err) ); // catch a potential error after 2 retries with the catch operator var observableOfResponse = get('someurl').retry(2).catch(trySomethingElse()); var subscription = observableOfResponse.subscribe( data => { // Displays the data from the URL or cached data console.log(data); } ); Here we see that we can either let the error (maybe a code 500?) happen like in the top example, or catch and manage it like in the bottom one
It’s pretty intuitive, once you know what operators are available
Note that in the first scenario an exception still gets thrown
Rounding up RxJS is an extensive subject and deserves more attention than I’m able to give in this little blog
I have tried in this article to condense a vast amount of information into a short overview that aimed to cover the core ideas of the library but at the same time be very practical: The information above is enough to start working with it immediately
So rounding up, I’ll say good luck and have fun to anyone using RxJS in their front-end
I know I willSoftware is eating your smartphone
Or something along those lines
So it is not surprising there is a lot to be said about mobile app development and especially about all the non-native frameworks that are around these days
We know it’s hard to find your way in all these voices so we have made a neat overview of our experience and findings for your convenience
But before we dive into it, let’s start with some clichés: Cliché #1: The world of mobile app development is ever changing
Face ID, ARCore, Jetpack, Swift 4: There is no shortage of ‘new’ things for mobile app users or developers and keeping track of everything can easily become a day-job
Thankfully there is a large community of writers (like me) that will help you navigate the landscape
But your hands are on the wheel and it’s your car when you crash it into a tree
In other words, you have to make the tough choices
The best thing we can do is guide you along the way
Cliché #2: Software development is about balancing trade-offs
A lot of frameworks and technologies for software development are selling their product or service like it’s the best thing since sliced bread
Especially the last couple of years I see more and more of their websites turned into a polished and well-tuned commercial website
With marketing slogans, customer blurbs, and video commercials
But every practiced software developer knows there is no such thing as a holy grail
Context is everything and when searching for the best tool for the job, it’s always good to look at both
Otherwise, you will be lured into a solution that has too many trade-offs because of clever, superficial marketing campaigns
Cliché #3: Android is better/worse than iOS
The usefulness of this discussion is zero
When you’re building a serious mobile app there is no “or” between Android and iOS
Sure, they have different market shares but you can’t neglect one over the other because of personal preference
Any serious app developer should target both
The hybrid: Toaster or skinjob
I think the Battlestar Galactica analogy is somewhat apt
Android as the sturdy and rough metal robots
iOS as the most human-like but still artificial machine
Whatever your point of view, supporting competing platforms for your app can be a bit frustrating
Writing and maintaining the same set of features multiple times has a real impact on development costs and time to market
It feels like waste so it’s not surprising a lot of hybrid app frameworks are available to mitigate these deficiencies
When you’re willing to leave the native ecosystem behind, another set of choices become available accompanied with their own pros and cons
Download the overview here At Luminis, we have developed a lot of mobile apps with native and various hybrid frameworks
We understand the difficulties, especially when you want to put things into context
To help you out we have summarized our insights in this high-level rundown
It’s especially useful when you have some functional requirements available and a rough idea of the user experience you seek
At the very least you will get some foreknowledge of the impact on user experience and development costs of these frameworksIn my latest project I have implemented all communication with my Elasticsearch cluster using the high level REST client
My next step was to setup and teardown an Elasticsearch instance automatically in order to facilitate proper integration testing
This article describes three different ways of doing so and discusses some of the pros and cons
Please refer to this repository for implementations of all three methods
docker-maven-plugin This generic Docker plugin allows you to bind the starting and stopping of Docker containers to Maven lifecycles
You specify two blocks within the plugin; configuration and executions
In the configuration block, you choose the image that you want to run (Elasticsearch 6.5.3 in this case), the ports that you want to expose, a health check and any environment variables
See the snippet below for a complete example: java <plugin> <groupId>io.fabric8</groupId> <artifactId>docker-maven-plugin</artifactId> <version>${version.io.fabric8.docker-maven-plugin}</version> <configuration> <imagePullPolicy>always</imagePullPolicy> <images> <image> <alias>docker-elasticsearch-integration-test</alias> <name>docker.elastic.co/elasticsearch/elasticsearch:6.5.3</name> <run> <namingStrategy>alias</namingStrategy> <ports> <port>9299:9200</port> <port>9399:9300</port> </ports> <env> <cluster.name>integration-test-cluster</cluster.name> </env> <wait> <http> <url>http://localhost:9299</url> <method>GET</method> <status>200</status> </http> <time>60000</time> </wait> </run> </image> </images> </configuration> <executions> <execution> <id>docker:start</id> <phase>pre-integration-test</phase> <goals> <goal>start</goal> </goals> </execution> <execution> <id>docker:stop</id> <phase>post-integration-test</phase> <goals> <goal>stop</goal> </goals> </execution> </executions> </plugin> You can see that I’ve bound the plugin to the pre- and post-integration-test lifecycle phases
By doing so, the Elasticsearch container will be started just before any integration tests are ran and will be stopped after the integration tests have finished
I’ve used the maven-failsafe-plugin in order to trigger the execution of tests ending with *IT.java in the integration-test lifecycle phase
Since this is a generic Docker plugin, there is no special functionality to easily install Elasticsearch plugins that may be needed during your integration tests
You could however create your own image with the required plugins and pull that image during your integration tests
The integration with IntelliJ is also not optimal
When running an *IT.java class, IntelliJ will not trigger the correct lifecycle phases and will attempt to run your integration test without creating the required Docker container
Before running an integration test from IntelliJ, you need to manually start the container from the “Maven projects” view by running the docker:start commando: After running, you will also need to run the docker:stop commando to kill the container that is still running
If you forget to kill the running container and want to run a mvn clean install later on it will fail, since the build will attempt to create a container on the same port – as far as I know, the plugin does not allow for random ports to be chosen
Pros: Little setup, only requires configuration of one Maven plugin Cons: No out of the box functionality to start the Elasticsearch instance on a random port No out of the box functionality to install extra Elasticsearch plugins Extra dependency in your build pipeline (Docker) IntelliJ does not trigger the correct lifecycle phases elasticsearch-maven-plugin This second plugin does not require Docker and only needs some Maven configuration to get started
See the snippet below for a complete example: java <plugin> <groupId>com.github.alexcojocaru</groupId> <artifactId>elasticsearch-maven-plugin</artifactId> <version>${version.com.github.alexcojocaru.elasticsearch-maven-plugin}</version> <configuration> <version>6.5.3</version> <clusterName>integration-test-cluster</clusterName> <transportPort>9399</transportPort> <httpPort>9299</httpPort> </configuration> <executions> <execution> <id>start-elasticsearch</id> <phase>pre-integration-test</phase> <goals> <goal>runforked</goal> </goals> </execution> <execution> <id>stop-elasticsearch</id> <phase>post-integration-test</phase> <goals> <goal>stop</goal> </goals> </execution> </executions> </plugin> Again, I’ve bound the plugin to the pre- and post-integration-test lifecycle phases in combination with the maven-failsafe-plugin
This plugin provides a way of starting the Elasticsearch instance from IntelliJ in much the same way as the docker-maven-plugin
You can run the elasticsearch:runforked commando from the “Maven projects” view
However in my case, this started the container and then immediately exited
There is also no out of the box possibility of setting a random port for your instance
However, there are solutions to this at the expense of having a somewhat more complex Maven configuration
Overall, this is a plugin that seems to provide almost everything we need with a lot of configuration options
You can automatically install Elasticsearch plugins or even bootstrap your instance with data
In practice I did have some problems using the plugin in my build pipeline
Upon downloading the Elasticsearch ZIP the build would sometimes fail, or in other cases when attempting to download a plugin
Your mileage may vary, but this was reason for me to keep looking for another solution
Which brings me to plugin number three
Pros: Little setup, only requires configuration of one Maven plugin No extra external dependencies High amount of configuration possible Cons: No out of the box functionality to start the Elasticsearch instance on a random port Poor integration with IntelliJ Seems unstable testcontainers-elasticsearch This third plugin is different from the other two
It uses a Java testcontainer that you can configure through Java code
This gives you a lot of flexibility and requires no Maven configuration
Since there is no Maven configuration, it does require some work to make sure the Elasticsearch container is started and stopped at the correct moments
In order to realize this, I have extended the standard SpringJUnit4ClassRunner class with my own ElasticsearchSpringRunner
In this runner, I have added a new JUnit RunListener named JUnitExecutionListener
This listener defines two methods testRunStarted and testRunFinished that enable me to start and stop the Elasticsearch container at the same points in time that the pre- and post-integration-test Maven lifecycle phases would
See the snippet below for the implementation of the listener: java public class JUnitExecutionListener extends RunListener { private static final String ELASTICSEARCH_IMAGE = "docker.elastic.co/elasticsearch/elasticsearch"; private static final String ELASTICSEARCH_VERSION = "6.5.3"; private static final String ELASTICSEARCH_HOST_PROPERTY = "spring.elasticsearch.rest.uris"; private static final int ELASTICSEARCH_PORT = 9200; private ElasticsearchContainer container; private RunNotifier notifier; public JUnitExecutionListener(RunNotifier notifier) { this.notifier = notifier; } @Override public void testRunStarted(Description description) { try { if (System.getProperty(ELASTICSEARCH_HOST_PROPERTY) == null) { log.debug("Create Elasticsearch container"); int mappedPort = createContainer(); System.setProperty(ELASTICSEARCH_HOST_PROPERTY, "localhost:" + mappedPort); String host = System.getProperty(ELASTICSEARCH_HOST_PROPERTY); RestAssured.basePath = ""; RestAssured.baseURI = "http://" + host.split(":")[0]; RestAssured.port = Integer.parseInt(host.split(":")[1]); log.debug("Created Elasticsearch container at {}", host); } } catch (Exception e) { notifier.pleaseStop(); throw e; } } @Override public void testRunFinished(Result result) { if (container != null) { String host = System.getProperty(ELASTICSEARCH_HOST_PROPERTY); log.debug("Removing Elasticsearch container at {}", host); container.stop(); } } private int createContainer() { container = new ElasticsearchContainer(); container.withBaseUrl(ELASTICSEARCH_IMAGE); container.withVersion(ELASTICSEARCH_VERSION); container.withEnv("cluster.name", "integration-test-cluster"); container.start(); return container.getMappedPort(ELASTICSEARCH_PORT); } } It will create an Elasticsearch Docker container on a random port for use by the integration tests
The best thing about having this runner is that it works perfectly fine in IntelliJ
Simply right-click and run your *IT.java classes annotated with @RunWith(ElasticsearchSpringRunner.class) and IntelliJ will use the listener to setup the Elasticsearch container
This allows you to automate your build pipeline while still keeping developers happy
Pros: Neat integration with both Java and therefore your IDE Sufficient configuration options out of the box Cons: More complex initial setup Extra dependency in your build pipeline (Docker) In summary, all three of the above plugins are able to realize the goal of starting an Elasticsearch instance for your integration testing
For me personally, I will be using the testcontainers-elasticsearch plugin going forward
The extra Docker dependency is not a problem since I use Docker in most of my build pipelines anyway
Furthermore, the integration with Java allows me to configure things in such a way that it works perfectly fine from both the command line and the IDE
Feel free to checkout the code behind this article, play around with the integration tests that I’ve setup there and decide for yourself which plugin suits your needs bestCRISP-DM stands for the cross-industry standard process for data mining which an open standard for data mining existing since 1999 and proposed by IBM
CRISP-DM suggests a set of steps to perform data mining projects to maximize the success of the project and minimize the common faults happening in any data-oriented projects
Later in 2015, an extended version of CRISP-DM is proposed by IBM so-called ASUM-DM (the Analytics Solutions Unified Method)
ASUM-DM is an extension of CRISP-DM having the same steps in data mining (development) plus an operational / deployment part
I personally pretty much a fan of CRISP-DM and ASUM-DM
In my daily consultancy life, I stick to the steps provided because it minimizes the risk of project failures
I believe following CRISP-DM and ASUM-DM methodologies properly distinguishes a senior data scientist from junior ones
Many data-scientists/data-miners have the tendency to quickly model the data to reach the insights ignoring proper understanding of the problem and the right data preparation
That is the reason CRISP-DM comes with clear steps that taking them minimizes the common failure in any data science/data mining projects
Being a data miner and later a data scientist for over 12 years, I believe CRISP-DM misses one crucial step
By writing this article I intend to add a new step in CRISP-DM/ASUM-DM which comes from some years of experience in data science
CRISP-DM Methodology CRISP-DM suggests these steps for data-mining/data-science: (1) Business understanding: which means the data scientist should properly understand the business of his/her client
Why is analytics important to them
How analytics can be of a great value for the business and so on
(2) Data understanding: which means the data scientist should go through all the fields within the data to understand the data like a domain expert
With a poor understanding of the data, a data scientist can barely provide high-quality data science solutions
(3) Data preparation: which is the most time-consuming step in any data science project being data preparation in the way that a model can ingest and understand it
(4) Modeling: the magical phase turning the raw-data to (actionable) insights
With recent advances in data science and the toolings such as AutoML and deep learning, modeling is less complicated as before
(4) Evaluation: checking the accuracy of the model which metrics such as a confusion matrix, RMSE, MAPE, and MdAPE
(5) Deployment: which means making the use of the model with the new data
As you can see in the picture, CRISP-Dm is an iterative approach, matches quite well with agile methodology
The steps can be taken in parallel and they are flexible enough to be redone quickly once there is a modification in any previous steps
ASUM-DM methodology: ASUM-DM adds a new deployment/operation wing to CRISP-DM
The development phase stays the same as CRISP-DM however in deployment new facets are added such as collaboration, version control, security, and compliance
The forgotten step in CRISP-DM and ASUM-DM: CRISP-DM repeats itself in ASUM-DM as the development part however it misses an important step being data validation
My CRISP-DM version looks like this
Why data validation
Data validation happens immediately after data preparation/wrangling and before modeling
it is because during data preparation there is a high possibility of things going wrong especially in complex scenarios
Data validation ensures that modeling happens on the right data
faulty data as input to the model would generate faulty insight! How is data validation done
Data validation should be done by involving minimum one external person who has a proper understanding of the data and business
In my situation is usually my clients who technically good enough to check my data
Once I go through data preparation and just before data modeling, I usually make data visualization and give my newly prepared data to the client
The clients with the help of SQL queries or any other tools try to validate if my output contains no error
Combing CRISP-DM/ASUM-DM with the agile methodology, steps can be taken in parallel meaning you do not have to wait for the green light for data validation to do the modeling
But once you get feedback from the domain expert that there are faults in the data, you need to correct the data by re-doing the data-preparation and re-model the data
What are the common causes leading to a faulty output from data preparation
Common causes are: 1
Lack of proper understanding of the data, therefore, the logic of the data preparation is not correct
2
Common bugs in programming/data preparation pipeline that lead to a faulty output
3
Data formats that make some troubles within the data-preparation step and generating faulty outputs with no error trace to be caught by the data scientist/engineer during the data-preparation
Conclusion: In this article, I would like to extend the CRISP-DM/ASUM-DM by adding a new step
The whole idea of these methodologies is to formalize the steps helping the data-scientists/data-miners to improve the success of the projects and reduce failures
In my CRISP-DM version, “data validation” step is added which ensures even more success of the project and prevents, even more, the failures and faults of any data-science/data-mining projectsLast year, at the end of summer, the project I was working on required a public REST API
During the requirements gathering phase we discussed the ‘level’ of our future REST API
In case you’re unfamiliar with Leonard Richardson’s REST maturity model I would highly recommend reading this article written by Martin Fowler about the model
In my opinion a public API requires really good documentation
The documentation helps to explain how to use the API, what the resource represents (explain your domain model) and can help to increase adoption of the API
If I have to consume an API myself I’m always relieved if there is some well written API documentation available
After the design phase we chose to build a Level 3 REST API
Documenting a level 3 REST api is not that easy
We looked at Swagger / OpenAPI, but in the 2.0 version of the spec, which was available at the time, it was not possible to design and or document link relations, which are part of the third level
After some research we learned there was a Spring project called Spring REST Docs, which allowed you to document any type of API
It works by writing tests for your API endpoints and acts as a proxy which captures the requests and responses and turns them into documentation
It does not only look at the request and response cycle, but actually inspects and validates if you’ve documented certain request or response fields
If you haven’t specified and documented them, your actual test will fail
This is really neat feature! It makes sure that your documentation is always in sync with your API
Using Spring REST Docs is pretty straight-forward
You can start by just adding a dependency to your Maven or Gradle based project
html <dependency> <groupId>org.springframework.restdocs</groupId> <artifactId>spring-restdocs-mockmvc</artifactId> <version>${spring.restdoc.version}</version> <scope>test</scope> </dependency> Now when you use for instance Spring MockMVC you can test an API resource by having the following code: html @Test public void testGetAllPlanets() throws Exception { mockMvc.perform(get("/planets").accept(MediaType.APPLICATION_JSON)) .andExpect(status().isOk()) .andExpect(jsonPath("$.length()",is(2))); } All the test does is performing a GET request on the /planets resource
Now to document this API resource all you need to do is add the document() call with an identifier, which will result in documentation for the /planets resource
html @Test public void testGetAllPlanets() throws Exception { mockMvc.perform(get("/planets").accept(MediaType.APPLICATION_JSON)) .andExpect(status().isOk()) .andExpect(jsonPath("$.length()",is(2))) .andDo(document("planet-list")); } Now when you run this test, Spring REST Docs will generate several AsciiDoc snippets for this API resource
Let’s inspect one of these asciidoc snippets
html [source,bash] ---- $ curl 'https://api.mydomain.com/v1/planets' -i -X GET \ -H 'Accept: application/hal+json' ---- Looks pretty neat right
It generates a nice example of how to perform a request against the API by using curl
It will show what headers are required or in case you want to send a payload how to pass the payload along with the request
Documenting how to perform an API call is nice, but it gets even better when we start documenting fields
By documenting fields in the request or response we will immediately start validating the documentation for missing fields or parameters
For documenting fields in the JSON response body we can use the responseFields snippet instruction
html @Test public void testGetPerson() throws Exception { mockMvc.perform(get("/people/{id}", personFixture.getId()) .accept(MediaTypes.HAL_JSON_VALUE)) .andExpect(status().isOk()) .andDo(document("people-get-example", pathParameters( parameterWithName("id").description("Person's id") ), links(halLinks(), linkWithRel("self").ignored() ), responseFields( fieldWithPath("id").description("Person's id"), fieldWithPath("name").description("Person's name"), subsectionWithPath("_links").ignored() )) ); } In the above example we have documented 2 fields: id and name
We can add a description, but also a type, specify if they are optional or we can even ignore specific sections like I did in the above example
Ignoring a section is possible in case you want to document them once since they will be available across multiple resources
Now if you are very strict with writing JavaDoc you might also want to consider using Spring Auto REST Docs
Spring Auto REST Docs uses introspection of you Java classes and POJOs to generate the field descriptions for you
It’s pretty neat, but I found some corner cases for when you use a hypermedia API
You can’t really create specific documentation for Link objects
The documentation comes from the Spring Javadocs itself, so we chose to leave auto rest docs out
Having a bunch of asciidoc snippets is nice, but it’s better to have some human readable format like HTML
This is where the maven asciidoctor plugin comes in
It has the ability to process the asciidoc files and turn it into a publishable format like HTML or PDF
To get the HTML output (also known as backend) all you need to do is add the maven plugin with the correct configuration
html <build> <plugins> ...
<plugin> <groupId>org.asciidoctor</groupId> <artifactId>asciidoctor-maven-plugin</artifactId> <version>1.5.3</version> <executions> <execution> <id>generate-docs</id> <phase>prepare-package</phase> <goals> <goal>process-asciidoc</goal> </goals> <configuration> <backend>html</backend> <doctype>book</doctype> </configuration> </execution> </executions> <dependencies> <dependency> <groupId>org.springframework.restdocs</groupId> <artifactId>spring-restdocs-asciidoctor</artifactId> <version>2.0.1.RELEASE</version> </dependency> </dependencies> </plugin> </plugins> Now to turn all the different asciidoc snippets into once single documentation page you can create an index.adoc file that aggregates the generated AsciiDoc snippets into a single file
Let’s take a look at an example: html = DevCon REST TDD Demo Jeroen Reijn; :doctype: book :icons: font :source-highlighter: highlightjs :toc: left :toclevels: 4 :sectlinks: :operation-curl-request-title: Example request :operation-http-response-title: Example response [[resources-planets]] == Planets The Planets resources is used to create and list planets [[resources-planets-list]] === Listing planets A `GET` request will list all of the service's planets
operation::planets-list-example[snippets='response-fields,curl-request,http-response'] [[resources-planets-create]] === Creating a planet A `POST` request is used to create a planet
operation::planets-create-example[snippets='request-fields,curl-request,http-response'] The above asciidoc snippet shows you how to write documentation in asciidoc and how to include certain operations and even how you can selectively pick certain snippets which you want to include
You can see the result in the Github pages version
The advantage of splitting the generation from the actual HTML production has several benefits
One that I found appealing myself is that by documenting the API in two steps (code and documentation) you can have multiple people working on writing the documentation
At my previous company we had a dedicated technical writer that wrote the documentation for our product
An API is also a product so you can have engineers create the API, tests the API and document the resources by generate the documentation snippets and the technical writer can then do their own tick when it comes to writing good readable/consumable content
Writing documentation is a trade by itself and I have always liked the mailchimp content style guide for some clear guidelines on writing technical documentation
Now if we take a look at the overall process we will see it integrates nicely into our CI / CD pipeline
All documentation is version control managed and part of the same release cycle of the API itself
If you want to take look at a working example you can check out my DevCon REST TDD demo repositoryon github or see me use Spring Rest Docs to live code and document an API during my talk at DevConSometimes your Product Owner or the business wants to know how much work there is on the Product Backlog
They want to get an idea for the Roadmap ahead
Or perhaps to acquire some (internal) funds
Or as a Scrum Team, you want to know what is coming
So there could be several reasons why you want to get an estimation of unestimated Product Backlog Items (PBI)
For this, there is a method called ‘magic estimation’
In this blog post, I will describe a method and my experience with it as a Scrum Master
I have done it with my Scrum Team for several Product Backlogs
We were already developing the products, so we had a baseline and feeling with the products and our estimations
First things first Preparing this session properly makes the time the whole team sits together more efficient
A boundary condition is to have the titles of the Product Backlog Items clearly formulated
There must be as little room for interpretation as possible
Print for every PBI the title on paper
Then cut this so you have a small strip for every PBI
Make sure your team has a baseline
Otherwise, determine one in the session together
Session Lay down one set of Fibonacci numbers of the planning poker cards in order on a table including the question mark
Leave enough space between the cards
Give every team member a set of about the same amount of strips of unique PBIs
Explain the process and rules to the team
It is not allowed to talk or practice non-verbal communication unless told so
If the product is not known to the team, let the Product Owner share her/his vision of the product
Let every team member, in turn, put every PBI of her/his stock at the number at which (s)he estimates it
You can let the team member read the title of the PBI first aloud
In my experience, this costs extra time, but the upside is that every team member has heard of every item on the Product Backlog
This way the team gets more feeling with what is out there
Everybody else is not allowed to give any kind of reaction
Write the number on the strip of paper
If someone does not know what is meant by a PBI, it can be placed at the question mark
This is treated later in the next step
Also write the question mark on the strip of the PBI
Perhaps the Product Owner wants to reformulate the title afterward to make it more clear
The PBIs placed at the question mark are now explained by the Product Owner
If the intention is clear, they can be divided amongst the group to be placed at a number
You can do this immediately or in the second round (see below)
Now starts the second round
This is done in total silence and without non-verbal communication
Every team member can simultaneously pick up a PBI and replace it
He can confirm the magic estimation by placing it at the same number or give it a different magic estimation by placing it at a different number
He also writes down the (new) number after the previous magic estimation
This way it is clear which PBIs have already passed round two
This is done until all PBIs have an extra number
Everybody can do it at her/his own speed, so some will replace more PBIs than others
But the session can continue faster this way
Now do another round
You can do more rounds if you want, but for us, three works well to get enough magic estimations per PBI to make a conclusion and for the session not to be too long
Roundup and draw conclusions in two steps: For PBIs that have changed slightly, for example, within a range of three following Fibonacci numbers, calculate the mean and write that down as the final magic estimation
This range can be [1,2,3] or [3,5,8]
If you think the distance between 3 and 8 is too big, you can also make ranges of a maximum of 3 story points, e.g
[2-5]
PBIs that have changed more than in the previous step need to be discussed centrally
Let every team member who has written a number on it explain the reason behind it in one sentence
Then try to discuss a common magic estimation
Keep this discussion compact, because it is not a regular refinement
If it is not possible to reach an agreement take the mean of the magic estimations that remain standing
Outcome All PBIs have a magic estimation
Everybody has an idea of the amount of work on the Product Backlog
But what is still unknown is the hidden work that surfaces when the team actually develops the PBIs
Keep that in mind, because it is a rougher estimation than the regular estimation
My sources of inspiration Your experiences with magic estimation
Please tell me yours in the comments
https://www.scrum.nl/blog/magic-estimation/ https://www.agilecockpit.com/magic-estimation/ https://campey.blogspot.com/2010/09/magic-estimation.html https://blog.mikepearce.net/2011/05/02/how-do-i-estimate-how-long-an-agile-project-will-take-before-we-start/Oops, I guess I should have tested that title better
I wasn’t expecting that many viewers to concurrently view this blog
If only I had performed load testing beforehand! Hopefully it’s not too late for you and your application
Learn from my mitsake and test how your application will react when many users access it
When building your application, you probably test your application in a lot of ways, such as unit testing or simply just running the application and checking if it does remotely what you expect it to do
If this succeeds you are happy
Hooray for you, it works for one person! The title of this blog also looked good when only one person was viewing it! Of course, you’re not in the business of making web applications that will only be used by just one person
Your app is going to be a success with millions of users
Can your application handle it
I don’t know, you don’t know…nobody knows! Read the rest of this blog post to see how we tested an application we are working on at Luminis and found out for ourselves
Why and how
Why
Here’s why! I work on a project team at Luminis and our job is to make sure that users of a certain type of connected device can always communicate with said device via their smart phones
There are thousands of these devices online at the moment and that number is just going to keep increasing
So instead of just hoping for the best, we decided to take matters into our own hands and see for ourselves just how robust and scalable our web applications really are
How
Here’s How: Apache JMeter JMeter WebSocket Plugin by my colleague Peter Doornbosch Docker Amazon Web Services (Elastic Container service, Fargate, S3 and CloudWatch) A lot of swearing when things don’t go the way you expect Apache JMeter I’ll let the JMeter doc speak for itself: “The Apache JMeter™ application is open source software, a 100% pure Java application designed to load test functional behavior and measure performance
It was originally designed for testing Web Applications but has since expanded to other test functions.” I’m sure that description got you in the mood to start using JMeter immediately (/endSarcasm)
The best way to look at JMeter for now is to think of it as a REST client that you can set up to do multiple requests at the same time instead of just performing one
You can create multiple threads that will run concurrently, configure the requests in many ways and view the results afterwards in many types of views
Download and install it from here: https://jmeter.apache.org/ When you startup JMeter, it looks like this: I know what you’re thinking
“Mine doesn’t/won’t look like that, I have a Windows PC”
Don’t worry, it works there too (provided you have Java installed)
It’s just Java! There are a lot of things you can do in JMeter, however I’m going to focus on the essentials you need to perform a load test on your REST endpoints
In the next section I will also show how to perform load tests if you use WebSockets
Let’s just ignore all of the buttons for a while and just focus on the item called “Test Plan”, which is highlighted in the first image
You can leave the name as it is, I personally never change it
I just change the name of the .jmx file, which is the extension for JMeter scripts
Thread Group Right clicking on the Test plan will bring up the context menu
The first thing you want to do is create a thread group, which will be used to create multiple threads making it possible to make requests in parallel: The most important settings here are: Number of threads (users): Number of threads that will be started when this test is started
What exactly these threads will do will be defined later on
Ramp-Up Period (in seconds): The number of seconds before all threads are started
JMeter will try to spread the start of the thread evenly within the ramp-up period
So if Ramp-Up is set to 1 second and Number of threads is 5, JMeter will start a thread every 0.2 seconds
Loop count:How many times you want the threads to repeat whatever it is you want it to do
I usually leave this set to 1 and just put loops later on in the chain
By default, the thread group is set with 1 thread, 1 second ramp up and 1 loop count
Let’s leave that like that for now
HTTP Request Sampler Now right click on the thread group and create a HTTP Request sampler: This should (maybe) look familiar! It kind of looks like every REST client ever, such as Postman
There’s a button labeled “advanced”
I never clicked it, neither should you (yet)
Fill in the following: Protocol [http]:http —- Can also be https if you have a certificate installed
Server name or IP: localhost —- It can also be a remote IP or your server name if you have one registered
Port number: 8151 —- Or any port your application is listening to
Method: GET —- This can also be any other HTTP method
If you select POST, you can also add the POST body
Path: <<path to an endpoint you want to test>> —- Example: /alexa/ temperature Parameters: Here you can add query parameters for your request, which would usually look like this in your URL /alexa/temperature?parameterName1=paramterValue1&parameterName2=paramterValue2 Body data: Where you can add the body data in whatever format you need to send your POST body data
Since we are going to test a GET endpoint, leave this empty for now
When you’re done, it should look like this: You could just press the play button now and it will perform exactly one GET request to the endpoint specified
However, you won’t get much feedback in JMeter at the moment
Let’s add a few more items to our test plan
HTTP Header Manager You might want to add some headers to your HTTP request
To do so, right click on the HTTP Request sampler and select HTTP Header Manager: Here you can add your HTTP headers
Some examples are “Authorization” where you might send an authentication token, or the “Content-Type” header when sending POST body data: View results We’re almost there! When running your JMeter script, you probably want to see the result of each action request
Right click on the Test Plan > Add > Listeners and add the following two items: View results in Tree View results in Table After running the script by pressing on the green play button, this is how these two views will display the results: Here we can immediately see that the request was successful due to the green icon next to it in the tree
The sampler results give us a lot of extra info, including the status code
We can also check the request we sent and the response data we get back by clicking on those tabs
Here we can see that the response data is 20.0, which is what I programmed my mock object to return: The table view looks like this: And if I run a request I know will fail, by sending an invalid token for example, then the table looks like this: If you want to clear all your results, click on the button at the top with the cog and two brooms (clear all)
User defined variables We’ve just added a lot of configuration in JMeter, however it is also possible to add a list of variables that you define yourself and use them throughout JMeter
This is needed for later on when we want to run JMeter as a script and not from the GUI
To do this, right click on the Test plan > Add > Config Element and select User defined variables
Let’s say I add the following variables that we previously entered into JMeter: Now that I have these four variables set up, I can refer to them in the following way: $(parameterName) This means that if I want to reference the “numberOfThreads” variable, I will add $(numberOfThreads) and it will use “1” in this case
With these variables, our Thread Group configuration looks like this: We’ll get back to these variables in a bit
CSV Data Set Config Up to this point, we have been running this request with a hardcoded authentication token
However, once we want to start running multiple requests and perform our load test, we might want to send a different authentication token per connection
This is possible by having a .csv file with these tokens and reading them in using the “CSV Data Set Config”
To add this, right click on the Test plan > Add > Config element and select “CSV Data Set Config”
Let’s say we have a .csv file that looks like this: 1, <<token1>> 2, <<token2>> 3, <<token3>> …etc If we set the CSV Data Set Config up in the following way, we can use the tokens per connection: Filename: Relative path to the .csv file (from the .jmx script) File-encoding: UTF-8 —- I’m not going to explain file encoding
Variable Names (comma-delimited): id, token Recycle on EOF?: False —- If set to true, if your numberOfThreads > numberOfTokensInCSVFile, it will startover from the top of the file when it runs out of tokens to use
Stop Thread on EOF?: True — This is because we don’t want to have more threads than the number of tokens we have in our .csv file What we’ve accomplished with this is that we’ve created two new variables, namely “id” and “token” which is available to use through JMeter
This is similar to the User Defined Variables and can be accessed in the same way ($(token) for example)
Our HTTP Header Manager config now looks like this: Now you should be ready to run multiple connections
Edit the numberOfThreads and rampUpTime in the User Defined Variables and see what happens! It probably won’t work in the first try and it will probably be your fault
Look at the errors in the view results listeners to see what the cause of the problem is
Run JMeter in Non-GUI mode We’ve seen how to setup a JMeter test and run it in the JMeter GUI
However, when you want to perform this at a large scale, you will probably want to run it on a server somewhere in a non-GUI mode as a script
In order to do this, you need to open your script one more time in JMeter and make a small adjustment
In the User Defined Variables, change the “numberOfThreads” value to: ${__P(numberOfThreads)} This means that the value for this variable will be passed on to the script when it is called
You can do this with all your variables, but for now let’s change this one
Navigate to the .jmx file location in the command line and run the following: apache ./apache-jmeter-3.3/bin/jmeter -n \ -t ./my_script.jmx \ -j ./ my_script.log \ -l ./my_script.xml \ -Jjmeter.save.saveservice.output_format=xml \ -Jjmeter.save.saveservice.response_data=true \ -Jjmeter.save.saveservice.samplerData=true \ -JnumberOfThreads=1 && \ echo -e "\n\n===== TEST LOGS =====\n\n" && \ cat my_script.log && \ echo -e "\n\n===== TEST RESULTS =====\n\n" && \ my_script.xml No, I don’t know what the Windows equivalent is of this
An explanation of what just happened: ./apache-jmeter-3.3/bin/jmeter: This is the location to my JMeter when running this script
You should set the path to your JMeter
-t: The location of the .jmx file
-j: The location the log file should be saved -l: The location the results .xml file should be saved save.saveservice.output_format: The format to save the results in
XML is a good choice because you can load it into the JMeter GUI results to view them
JnumberOfThreads: This is the value will be passing which will be mapped to the “numberOfThreads” variables in our script
If you want to pass more values, be sure to add a CAPITAL J before the variable name
The rest of the command is to just show output immediately to the command line
If you run this in the background, you can always follow the progress in the .log file
WebSockets I’ve started this story by explaining why our team looked into load testing
The connected devices use a WebSocket connection to connect to our backend application
In order to test this, we couldn’t use the HTTP Request Sampler
We needed a WebSocket Sampler
JMeter doesn’t come with such a sampler by default, so our colleague Peter Doornbosch decided to make his own JMeter Sampler
You can find instructions on how to install this sampler into JMeter on the GitHub page for his sampler: https://github.com/ptrd/jmeter-websocket-samplers
Be sure to give it a star! You can add the samplers in the same way you would add the HTTP Request Sampler
There are multiple WebSocket samplers you can use, however the one we use the most are the “WebSocket Open Connection” and the “WebSocket request-response Sampler”
The first allows us to open a WebSocket connection with a host
It is very straight-forward, similar to the HTTP request sampler
The “WebSocket request-response Sampler” allows us to send a message via the WebSocket connection created
You can send text or binary
Again, this is very straight-forward
For any more explanation on this Sampler and how to do more complicated things like Secury WebSockets, see the documentation on GitHub
There are a lot more things you can do with JMeter, however those will remain out of scope of this blog because it’s my blog and I said so
Docker I’m not going to explain what Docker is in this blog
If you don’t know or want a refresher, view this page: https://www.docker.com/what-docker Our Dockerfile looks like this: apache # Use a minimal base image with OpenJDK installed FROM openjdk:8-jre-alpine3.7 # Install packages RUN apk update && \ apk add ca-certificates wget python python-dev py-pip && \ update-ca-certificates && \ pip install --upgrade --user awscli # Set variables ENV JMETER_HOME=/usr/share/apache-jmeter \ JMETER_VERSION=3.3 \ WEB_SOCKET_SAMPLER_VERSION=1.2 \ TEST_SCRIPT_FILE=/var/jmeter/test.jmx \ TEST_LOG_FILE=/var/jmeter/test.log \ TEST_RESULTS_FILE=/var/jmeter/test-result.xml \ USE_CACHED_SSL_CONTEXT=false \ NUMBER_OF_THREADS=1000 \ RAMP_UP_TIME=25 \ CERTIFICATES_FILE=/var/jmeter/certificates.csv \ KEYSTORE_FILE=/var/jmeter/keystore.jks \ KEYSTORE_PASSWORD=secret \ HOST=your.host.com \ PORT=443 \ OPEN_CONNECTION_WAIT_TIME=5000 \ OPEN_CONNECTION_TIMEOUT=20000 \ OPEN_CONNECTION_READ_TIMEOUT=6000 \ NUMBER_OF_MESSAGES=8 \ DATA_TO_SEND=cafebabecafebabe \ BEFORE_SEND_DATA_WAIT_TIME=5000 \ SEND_DATA_WAIT_TIME=1000 \ SEND_DATA_READ_TIMEOUT=6000 \ CLOSE_CONNECTION_WAIT_TIME=5000 \ CLOSE_CONNECTION_READ_TIMEOUT=6000 \ AWS_ACCESS_KEY_ID=EXAMPLE \ AWS_SECRET_ACCESS_KEY=EXAMPLEKEY \ AWS_DEFAULT_REGION=eu-central-1 \ PATH="~/.local/bin:$PATH" \ JVM_ARGS="-Xms2048m -Xmx4096m -XX:NewSize=1024m -XX:MaxNewSize=2048m -Duser.timezone=UTC" # Install Apache JMeter RUN wget http://archive.apache.org/dist/jmeter/binaries/apache-jmeter-${JMETER_VERSION}.tgz && \ tar zxvf apache-jmeter-${JMETER_VERSION}.tgz && \ rm -f apache-jmeter-${JMETER_VERSION}.tgz && \ mv apache-jmeter-${JMETER_VERSION} ${JMETER_HOME} # Install WebSocket samplers RUN wget https://bitbucket.org/pjtr/jmeter-websocket-samplers/downloads/JMeterWebSocketSamplers-${WEB_SOCKET_SAMPLER_VERSION}.jar && \ mv JMeterWebSocketSamplers-${WEB_SOCKET_SAMPLER_VERSION}.jar ${JMETER_HOME}/lib/ext # Copy test plan COPY NonGUITests.jmx ${TEST_SCRIPT_FILE} # Copy keystore and table COPY certs.jks ${KEYSTORE_FILE} COPY certs.csv ${CERTIFICATES_FILE} # Expose port EXPOSE 443 # The main command, where several things happen: # - Empty the log and result files # - Start the JMeter script # - Echo the log and result files' contents CMD echo -n > $TEST_LOG_FILE && \ echo -n > $TEST_RESULTS_FILE && \ export PATH=~/.local/bin:$PATH && \ $JMETER_HOME/bin/jmeter -n \ -t=$TEST_SCRIPT_FILE \ -j=$TEST_LOG_FILE \ -l=$TEST_RESULTS_FILE \ -Djavax.net.ssl.keyStore=$KEYSTORE_FILE \ -Djavax.net.ssl.keyStorePassword=$KEYSTORE_PASSWORD \ -Jhttps.use.cached.ssl.context=$USE_CACHED_SSL_CONTEXT \ -Jjmeter.save.saveservice.output_format=xml \ -Jjmeter.save.saveservice.response_data=true \ -Jjmeter.save.saveservice.samplerData=true \ -JnumberOfThreads=$NUMBER_OF_THREADS \ -JrampUpTime=$RAMP_UP_TIME \ -JcertFile=$CERTIFICATES_FILE \ -Jhost=$HOST \ -Jport=$PORT \ -JopenConnectionWaitTime=$OPEN_CONNECTION_WAIT_TIME \ -JopenConnectionConnectTimeout=$OPEN_CONNECTION_TIMEOUT \ -JopenConnectionReadTimeout=$OPEN_CONNECTION_READ_TIMEOUT \ -JnumberOfMessages=$NUMBER_OF_MESSAGES \ -JdataToSend=$DATA_TO_SEND \ -JbeforeSendDataWaitTime=$BEFORE_SEND_DATA_WAIT_TIME \ -JsendDataWaitTime=$SEND_DATA_WAIT_TIME \ -JsendDataReadTimeout=$SEND_DATA_READ_TIMEOUT \ -JcloseConnectionWaitTime=$CLOSE_CONNECTION_WAIT_TIME \ -JcloseConnectionReadTimeout=$CLOSE_CONNECTION_READ_TIMEOUT && \ aws s3 cp $TEST_LOG_FILE s3://performance-test-logging/uploads/ && \ aws s3 cp $TEST_RESULTS_FILE s3://performance-test-logging/uploads/ && \ echo -e "\n\n===== TEST LOGS =====\n\n" && \ cat $TEST_LOG_FILE && \ echo -e "\n\n===== TEST RESULTS =====\n\n" && \ cat $TEST_RESULTS_FILE This file can be divided into 9 sections: Select the base Docker image
In this case it was a minimal base image with OpenJDK installed Some bootstrap things to be able to install everything we need later
Set the environment variables
These will be referenced later in the Docker file
Worth noting:You should add your Amazon keys in this section, so that the result and log file can be copied to Amazon S3
These keys need to be changed: apache AWS_ACCESS_KEY_ID=<<Your access key ID>> AWS_SECRET_ACCESS_KEY=<<Your secret access key>> AWS_DEFAULT_REGION=<<Your AWS region>> These should be available in account settings
Install Apache JMeter
Install the WebSocket Samplers made by Peter Doornbosch Copy the test plan (.jmx) to the location indicated in the environment variable
In this case when we build the Docker image, it is in the same directory as the Docker file
Copy some keystore information needed for WSS Expose port 443
This statement doesn’t actually do anything
It is just for documentation
(see: https://docs.docker.com/engine/reference/builder/#expose) The main command where we run JMeter with all the configurations and values we want to pass
This is similar to the command we used earlier to run JMeter as a script (non-GUI)
What we also do here is use the Amazon CLI to copy our JMeter log and result files to Amazon s3 (storage)
This will be explained in the next section
This is it for the Docker part of things
Hopefully there is enough information in this section for you to setup your own Dockerfile
In the next section, we will see how to build the Docker image and upload it to Amazon and run it there
Amazon Web Services Alright, so now you know how to use JMeter to design your test script and how to create a Docker image that sets up an environment needed to run your script
The reason you would want to run these kinds of tests in a Cloud service such as Amazon Web Services (AWS) in the first place is because your Personal Computer has its limits
For a MacBook Pro for example, we could only simulate around 2100 WebSocket connections before we started getting errors stating that “no more native threads could be created”
AWS gives us the ability to run Docker containers on Amazon EC2 clusters which will run the tests for us
In order to do this, you will first need to sign up with AWS: https://portal.aws.amazon.com/billing/signup#/start Once you have your account ready, log in to aws.amazon.com and you will hopefully see something like this: In the search bar, type “Elastic Container Service” and select it
This will be the service we will use to run our Docker container
Elastic Container Service The Elastic Container service is an AWS service that allows us to run our Docker containers in the Cloud
There are three things we are going to discuss regarding the Elastic Container Service: Repository: Where we save our Docker images Task Definitions: Where we define how to run our Docker containers Clusters: Where we start a cluster of VM’s, which will run the Tasks containing the Docker container which contains our JMeter script
This is what it looks like: Repositories Select “Repositories” to get started
As mentioned, this is where we will create a repository to push our Docker image to
Whenever we make a change to our Docker image, we can push to the same repository
Select “Create repository”
First thing you have to do is think of a good name for your repository: Now this next page I personally really like
It is a page which all the Docker commands needed to get your Docker image pushed to this newly created repository
Follow the instructions on this page: It’s always nice not having to think too much… If everything on this page goes well, you should see the following: With the table at the bottom showing all the versions of the image that have been pushed
If you want to push to this repository again, just click on the “View Push Commands” button to see the Docker commands again
I never really used any of the other tabs or buttons on this page, so let’s ignore those
Task Definitions Go to Task Definitions and select “Create a new Task Definition”
When prompted to select a compatibility type, choose “Fargate”
On the next page, simply enter a valid name for the task and scroll to the bottom where the button “add container” is situated
Ignore all the other settings on this page
Click on that button and a modal will show up
Fill in the following: Container name: You’re good at this by now Image: This can be found by going to repositories, clicking on your repository and copying the Repository URI (See last image) Port mappings: 443 (if you are using secure, otherwise 80) Environment Variables: Here you can overwrite any variables that are set in the Docker file
Scroll back up to the Docker section and notice that the Docker file was configured for 1000 threads
If I add an environment variables here with name “NUMBER_OF_THREADS” and set the value to 1500, the test will run with 1500 instead of the default set in the Docker file
Log configuration: Make sure Auto-configure CloudWatch logs is on
Leave everything else open/unchanged
Once you are done with this, click on “Add” to close the container settings modal and then click on “Create” to create the task definition
The new task definition should now show up in the list of task definitions
Now we’re ready to run our test! Clusters Now we’re going to start a cluster of EC2 instances, which will run our task we just defined
Navigate to Clusters and select “Create cluster”
You will be prompted with three options
I usually go with EC2 Linux + Networking
Give the cluster a name and leave everything else the way it is and select “Create”
Note:If you are going to be running a lot of requests and notice later on that the EC2 instance type does not have enough memory or CPU, you can select a larger instance type.See this page for more info: https://aws.amazon.com/ec2/instance-types/ If everything works out you should see a lot of green: Select “View Cluster” to go to the Cluster management page: The last thing to do is to open the “Task” tab, select run new task and fill in the following: Launch type: FARGATE Task definition: The task we defined in the previous sub-section Cluster: The cluster we just created Number of tasks: 1 —- You could add more, however this would mean that you would have the n times EC2 instances performing the same operations
Cluster VPC:Select the first one Subnets: Select the first one The rest can be left unedited
Note: You can still overwrite environment variables for this run specifically by clicking on “Advanced Options” and clicking on “Container Overrides”
You can change any of the values there
Select “Run Task” to finally run your test
Viola! Your test is now running: NOTE:The JMeter Log file and .xml file will be copied to S3 “bucket” specified in the Docker file: aws s3 cp $TEST_LOG_FILE s3://performance-test-logging/uploads/ && \ aws s3 cp $TEST_RESULTS_FILE s3://performance-test-logging/uploads/ && \ S3 is another AWS service used for storage
You can find it under the list of services, in the same way that you found the Elastic Container service
CloudWatch Remember when I told you to make sure that auto config was on for Cloud Watch
This is where it comes in handy
CloudWatch is a service that offers many monitoring capabilities, including log streaming and storing
In this case, we are interested in following our tests as they are being run
Click on the Amazon logo to go back to the screen with all the services, type in CloudWatch and select it
It should look something like this: On the left navigation bar, select “Logs”
You should see a table with log groups
One of the log groups should have the name of your Task Definition that you defined earlier
Click on it to see a list of logs for test runs that you have performed
When clicking on one, you will be able to see the logging of that test
It is also possible to the see the live logging as the test is being performed: As the test is running, the log file becomes pretty big and hard to follow
Here are some keywords you can type into the “filter events” text field in order to get some useful info of the test that is currently running: “summary”: This will show you the JMeter summar of the test, including the number of error percentages
“jmeter”: To see a list of all the parameters used for this test “error”: To see a list of all the errors
You can click on an error log to see more info
Some advice based on experience Here is a list of some things we figured out while running these tests on our application: Feedback is important
Make sure you have enough feedback to be sure that your tests are being performed the way you expect
Some of the things you can do we already covered in this blog, such as setting up CloudWatch and copying the .log and .xml files to s3 to be viewed later on
Other valuable feedback is logging in your application
Set it to Debug mode if possible during these tests
If you have any type of monitoring that can check the server and how many open TCP connections it has would also be great for proving that your tests are successful if you are using WebSockets
You might get errors because of your client rather than your application on the server
If you try to do too many requests or create too many websocket connections from one AWS EC2 instance, you might start to run into client side errors
For us, we could only run around 5000 WebSocket connections per instance
What we ended up doing was creating separate containers and task definitions per 5000 connections and running those tasks simultaneously
Don’t only test your application for number of connections/request, but also test the connection/request rate it can handle
This means decreasing the ramp up time in order to increase the number of connections/requests per second
This is important to know, as your applications might be challenged with this connection rate in the future
For us it was important, since restarting the application meant that all connected devices would start connecting within a certain time period
A single TCP port can handle 64k open connections
If you want to perform a test where you will need more than this, you will have to use another port and have your application listen to both ports
You will also need to have a load balancer of some kind which can distribute the load between the ports
Conclusion I hope this blog has given you an idea of what is possible when combining these three technologies
I know in a lot of the sections I simply told you to do something without giving much of an explanation as to what everything does
There are a lot of things you can do with JMeter, Docker and AWS and I encourage you to look them up and find what works for you
This setup worked for our case, and we are planning on running a test with 100k connections in the near future using this stack
Thanks for reading!Has your Daily Scrum become a bit boring
Is everybody just sending, but not listening
Has the interaction been lost
Then it’s time to spice up your stand-up! It wasn’t this awful in my team, but during one of our Sprint Retrospective we discussed our Daily Scrums
The interaction was a little bit lost
One of the Agile principles is: Individuals and interactions over processes and tools.The pitfall of the Daily is that it can feel like a mandatory part of the process instead of an opportunity to exchange information and help each other
A part of that mandatory feeling is answering the three questions: What did I do since the last Daily
What am I going to do until the next Daily
Do I encounter any impediments
In a previous team it even felt like reporting that you actually worked
Off course we kept the Daily and the three questions
But we changed the format to make it more dynamic
Every team member used to answer the questions for him or her in turn
Now we put more focus on the Sprint progress and the progress of completing Story’s and deploying releases
We did this by addressing each lane on the Scrum Board separately
On each lane there is either a Story, the current release, or non-blocking bugs on the bugs lane and some independent tasks on the support lane
We created the bugs and support lanes to keep things transparent, even if team members were working outside the Story’s we’ve got in a Sprint
Also the deployments of the release to Acceptance and Production needs some work of the team
Therefore we created a separate lane for it as well
By addressing each lane separately we keep focus on progress on the completion of the work of every lane
Now every team member answers the three questions in regard to the lane
For example: What did I do since the last Daily for this Story
What am I going to do until the next Daily for this Story
Do I encounter any impediments while working on this Story
The answers also include: When do I expect to be done with a particular work item
Such that another team member can review it
Or it can be tested
Or somebody else can use it for another work item
This works especially well if back-end and front-end work is done by specialized developers
This improved interaction between team members (individuals) to tune their work to each other
At our next Sprint Retrospective we evaluated this change
Everybody was really positive about the extra interaction it brought
Besides that the focus on completion led to a more steady burn-down
We kept it ever sinceIn a data science analogy with the automotive industry, the data plays the role of the raw-oil which is not yet ready for combustion
The data modeling phase is comparable with combustion in the engines and data preparation is the refinery process turning raw-oil to the fuel i.e., ready for combustion
In this analogy data analytics pipeline includes all the steps from extracting the oil up to combustion, driving and reaching to the destination (analogous to reach the business goals)
As you can imagine, the data (or oil in this analogy) goes through a various transformation and goes from one stage of the process to another
But the question is what is the best practice in terms of data format and tooling
Although there are many tools that make the best practice sometimes very use-case specific but generally JSON is the best practice for the data-format of communication or the lingua franca and Python is the best practice for orchestration, data preparation, analytics and live production
1 Datapipeline Architect Example What is the common inefficiency and why it happens
The current inefficiency is overusing of tabular (csv-like) data-formats for communication or lingua franca
I believe data scientists still overuse the structured data types for communication within data analytics pipeline because of standard data-frame-like data formats offered by major analytic tools such as Python and R
Data scientists start getting used to data-frame mentality forgetting the fact that tabular storage of the data is a low scale solution, not optimized for communication and when it comes to bigger sets of data or flexibility to add new fields to the data, data-frames and their tabular form are non-efficient
DataOps Pipeline and Data Analytics A very important aspect for analytics being ignored in some circumstances is going live and getting integrated with other systems
DataOps is about setting up a set of tools from capturing data, storing them up to analytics and integration, falling into an interdisciplinary realm of the DevOps, Data Engineering, Analytics and Software Engineering (Hereinafter I use data analytics pipeline and DataOps pipeline interchangeably.) The modeling part and probably some parts in data prep phases need a data-frame like data format but the rest of the pipeline is more efficient and robust if is JSON native
It allows adding/removing features easier and is a compact form for communication between modules
2 The role of Python Python is a great programming language used not only by the scientific community but also the application developers
It is ready to be used as back-end and by combining it with Django you can build up full-stack web applications
Python has almost everything you need to set up a DataOps pipeline and is ready for integration and live production
Python Example: transforming CSV to JSON and storing it in MongoDB To show some capabilities of Python in combination with JSON, I have brought a simple example
In this example, a dataframe is converted to JSON (Python dictionaries) and is stored in MongoDB
MongoDB is an important database in today’s data storage as it is JSON native storing data in a document format bringing high flexibility 
json <br />### Loading packages from pymongo import MongoClient import pandas as pd # Connecting to the database client = MongoClient('localhost', 27017) # Creating database and schema db = client.pymongo_test posts = db.posts # Defining a dummy dataframe df = pd.DataFrame({'col1': [1, 2], 'col2': [0.5, 0.75]}, index=['a', 'b']) # Transforming dataframe to a dictionary (JSON) dic=df.to_dict() # Writing to the database result = posts.insert_one(dic) print('One post: {0}'.format(result.inserted_id)) The above example shows the ability of python in data transformation from dataframe to JSON and its ability to connect to various tooling (MongoDB in this example) in DataOps pipeline
Recap This article is an extension to my previous article on future of data science (https://bit.ly/2sz8EdM)
In my earlier article, I have sketched the future of data science and have recommended data scientists to go towards full-stack
Once you have a full stack and various layers for DataOps / data analytics JSON is the lingua franca between modules bringing robustness and flexibility for this communication and Python is the orchestrator of various tools and techniques in this pipeline
1: The picture is courtesy of https://cdn-images-1.medium.com/max/1600/1*8-NNHZhRVb5EPHK5iin92Q.png 2: The picture is courtesy of https://zalando-jobsite.cdn.prismic.io/zalando-jobsite/2ed778169b702ca83c2505ceb65424d748351109_image_5-0d8e25c02668e476dd491d457f605d89.jpgLast month we had another tech day! On this day we try to do cool stuff
This time I had the idea to use facial recognition on my magic mirror (See my previous blog)
So with a group of 5 people, we started thinking of the requirements
When someone is standing in front of the mirror, we want the mirror to detect that
This means we need motion detection! We also want to recognise the user and his emotion
If we can recognise the user, it would be cool to display a message on the mirror or even speak out a message
Well enough requirements for a day! We divided the group in two teams, one working on the frontend, the other on the backend and it was time to get to work! Frontend For the frontend we used tracking.js to detect if someone is standing in front of the camera
Once the person is detected, we take a picture and send it to our backend
Here is some code of the tracking part
To prevent that every movement resulted in taking a picture, we also added a check that the next picture will be taken after 10 seconds
java // Start tracking tracker.on('track', function (event) { context.clearRect(0, 0, canvas.width, canvas.height); // Track a person event.data.forEach(function (rect) { const timeAgo = new Date(new Date().getTime() - 10 * 1000); const inLastTime = pictureTaken.getTime() > timeAgo; // Take a new picture if (rect.total >= 1 && !inLastTime) { takeSnapshot(); pictureTaken = new Date(); } // Draw rectangle context.strokeStyle = '#a64ceb'; context.strokeRect(rect.x, rect.y, rect.width, rect.height); context.font = '11px Helvetica'; context.fillStyle = "#fff"; context.fillText('x: ' + rect.x + 'px', rect.x + rect.width + 5, rect.y + 11); context.fillText('y: ' + rect.y + 'px', rect.x + rect.width + 5, rect.y + 22); }); }); Backend At our DevCon conference my colleague Bert Ertman showed how he used AWS Rekognition to identify who was ringing his doorbell
This looked very promising so we decided to use this service as well for the face recognition part
We want the backend to receive a photo taken by the frontend, which than can be used to check with AWS Rekognition if the face is recognised
To make this process work we needed to configure a few things for AWS
Note that I’m using the AWS command line interface to execute most of these steps, but this can also be done in code
Add AWS credentials to your local machine
(link) Create a bucket on S3 Upload a file to the bucket Create a collection java aws rekognition create-collection \ --collection-id "someCollectionId" \ --region eu-west-1 \ --profile default 5.Index each face that you want to be recognised java aws rekognition index-faces \ --image '{"S3Object":{"Bucket":"bucket-name","Name":"file-name"}}' \ --collection-id "collection-id" \ --detection-attributes "ALL" \ --external-image-id "example-image.jpg" \ --region eu-west-1 \ --profile default Now that we have indexed a face, we can check if the person is recognised by AWS Rekognition
The following command will search for faces in the given image
java aws rekognition search-faces-by-image \ --image '{"S3Object":{"Bucket":"bucket-name","Name":"Example.jpg"}}' \ --collection-id "collection-id" The response will give us each face that has been matched and with which confidence it has matched
One other request that we can do is the detect faces request
This response contains information like emotions, gender but also if the person is wearing (sun)glasses or smiling
java aws rekognition detect-faces \ --image '{"S3Object":{"Bucket":"bucket","Name":"file"}}' \ --attributes "ALL" End of tech day At the end of this day we had a working demo app which tracked the user, took a picture and displayed some information about the user on our laptop.While this was already great, it would even be greater if this worked on my raspberry pi and magic mirror
So I decided to remove the trackingjs part and use a camera connected to my raspberry pi as input
On the backend I created an endpoint which can be called in the future by an IoT button, just to prevent that the camera is always on or is taking pictures every time a person walks by
Adding AWS Polly Because I want to give the user a personal message when he is recognised, I have added AWS Polly to turn a message that I constructed with the information from AWS Rekognition into lifelike speech
All you need to do is sent a synthesize speech request to AWS and you will get a response which contains the audio stream
1 2 3 4 5 aws polly synthesize-speech \ --output-format mp3 \ --voice-id Joanna \ --text 'Hello Roberto
You look happy today' \ hello.mp3 This is now the flow on my raspberry pi
The backend is created with Spring boot and provides an endpoint to trigger the whole process
The backend will send a request to my mirror with the text which is shown by the IFTTT module of the MagicMirror platform
The backend will also play the audio stream that we received from AWS Polly
Result I have made a video to show you the result
You will see that it recognised my happy face, greets me (in Dutch) and tells me that I look happy 🙂 I’m going to continue to make the code more configurable, so it may be useful to others
Let me know if you have any questions or thoughts on what you think that need to be configurable
[1]: https://amsterdam.luminis.eu/2017/07/25/techday-smart-mirror/ [2]: https://trackingjs.com/ [3]: https://aws.amazon.com/rekognition/ [4]: https://docs.aws.amazon.com/cli/latest/userguide/cli-config-files.html [5]: https://aws.amazon.com/polly/ [6]: https://github.com/jc21/MMM-IFTTT [7]: https://magicmirror.builders/I got the opportunity to attend an offsite event organized by a client
We learned and shared our experiences, our problems, with the motive of finding a way to work more effectively together, be it technical or process related
There were posters of superhero movies with our faces morphed on it
There was so much energy from the start till the end of the event
We had meals together
We played chess, football, quizzes and what not
The talks were meticulously planned and picked for our group
Wonderful sessions! And I would like to share my experience for one of the agile sessions here
One of the first tracks I attended was about Captain Review! The goal was to talk about how you can have an effective review and have fun doing it
Our scrum coach, Jill Janssen gave us the intro
We discussed the importance of review and what should you look for in a feedback
She talked briefly about the elements of a good review:- Done – Overview what will and will not be demonstrated, taking note of those things that are complete but won’t be demoed
Not done – Explain the reasons for the incomplete items, but save the improvement discussion for the Sprint Retrospective
Demo – Demo only new/changed functionality, beware of ad hoc demo requests
Feedback – Obtain feedback, which will be discussed in detail
Mention key occurrences, events, or problems from the sprint
Refinement – Refine, adjust, and review product backlog based on feedback
It was an eye opener to know about a few signs of an ineffective Sprint review(and their solutions) Team does not request feedback from participants/stakeholders Prepare the sprint review to maximize feedback Ask engaging questions to the audience Leave enough time after each item to discuss it while the information is fresh
Team reports to management to sign off on the increment The key participants are the team, product owner and stakeholders, not management
There is nothing wrong with management reports, but they are not the purpose of the Sprint Review
Signoff comes from the Product Owner
Since this session was focussed on getting a feedback, we played a little game where a volunteer took a seat in front of the other participants and was asked to throw a couple of balls, one by one in a box behind him
This box was not directly behind him and he didn’t know how far or to which side of him, was it placed
Also, the audience was not allowed to give any hints
Obviously, without a feedback when blindly throwing balls into the box, there was no success
In the next round, Jill allowed the audience to give cues
The brave volunteer had more success in landing the balls in the box with a proper feedback
This demonstrated how important is feedback
And how can we achieve more when we keep the communication lines open and regular
Later, we played another game where we were offered a few products, for which we had to do a review with the group
The group was divided into 5 and were given a choice of products to do review session for
To choose from, there was a pack of pencils, sticky notes, socks with anti-slip patches, chocolates and more
I was happy that my group member got us a pair of anti-slip socks
My love for yoga sprung up as soon as I saw the product and a plan for the demo and feedback started coming to me
We brainstormed over a few ideas for 5-10 minutes and had a wonderful plan
To elaborate on the exercise – We had to sell an additional feature on the product
It was the usual drill – Intro, demo and a feedback
After this, we had to gather feedback for the whole group
A few of the suggestions to gather feedback were: – Post-its: Let stakeholders write down the answers to a question on post-it notes
Limit the answer to one per post-it note
Let all stakeholders write down their answers first without revealing them to others
Depending on the question asked, this can take 2 to 5 minutes
Shout outs: Ask stakeholders to shout out their answer, one after another
You can limit the number of shout-outs you’ll collect
Take notes on a flip chart
Message Me: Let stakeholders write down their answers in silence
Depending on the time available, let them write it down in format of a letter (long), email (a couple of lines), social media update (rather short), or tweet (short; 140 character)
High Five: Let stakeholders find their answer in silence
Let them show that they’ve found their answer by putting a fist on their chest
Then count down “3, 2, 1, now!” On “now!” the stakeholders should hold up 0 to 5 fingers with one hand
For every stakeholder, shout out their answer
Note the result on a flip chart or whiteboard, e.g., 1 x 3 fingers, 4 x 4 fingers, and 2 x 5 fingers
Follow-up Investigation: Ask each stakeholder why they gave their answer
Or ask only the stakeholders with the lowest and highest amount of presented fingers
Follow-up Debate: Ask the stakeholders with low finger answers to pair with the ones with high-finger answers
Let the pairs debate their different views for a couple of minutes
Afterwards, let the pair shout out a summary of their debate
Thumbs Up: Let stakeholders give their answer by showing their thumbs in 3 directions: thumb up (positive), thumb to the side (neutral; undecided; meh.), thumb down (negative)
Follow-up Investigation: Ask each stakeholder or only the ones you are interested in (for example, only the negative or undecided ones) why they gave their answer
We had some fun coming up with features
One was low fat chocolate
Of course, we were given a demo, or you can say bribed
And we got to eat the chocolates! Also, the feedback loop became very interesting with the happy group of people in the session
Everyone enjoyed giving feedback like – “The white color pencil does not work on the white sticky note
Maybe you guys should collaborate on making this work.” We were having quite some fun, which helped me maintain focus on the conversation
I grabbed the opportunity to take my turn after the chocolates team were done with their review
My pitch was – After we were done eating those delicious chocolates, we still should make sure that we have burned whatever fat we consumed
In the Netherlands, anyways, fitness is serious business
You have been using our cotton socks happily so far
At this time, I had the sock on display with the anti-slip part showing
I flipped the sock, and announced, we have added a new feature to our existing successful product, for the yoga and Pilates enthusiasts with anti-slip patches
I have one on my foot, and I will show you how much I trust it
I donned a simple yoga pose and claimed that I could hold it until the end of the review
<Applause> I must shamelessly state that it was the best demo so far
Meanwhile, I said, let me run a fresh sock around
You can put your hand in and check the grip on the table
Its clean, never worn and just out of the package
While people were testing it out, I started taking questions
Remember, I was on my one foot
I needed to make it short 😉 The group asked me why it was so heavy
In their opinion it should have been lighter
After all, it is for exercise
At this point, I had to think on my feet errr… foot
But I understood that this will take longer
So, I slipped my foot back on the ground without anyone noticing the flaw in the demo(J) and I replied – This is for grounding you
Remember this is not for running, but for more grounding forms of exercises
So, it’s not a bug, but a feature
<Laughter> Another query I got, was about the durability
Can I toss it in washing machine and up to how many times
To this, I came up with an explanation that our team does not discontinue with existing features or replaces features
We have continued the existing capability and you can still wash them every day for 100 times
Although I am not sure if you will be able to keep up with exercise every day for 3 continuous months
J <Laughter> When the demo product came back to our table, I asked the people to vote with thumbs up/down
I had a quick look around, and everyone had good feedback
There was only one person who had a horizontal thumb (neither satisfied nor dissatisfied)
I claimed that it’s a winner feature for all
And pointed out to that person – You sir! I think you just want to know about my yoga classes
Don’t you
Concluded the best review I ever had with applause and laughter
It was really fun experience and I thank our coach and the host for the wonderful exercise
I am all invigorated to make the reviews with my team at work more fun and engaging
P.S.: – The socks are really nice
I am keeping themThe past week we visited Elasticon 2018 in San Francisco
In our previous blog post we wrote about the Keynote and some of the more interesting new features of the elastic stack
In this blog post, we take one of the cool new products for a spin: Canvas
But what is Canvas
Canvas is a composable, extendable, creative space for live data With Canvas you can combine dynamic data, coming from a query against Elasticsearch for instance, with nice looking graphs
You can also use tables, images and combine them with the data visualizations to create stunning, dynamic infographics
In this blog post, we create a Canvas about the tweets with the tags Elasticon during the last day of the elastic conference last week
Below is the canvas we are going to create
It contains a number of different elements
The top row contains a pie chart with the language of the tweets, a bar chart with the number of tweets per time unit, followed by the total tracked tweets during the second day of elasticon
The next two elements are using the sentiment of the tweets
This was obtained using IBM Watson
Byron wrote a basic integration with Watson, he will give more details in a next blog post
The pie chart shows the complete results, the green smiley on the right shows the percentage of positive tweets of the total number of tweets that we could analyze without an error or be neutral
With the example in place, it is time to discuss how to create these canvasses yourself
First some information about the installation
A few of the general concepts and finally sample code for the used elements
Installing canvas Canvas is part of elastic Kibana
You have to install canvas as a plugin into Kibana
You do need to install X-Pack in Elasticsearch as well as in Kibana
The steps are well described in the installation page of Canvas
Beware though, installing the plugins in Kibana takes some time
They are working on improving this, but we have to deal with it at the moment
If everything is installed, start Kibana in your browser
At this moment you could start creating the canvas, however, you have no data
So you have to import some data first
We used Logstash with a twitter input and elastic output
Cannot go into to much detail or else this blog post will be way too long
Might do this is a next blog post
For now, it is enough to know we have an index called twitter that contains tweets
Creating the canvas with the first element When clicking on the tab Canvas we can create a new Workpad
A Workpad can contain one of the multiple pages and each page can contain multiple elements
A Workpad defines the size of the screen
Best is to create it for a specific screen size
At elasticon they had multiple monitors, some of them horizontal, others vertical
It is good to create the canvas for a specific size
You can also choose a background color
These options can be found on the right side of the screen in the Workpad settings bar
It is good to know that you can create a backup of you Workpad from the Workpads screen, there is a small download button on the right side
Restoring a dashboard is done by dropping the exported JSON into the dialog
Time to add our first element to the page
Use the plus sign at the bottom of the screen to add an element
You can choose from a number of elements
The first one we’ll try is the pie chart
When adding the pie chart, we see data in the chart
Hmm, how come, we did not select any data
Canvas comes with a default data source, this data source is used in all the elements
This way we immediately see what the element looks like
Ideal play around with all the options
Most options are available using the settings on the right
With the pie, you’ll see options for the slice labels and the slice angles
You can also see the Chart style and Element style
These configuration options show a plus signed button
With this button, you can add options like color pallet and text size and color
For the element, you can set a background color, border color, opacity and padding Next, we want to assign our own data source to the element
After adding our own data source we most likely have to change the parameters for the element as well
In this case, we have to change the Slice labels and angles
Changing the data source is done using the button at the bottom, click the Change Datasource button/link
At the moment there are 4 data sources: demo data, demo prices, Elasticsearch query and timeline query
I’ll choose the Elasticsearch query, select the index, don’t use a specific query and select the fields I need
Selecting the fields I need can speed up the element as we only parse the data that we actually need
In this example, we only use the sentiment label
The last thing I want to mention here is the Code view
After pushing the >_ Code button you’ll see a different view of your element
In this view, you’ll get a code approach
This is more powerful than the settings window
But with great power comes great responsibility
It is easy to break stuff here
The code is organized in different steps
The output of each step is, of course, the input for the next step
In this specific example, there are five steps
First a filter step, next up the data source, then a point series that is required for a pie diagram
Finally the render step
If you change something using the settings the code tab gets updated immediately
If I add a background color to the container, the render step becomes: render containerStyle={containerStyle backgroundColor="#86d2ed"} If you make changes in the code block, use the Run button to apply the changes
In the next sections, we will only work in this code tab, just because it is easier to show to you
Adding more elements The basics of the available elements or function are documented here
We won’t go into details for all the different elements we have added
Some of them use the defaults and therefore you can add them yourselves easily
The first one I do want to explain is the Twitter logo with the number of tweets in there
This is actually two different elements
The logo is a static image
The number is more interesting
This makes use of the escount function and the markdown element
Below is the code
java filters | escount index="twitter" | as num_tweets | markdown "{{rows.0.num_tweets}}" font={font family="'Open Sans', Helvetica, Arial, sans-serif" size=60 align="left" color="#ffffff" weight="undefined" underline=false italic=false} The filters are used to facilitate filtering (usually by time) using the special filter element
The next item is escount which does what you expect
It counts the number of items in the provided index
You can also provide a query to limit the results, but we did not need it
The output for escount is a number
This is a problem when sending it to a markdown element
The markdown element only accepts a datatable
Therefore we have to use the function as
This accepts a number and changes it into a datatable
The markdown element accepts a table and exposes it as rows
Therefore we use the rows to obtain the first row and of that row the column num_tweets
When playing with this element it is easy to remove the markdown line, Canvas will then render the table by default
Below the output for only the first two rows as well as the changes after adding the third line (as num_tweets) html 200 html num_tweets # 200 Next up are the text and the photo belonging to the actual tweets
The photo is a bit different from the Twitter logo as it is a dynamic photo
In the code below you can see that the image element does have a data URL attribute
We can use this attribute to get one cell from the provided data table
The getCell function has attributes for the row number as well as the name of the column
html esdocs index="twitter*" sort="@timestamp, desc" fields="media_url" count=5 query="" | image mode="contain" dataurl={getCell c="media_url" r=2} | render With the text of the tweet, it is a bit different
Here we want to use the markdown widget, however, we do not have the data URL attribute
So we have to come up with a different strategy
If we want to obtain the third item, we select the top 3 and from the top 3, we take the last item
html filters | esdocs index="twitter*" sort="@timestamp, desc" fields="text, user.name, created_at" query="" | head 3 | tail 1 | mapColumn column=created_at_formatted fn=${getCell created_at | formatdate 'YYYY-MM-DD HH:mm:ss'} | markdown "{{#each rows}} **{{'user.name'}}** (*{{created_at_formatted}}*) {{text}} {{/each}}" font={font family="'American Typewriter', 'Courier New', Courier, Monaco, mono" size=18 align="right" color="#b83c6f" weight="undefined" underline=false italic=false} The row that starts with mapColumn is a way to format the date
The mapColumn can add a new column with the name as provided by the column attribute and the value as the result of a function
The function can be a chain of functions
In this case, we obtain the column create_at of the datatable and pass it to the format function
Creating the partly green smiley The most complicated feature was the smiley that turns green the more positive tweets we see
The positiveness of the tweets was determined using IBM Watson interface
In the end, it is the combination of twee images, one grey smiley, and one green smiley
The green smiley is only shown for a specific percentage
This is the revealImage function
First, we show the complete code
html esdocs index="twitter*" fields="sentiment_label" count=10000 | ply by="sentiment_label" fn=${rowCount | as "row_count"} | filterrows fn=${if {getCell "sentiment_label" | compare "eq" to="error"} then=false else=true} | filterrows fn=${if {getCell "sentiment_label" | compare "eq" to="neutral"} then=false else=true} | staticColumn column="total" value={math "sum(row_count)"} | filterrows fn=${if {getCell "sentiment_label" | compare "eq" to="positive"} then=true else=false} | staticColumn column="percent" value={math "divide(row_count, total)"} | getCell "percent" | revealImage image={asset "asset-488ae09a-d267-4f75-9f2f-e8f7d588fae1"} emptyImage={asset "asset-0570a559-618a-4e30-8d8e-64c90ed91e76"} The first line is like we have seen before, select all rows from the twitter index
The second row does kind of a grouping of the rows
It groups by the values of sentiment_label
The value is a row count that is specified by the function
If I remove all the other rows we can see the output of just the ply function
html sentiment_label row_count negative 32 positive 73 neutral 81 error 14 The next steps filter out the rows for error and neutral, then we add a column for the total number of tweets with a positive or negative label
Now each row has this value
Check the following output
html sentiment_label row_count total negative 32 105 positive 73 105 The next line removes the negative row, then we add a column with the percentage, obtain just one cell and call the revealImage function
This function has a number input and attributes for the image as well as the empty or background image
That gives us all the different elements on the canvas
Concluding We really like the options you have with Canvas
You can easily create good-looking dashboard that contains static resources, help texts, images combined with dynamic data coming from Elasticsearch and in the future most likely other resources
There are some improvements possible of course
It would be nice if we could also select doc_value fields and using aggregations in a query would be nice as well
We will closely monitor the progression as well believe this is going to be a very interesting technology to keep using in the futurePrologue At Luminis Arnhem we are currently working together with Nedap on the MACE project, a prototype implementation of a site access control system
In the MACE scenario, the users can access a physical location/open a door using a digital identity card on their mobile device
These digital identities replace those cumbersome physical cards that either take up space in your wallet or always seem to disappear when you need them
The MACE mobile application receives these identities from a server application, which are then sent to a MACE reader device via Bluetooth Low Energy (BLE), Near Field Communication (NFC) or can be read by the reader via a Quick Response (QR) code
The idea is that if the correct identity is read by the reader, i.e
an identity that has the authority to gain access to the location guarded by the reader, then the user gains access to a physical location
In order to create an initial prototype we needed to delve into Android BLE
In this blog I will describe how BLE works in the MACE scenario as well as how the implementation looks like in Android
For more information on the MACE project itself, visit the MACE homepage
On BLE There are two roles in BLE communication
These are the central and peripheral roles
The central role is responsible for scanning for advertisements which are made by peripherals
Once the central and peripheral are in range of each other, the central will start to receive these advertisements and can choose to connect to a peripheral
Think of it as the central being a job seeker and a peripheral being one of those recruiters on LinkedIn
What I’m trying to say about peripherals is that they continuously (in most cases) advertise without even knowing if the central has any interest or use for its services
Luckily, the central can use a filter to only see advertisements that it finds interesting
This is done based on a UUID filter, which on the application level only shows advertisements that contain a service with that UUID
Each advertisement contains (at least) the following information: –Bluetooth Device Address: Similar to a MAC address used to identify a BLE device
–Device name: A custom name for the peripheral device which can be configured
–RSSI value: The Received Signal Strength Indicator in decibel per meter
This is a value that indicates the strength of the signal, ranging from -100 to 0
The closer the RSSI is to 0, the better the signal is
–List of Services: A list of services that are provided by the peripheral
In the BLE scenario, a service is a collection of characteristics
A characteristic contains a single value and a number of descriptors
Descriptors describe the value contained in the characteristic, such as a max length or type
The image above illustrates the hierarchy of these concepts
The line between Peripheral and Service depicts the “Peripheral has one or many Services” relationship
The rest of this blog will describe the building blocks for creating an Android BLE central implementation, such as scanning for advertisements, processing an advertisement, connecting to a device (peripheral) and reading and writing, from and to a BLE peripheral
We will not discuss BLE in more depth, as that is not the goal of this blog
Main act: Android implementation Ask nicely The first thing we need to do is to ask for permission to turn on Bluetooth on the mobile device and use it
For Android versions lower than 6.0, we only need to add the permissions to the Android Manifest: <uses-permission android:name=“android.permission.BLUETOOTH"/> (ble_permission.png) For BLE on Android 6.0 or higher, we noticed that our app was crashing when trying to use the Bluetooth functionality
It turns out that you need to add one of the following permissions to the Android Manifest: <uses-permission android:name="android.permission.ACCESS_COARSE_LOCATION"/> <uses-permission android:name="android.permission.ACCESS_FINE_LOCATION"/> These permissions are required for determining the rough or precise location of the mobile device respectively
You might be wondering: Why do I need these permissions that are related to location to turn on BLE
Good question
I believe it has something to do with the fact that the signal strength of your mobile device to a BLE peripheral can be used in combination with other sources to determine your location
I have not yet looked further into this
However in the MACE application we do use the RSSI value to estimate how far you are from the MACE reader, so it is not implausible
As of Android 6.0, users now get prompted on whether to give an app certain permissions while the app is running, instead of simply accepting all permissions when installing the app
We have to change our code to ask the user for this permission the first time they try to use the BLE functionality in the app: if (activity.checkSelfPermission(Manifest.permission.ACCESS_COARSE_LOCATION) != PackageManager.PERMISSION_GRANTED) { activity.requestPermissions(new String[]{Manifest.permission.ACCESS_COARSE_LOCATION}, 1); } This code checks if the permission is already granted
If that is not the case, the user will be prompted with the request to give the ACCESS_COARSE_LOCATION permission
To handle the result of this action, you need to override the onRequestPermissionResult method
See this page for more info
In case Bluetooth is turned off on the mobile device, we also need to ask the user if we can turn it on
To do this, we need an instance of the Bluetooth adapter
We will discuss this is the next section
Bluetooth turn-ons Now that we have politely asked for permission from the user to use Bluetooth, it is time to get to work
The first thing we should do is get the Bluetooth Adapter instance
The Bluetooth Adapter instance can be retrieved using the BluetoothManager
If there is no BluetoothManager present, this means that the device does not support Bluetooth
An example: BluetoothManager bluetoothManager = (BluetoothManager) context.getSystemService(Context.BLUETOOTH_SERVICE); if (bluetoothManager == null){ //Handle this issue
Report to the user that the device does not support BLE } else { BluetoothAdapter adapter = bluetoothManager.getAdapter(); } Once we have the adapter, we can get the party started
As mentioned before, we first need to check if Bluetooth is turned on for the mobile device
If this is not the case, we will ask the user to turn this on
This can be done in the following way: if(adapter != null && !adapter.isEnabled()){ Intent intent = new Intent(BluetoothAdapter.ACTION_REQUEST_ENABLE); activity.startActivityForResult(intent, 1); }else{ System.out.println("BLE on!"); //do BLE stuff } The handling of this request is done in the same way as when we asked for the permissions in the previous section
From this point on, we know that BLE is up and running on the mobile device
Hello…is it me you’re looking for
It’s time to start scanning! When we start the scanner, the mobile device will start receiving advertisements of BLE peripherals around it (range varies per device)
With the BluetoothAdapter we retrieved in the previous section, you can get an instance of a BLE scanner
We need to supply this scanner with at least an implementation of a ScanCallback, which describes what we should do with the results of the scan
Optionally, we can also supply the scanner with filters and settings
The filters help us specify our search in order to find the devices we are looking for
The settings are used to determine how the scanning should be performed
This is a simple example of how to do all of this and get the scanner running: public void startScanning(){ BluetoothLeScanner scanner = adapter.getBluetoothLeScanner(); ScanSettings scanSettings = new ScanSettings.Builder().setScanMode(ScanSettings.SCAN_MODE_LOW_LATENCY).build(); List<ScanFilter> scanFilters = Arrays.asList( new ScanFilter.Builder() .setServiceUuid(ParcelUuid.fromString("some uuid")) .build()); scanner.startScan(scanFilters, scanSettings, new MyScanCallback()); } public class MyScanCallback extends ScanCallback { @Override public void onScanResult(int callbackType, final ScanResult result) { //Do something with results } @Override public void onBatchScanResults(List<ScanResult> results) { //Do something with batch of results } @Override public void onScanFailed(int errorCode) { //Handle error } } A few things to note: -There are a few scan modes available
SCAN_MODE_LOW_LATENCY has the highest frequency of scanning and will thus cause more battery drain
SCAN_MODE_LOW_POWER has the lowest frequency and is best for battery usage
This mode would mostly be used for background scanning or if a low frequency of results is acceptable for your application
SCAN_MODE_BALANCED is somewhere in between the previously mentioned modes
SCAN_MODE_OPPORTUNISTIC is a special scan mode, where the application itself does not scan but instead listens in on other applications scan results
-The scan filter in this case is used for only detecting advertisements that contain a certain uuid as a service
All other advertisements will be ignored
-The ScanCallback has a onBatchScanResults function, which is only called when a flush method is called on the scanner
The scanner has the ability to queue scan results before calling the callback method, however I have not looked further into this
-In a real application, you might want to keep a reference to the scanner so that you can stop it from scanning when this is necessary
So many advertisements… A ScanResult object is returned for each advertisement scanned
The frequency of advertisements received is dependent on the scan mode
The scan callback will get called for each advertisement
This can become overwhelming
In the MACE project, we stop scanning as soon as we find a device we want to connect with
The ScanResult object contains some information on the Device such as the BLE address, the service uuids and the name given to the device
The ScanResult object also contains the RSSI, which tells you roughly how close the mobile device is to the peripheral device the advertisement belongs to
With the information in the ScanResult it is possible to do a second-level filtering, such as checking if the peripheral is close enough using RSSI or checking if the device name is what you expect it to be
If everything checks out, we can proceed to connecting to the peripheral
Give it everything you gatt! The concept of two BLE devices communicating with each other via services and characteristics is called Generic Attribute Profile, GATT for short
In order to use this in the mobile application, we need a BluetoothGatt instance
This can be obtained in the following way: @Override public void onScanResult(int callbackType, final ScanResult result) { BluetoothDevice device = adapter.getRemoteDevice(result.getDevice().getAddress()); BluetoothGatt gatt = device.connectGatt(mContext, false, new myGattCallBack()); } We first get an instance of the device using the BLE address obtained in the scan result
Using this device object we then call the connectGatt function, which gives us a Bluetooth Gatt instance
The first parameter in the connectGatt method is the Android App context
The second parameter is indicates whether or not we should automatically connect to the device once it appears
This concept is also known as having devices paired with each other
Since we have commitment issues, we set this to false
The last parameter is the callback which is called by the BluetoothGatt instance when there is a response from the peripheral
The callback should extend the BluetoothGattCallback class
You should create your own implementation and override (at least) the following methods: -onConnectionStateChange(BluetoothGatt gatt, int status, int newState) -onServiceDiscovered(BluetoothGatt gatt, int status) -onCharacteristicRead(BluetoothGatt gatt, BluetoothGattCharacteristic characteristic, int status) Once you call the connectGATT method, the onConnectionStateChange method will be called
If the newState value is 2 (Connected), we can carry on
If this is not the case, then we cannot communicate with the peripheral and all other operations on the gatt instance will fail
Assuming we have connected successfully with the peripheral device, we can now trigger the service discovery by calling: gatt.discoverServices(); which asks the peripheral device for a list of all services
These services are then loaded into the gatt instance
The onServiceDiscovered method will be called with a status of 0 if it was successful
If this is the case, we can call: List<BluetoothGattService> services = gatt.getServices(); since the gatt instance now has these services after the discovery
Note that these services also contain a list of characteristics
From this point on it is a matter of finding the right characteristic you are looking for within the right service to read from or write to
These are not the characteristics you are looking for… It is important to make an agreement as to which UUIDS will be used for which services and characteristics on the peripheral device
These UUIDS need to be known on the mobile application
Using these UUIDS, we can loop through the services in the following way: private String serviceUUID = "xyz"; private String characteristicUUID = "xyz"; private BluetoothGattCharacteristic characteristic = null; private void findCharacteristic(List<BluetoothGattService> services){ for(BluetoothGattService service: services){ if(service.getUuid().toString().equalsIgnoreCase(serviceUUID){ for(BluetoothGattCharacteristic serviceCharacteristic : service.getCharacteristics()){ if(serviceCharacteristic.getUuid().toString().equalsIgnoreCase(characteristicUUID)) { characteristic = serviceCharacteristic; } } } } } Now we actually have an object that represents the characteristic that we want to read from or write to
Change the world around you We have reached the final step in this blog
What we want to do is finally read from and write to a peripheral device
We do this by using the characteristic object obtained in the previous section in combination with our GATT instance
To read a data in the characteristic, we simply call: boolean successfullyRead = gatt.readCharacteristic(characteristic); @Override public void onCharacteristicRead(BluetoothGatt gatt, BluetoothGattCharacteristic characteristic, int status){ byte[] characteristicValue = characteristic.getValue(); //Do something with the value } The onCharacteristicRead method is called with the characteristic
You can get the value in the characteristic by calling characteristic.getValue(), which returns a byte array with the value that the characteristic contains
To write to the characteristic, we first have to set the value we want to write into the characteristic object we obtained
Once this is set, we can call write characteristic using the gatt instance: byte[] valueToWrite = new byte[8]; Arrays.fill(valueToWrite, (byte) 0x00); characteristic.setValue(valueToWrite); boolean successfullyWritten = gatt.writeCharacteristic(characteristic); The boolean successfullyWritten is true if the write action was successful, otherwise it returns false
Epilogue You now have the basic building blocks to build an Android BLE central implementation
What you do from here on out is up to your imagination
Of course we couldn’t cover everything in this blog
Certain things such as threading, error handling and battery optimisation were omitted in the hope to keep this entry concise
These topics will need to be covered in another blog
In any case, you now know the essentials
Have fun and remember, scan responsiblyThe past few days have been fantastic
Together with Byron I am visiting San Francisco
We have seen amazing sights, but yesterday the reason why we came started
Day one of Elasticon starts with the keynote showing us cool new features to come and sometimes some interesting news
In this blog post, I want to give you a short recap of the keynote and tell you what I think was important
Rollups With more and more data finding its way to elasticsearch, some indexes become too large for their purpose
We do not need to keep all data of the past weeks and months
We just want to keep the data needed for aggregations we show on a dashboard
Think about an index containing logs of our web server
We have a chart with HTTP status codes, response times, browsers, etc
Now you can create a rollup configuration providing the aggregations we want to keep, containing a cron expression telling when to run and some additional information about how much data to keep
The result is a new index with a lot less data that you can keep for your dashboards
More information about the rollup functionality can be found here
Canvas Last year at Elasticon Canvas was already shown
Elastic continued with the idea and it is starting to look amazing
With Canvas you can create beautiful looking dashboards that go a big step further than the standard dashboards in Kibana
You can customise almost everything you see
It comes with options to put an image on the background, a lot of color options, new sort of data visualisation integrated of course with elasticsearch
In a next blog post I’ll come up with a demo, it is looking very promising
Want to learn more about it, check this blog post
Kubernetes/Docker logging One of the areas I still need to learn is the Docker / Kubernetes ecosystem
But if you are looking for a solution to monitor you complete Kubernetes platform, have a look at all the options that elastic has these days
Elastic has impressive support to monitor all the running images
It comes with standard dashboards in Kibana
It now has a dedicated space in Kibana called the Infra tab
More information about the options and how to get started can be found here
Presenting Geo/Gis data A very cool demo was given on how to present data on a Map
The demo showed where all Elasticon attendees were coming from
The visual component has an option of creating different layers
So you can add data to give the different countries a color based on the number of attendees
In a next layer show the bigger cities where people are coming from in small circles
Use a logo of the venue in another layer
Etc
Really interesting if you are into geodata
In all makes use of the Elastic Maps Service
If you want more information about this, you can find it here
Elastic Site Search Up till now there was news about new ways to handle your logs coming from application monitoring, infrastructure components, application logs
We did not hear about new things around search, until showing the new product called Elastic Site Search
This was previously known as Swiftype
With Google naming its product google search appliance end of life, this is a very interesting replacement
Working with relevance, synonyms, search analytics is becoming a lot easier with this new product
More information can be found here
Elastic cloud sizing If you previously looked at the cloud options elastic offers, you might have noticed that choosing elastic nodes did not give you a lot of flexibility
When choosing the amount of required memory, you also got a fixed amount of disk space
With the upcoming release, you have a lot more flexibility when creating your cluster
You can configure different flavours of clusters
One of them being hot-warm cluster
With specific master nodes, hot nodes for recent indexes with more RAM and faster disks, warm nodes containing the older indices with bigger disks
This is a good improvement if you want to create a cluster in the cloud
More information can be found here
Opening up X-Pack Shay told a good story about creating a company that supports an open source product
Creating a company only on support is almost impossible in the long run
Therefore they started working on commercial additions now called the X-Pack
Problem with these products was that the code was not available
Therefore working with elastic to help them improve the product was not possible
Therefore they are now opening up their repositories
Beware, it is not becoming free software
You still need to pay, but now it becomes a lot easier to interact with elastic about how stuff works
Next to that, they are going to make it easier to work with the free stuff in X-Pack
Just ask for a license once instead of every year again
And if I understood correct, the download will contain the free stuff in easier installation packages
More information about the what and why in this blog post from Shay Banon
Conference Party Nice party but I had to sign a contract to prohibit me from telling stories about the party
I do plan on writing more blog post the coming two daysKafka Streams is a lightweight library that reads from a Kafka topic, does some processing and writes back to a Kafka topic
It processes messages one by one but is also able to keep some state
Because it leverages the mechanics of Kafka consumer groups, scaling out is easy
Each instance of your Kafka Streams application will take responsibility for a subset of topic partitions and will process a subset of the keys that go through your stream
Kafka Streams at first glance One of the first things we need to keep in mind when we start developing with Kafka Streams is that there are 2 different API’s that we can use
There is the high-level DSL (using the KStreamsBuilder) and the low-level Processor API (using the TopologyBuilder)
The DSL is easier to use than the Processor API at the cost of flexibility and control over details
We will begin our journey by learning more about the DSL, knowing that sooner or later we will have to rewrite our application using the Processor API
It doesn’t take long before we stumble upon the word count example
code val source: KStream[String, String] = KStreamBuilderS.stream[String, String]("streams-file-input") val counts: KTableS[String, Long] = source .flatMapValues { value => value.toLowerCase(Locale.getDefault).split(" ") } .map { (_, value) => (value, value) } .groupByKey .count("Counts") Easy! Let’s just skip the documentation and finish our application! Our first Kafka Streams application Unfortunately, we soon return to the documentation because each aggregation on our KStream seems to return a KTable and we want to learn more about this stream/table duality
Aggregations also allow for windowing, so we continue reading about windowing
Now that we know something about the theoretical context, we return to our code
For our use case, we need 1 aggregate result for each window
However, we soon discover that each input message results in an output message on our output topic
This means that all the intermediate aggregates are spammed on the output topic
This is our first disappointment
A simple join The DSL has all the usual suspects like filter, map, and flatMap
But even though join is also one of those usual suspects that we have done a thousand times, it would be best to read the documentation on join semantics before trying out some code
For in Kafka Streams, there are a bit more choices involved in joining, due to the stream/table duality
But whatever our join looks like, we should know something about Kafka’s partitioning
Joins can be done most efficiently when the value that you want to join on, is the key of both streams and both source topics have the same number of partitions
If this is the case, the streams are co-partitioned, which means that each task that is created by the application instances, can be assigned to one (co-)partition where it will find all the data it needs for doing the join in one place
State Wherever there are aggregations, windows or joins, there is state involved
KS will create a new RocksBD StateStore for each window in order to store the messages that fall within that window
Unfortunately, we would like to have a lot of windows in our application, and since we also have about 500,000 different keys, we soon discover that this quickly grows out of hands
After having turned the documentation inside out, we learn that each one of those stores has a cache size of 100 MB by default
But even after we change this to 0, our KS application is too state-heavy
Interactive queries The same StateStores that allow us to do joining and aggregating also allows us to keep a materialized view of a Kafka topic
The data in the topic will be stored by the application
If the topic is not already compacted, the local key-value-store will compact your data locally
The data will constantly be updated from the topic it is built from
The store can be scaled out by running an extra instance of our application
Partitions will be divided among tasks, and tasks will be divided among instances, which results in each instance holding a subset of the data
This can lead to the situation where an instance is queried for a key that is contained in another instance
The queried instance may not be able to provide the value corresponding to the key, it knows, however, which other instance does hold the key
So we can relay the query
The downside is that we have to implement this API by ourselves
In order for this to work, we need an extra configuration element code val p = new Properties() p.put(StreamsConfig.APPLICATION_SERVER_CONFIG, s"${settings.Streams.host}:${settings.Streams.port}") Testing our app A popular library for unit-testing Kafka Streams apps seems to be MockedStreams
However, not all topologies can be successfully tested with MockedStreams
But we can skip unit testing; integration tests are more important, anyway
Should we try using some EmbeddedKafka or spin up docker containers with docker-it
In the future, testing Kafka Streams apps will hopefully be easier (https://cwiki.apache.org/confluence/display/KAFKA/KIP-247%3A+Add+public+test+utils+for+Kafka+Streams)
When testing our topology, we wonder why our stream is not behaving like we would expect
After a while, we start banging our heads against a wall
Then we turn the documentation inside out again and we find some settings that may be helpful
code p.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, "100") p.put(ProducerConfig.BATCH_SIZE_CONFIG, "1") Our second Kafka Streams application Then the inevitable happens: we need to do something that is not covered by the DSL; we need to resort to the Processor API
This would look a bit like the code below
code val builder: TopologyBuilder = new KStreamBuilder() builder .addSource("in", keySerde.deserializer(), valueSerde.deserializer(), settings.Streams.inputTopic) .addProcessor("myProcessor", new MyProcessorSupplier(), "in") .addSink("out", settings.Streams.outputTopic, keySerde.serializer(), valueSerde.serializer(), "myProcessor") The Processor interface lets us implement a process method that is called for each message, as well as a punctuate method that can be scheduled to run periodically
Inside these methods, we can use ProcessorContext.forward to forward messsages down the topology graph
Periodically, in Kafka 0.11, means stream-time which means punctuate will be triggered by the first message that comes along after the method is scheduled to run
In our case we want to use wallclock-time, so we use the ScheduledThreadPoolExecutor to do our own scheduling
But if we do this, the ProcessorContext might have moved to a different node in our topology and the forward method will have unexpected behavior
The workaround for this is to get hold of the current node object and pass it along to our scheduled code
code val currentNode = context.asInstanceOf[ProcessorContextImpl].currentNode().asInstanceOf[ProcessorNode[MyKey, MyValue]] In Kafka 1.0 a PunctuationType was introduced to make it possible to choose between wallclock-time and stream-time
Conclusion By the time we had finished our application, we had re-written it a few times, seen all parts of the documentation more often than we would like and searched through the KS source code
We were unfamiliar with all the settings and unaware of all the places where messages could be cached, delayed, postponed or dropped, and at certain moments we started doubting our notion of time
In retrospect, we should have kept things simple
Don’t use too much state
Don’t think a clever Processor will bend KS’s quirks in our favor
Don’t waste too much code on workarounds, because before you know it there will be a new version released that will break the API or obsolete our workarounds
And for those of you wanting to write an article or blog-post on Kafka Streams, make sure to finish it before the new release gets out
Resoures https://docs.confluent.io/current/streams/ http://www.bigendiandata.com/2016-12-05-Data-Types-Compared/ https://www.confluent.io/blog/avro-kafka-data/ https://github.com/confluentinc/kafka-streams-examples https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams+Join+SemanticsKafka Streams is a lightweight library that reads from a Kafka topic, does some processing and writes back to a Kafka topic
It processes messages one by one but is also able to keep some state
Because it leverages the mechanics of Kafka consumer groups, scaling out is easy
Each instance of your Kafka Streams application will take responsibility for a subset of topic partitions and will process a subset of the keys that go through your stream
Kafka Streams at first glance One of the first things we need to keep in mind when we start developing with Kafka Streams is that there are 2 different API’s that we can use
There is the high-level DSL (using the KStreamsBuilder) and the low-level Processor API (using the TopologyBuilder)
The DSL is easier to use than the Processor API at the cost of flexibility and control over details
We will begin our journey by learning more about the DSL, knowing that sooner or later we will have to rewrite our application using the Processor API
It doesn’t take long before we stumble upon the word count example
code val source: KStream[String, String] = KStreamBuilderS.stream[String, String]("streams-file-input") val counts: KTableS[String, Long] = source .flatMapValues { value => value.toLowerCase(Locale.getDefault).split(" ") } .map { (_, value) => (value, value) } .groupByKey .count("Counts") Easy! Let’s just skip the documentation and finish our application! Our first Kafka Streams application Unfortunately, we soon return to the documentation because each aggregation on our KStream seems to return a KTable and we want to learn more about this stream/table duality
Aggregations also allow for windowing, so we continue reading about windowing
Now that we know something about the theoretical context, we return to our code
For our use case, we need 1 aggregate result for each window
However, we soon discover that each input message results in an output message on our output topic
This means that all the intermediate aggregates are spammed on the output topic
This is our first disappointment
A simple join The DSL has all the usual suspects like filter, map, and flatMap
But even though join is also one of those usual suspects that we have done a thousand times, it would be best to read the documentation on join semantics before trying out some code
For in Kafka Streams, there are a bit more choices involved in joining, due to the stream/table duality
But whatever our join looks like, we should know something about Kafka’s partitioning
Joins can be done most efficiently when the value that you want to join on, is the key of both streams and both source topics have the same number of partitions
If this is the case, the streams are co-partitioned, which means that each task that is created by the application instances, can be assigned to one (co-)partition where it will find all the data it needs for doing the join in one place
State Wherever there are aggregations, windows or joins, there is state involved
KS will create a new RocksBD StateStore for each window in order to store the messages that fall within that window
Unfortunately, we would like to have a lot of windows in our application, and since we also have about 500,000 different keys, we soon discover that this quickly grows out of hands
After having turned the documentation inside out, we learn that each one of those stores has a cache size of 100 MB by default
But even after we change this to 0, our KS application is too state-heavy
Interactive queries The same StateStores that allow us to do joining and aggregating also allows us to keep a materialized view of a Kafka topic
The data in the topic will be stored by the application
If the topic is not already compacted, the local key-value-store will compact your data locally
The data will constantly be updated from the topic it is built from
The store can be scaled out by running an extra instance of our application
Partitions will be divided among tasks, and tasks will be divided among instances, which results in each instance holding a subset of the data
This can lead to the situation where an instance is queried for a key that is contained in another instance
The queried instance may not be able to provide the value corresponding to the key, it knows, however, which other instance does hold the key
So we can relay the query
The downside is that we have to implement this API by ourselves
In order for this to work, we need an extra configuration element code val p = new Properties() p.put(StreamsConfig.APPLICATION_SERVER_CONFIG, s"${settings.Streams.host}:${settings.Streams.port}") Testing our app A popular library for unit-testing Kafka Streams apps seems to be MockedStreams
However, not all topologies can be successfully tested with MockedStreams
But we can skip unit testing; integration tests are more important, anyway
Should we try using some EmbeddedKafka or spin up docker containers with docker-it
In the future, testing Kafka Streams apps will hopefully be easier (https://cwiki.apache.org/confluence/display/KAFKA/KIP-247%3A+Add+public+test+utils+for+Kafka+Streams)
When testing our topology, we wonder why our stream is not behaving like we would expect
After a while, we start banging our heads against a wall
Then we turn the documentation inside out again and we find some settings that may be helpful
code p.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, "100") p.put(ProducerConfig.BATCH_SIZE_CONFIG, "1") Our second Kafka Streams application Then the inevitable happens: we need to do something that is not covered by the DSL; we need to resort to the Processor API
This would look a bit like the code below
code val builder: TopologyBuilder = new KStreamBuilder() builder .addSource("in", keySerde.deserializer(), valueSerde.deserializer(), settings.Streams.inputTopic) .addProcessor("myProcessor", new MyProcessorSupplier(), "in") .addSink("out", settings.Streams.outputTopic, keySerde.serializer(), valueSerde.serializer(), "myProcessor") The Processor interface lets us implement a process method that is called for each message, as well as a punctuate method that can be scheduled to run periodically
Inside these methods, we can use ProcessorContext.forward to forward messsages down the topology graph
Periodically, in Kafka 0.11, means stream-time which means punctuate will be triggered by the first message that comes along after the method is scheduled to run
In our case we want to use wallclock-time, so we use the ScheduledThreadPoolExecutor to do our own scheduling
But if we do this, the ProcessorContext might have moved to a different node in our topology and the forward method will have unexpected behavior
The workaround for this is to get hold of the current node object and pass it along to our scheduled code
code val currentNode = context.asInstanceOf[ProcessorContextImpl].currentNode().asInstanceOf[ProcessorNode[MyKey, MyValue]] In Kafka 1.0 a PunctuationType was introduced to make it possible to choose between wallclock-time and stream-time
Conclusion By the time we had finished our application, we had re-written it a few times, seen all parts of the documentation more often than we would like and searched through the KS source code
We were unfamiliar with all the settings and unaware of all the places where messages could be cached, delayed, postponed or dropped, and at certain moments we started doubting our notion of time
In retrospect, we should have kept things simple
Don’t use too much state
Don’t think a clever Processor will bend KS’s quirks in our favor
Don’t waste too much code on workarounds, because before you know it there will be a new version released that will break the API or obsolete our workarounds
And for those of you wanting to write an article or blog-post on Kafka Streams, make sure to finish it before the new release gets out
Resoures https://docs.confluent.io/current/streams/ http://www.bigendiandata.com/2016-12-05-Data-Types-Compared/ https://www.confluent.io/blog/avro-kafka-data/ https://github.com/confluentinc/kafka-streams-examples https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams+Join+SemanticsNon-fiction books can be insightful, but I have noticed that I’d expect those insights to come back to me right when they would be helpful –– like when there’s an obstacle in a project, I will have an “a-ha” moment and remember that great idea I have read about years ago
In reality, not so much
So I’m trying something new
After reading a book, I’ll try to write down the most important stuff that could be useful some day in my digital notepad
I did this with the book Digital Transformation Book by David L
Rogers as well, a book I’ve written about before (for an introduction of the book, start there)
My notes quickly got annotated with my own experiences, and as I thought that would be valuable to share, I started to write this post
Rogers discusses in his book the digital transformation that requires pre-internet companies to rethink their business and way of doing things
Two themes are repeated throughout, and are obviously connected: Be agile and adapt; and have a company culture that is acceptable of change
Although primarily aimed at (C-level) managers, there is definitely some good stuff in it for designers and developers regarding those themes
Everything Is Marketing In this digital age the network of customers and stakeholders has grown and become more complex
Customers are no longer passive consumers that are just at the receiving end; they now interact with the brand, the market and each other
This also means that the number of touch points in the customer journey is growing, turning marketing into a company-wide activity
For the customer there is only one experience with the company: Using the product, visiting the company’s Twitter page or asking for support by email, it is all the same brand to the customer
All that should be in sync to such a degree that the customer will have consistent experience of the brand (and preferably a positive one)
In traditional companies where different departments take care of specific parts of the brand, it will be hard to achieve this
Those walls needs to be taken down, the silos dismantled
What can you do regarding this one brand perspective
To be more sensitive to the outside world and more customer-centric
Quite easily actually: Work together within your team beyond your own expertise to create a great product or service
We call this product thinking and my fellow designer Ivo Domburg wrote about this earlier in Dutch
On top of that, you can talk to people of other departments, outside of your team
Ask the people at customer service what kind of issues customers are experiencing with the product
Get to know what content is created for the website and social media accounts
There is no excuse here, you can literally walk over to the next room or floor to find these insights
Developing Great Products Let us zoom in on the development process
Concepts like working agile and having a culture that is acceptable of change should make sense to us as developers and designers
However, some companies or clients are less familiar with this mindset
There are some ways to do your part to help your company or client improve on this, even within a project
Rogers describes in his book experimental processes and principles based on what he observed at successful companies, which I have remixed using my own experience into the following five principles that can guide you in that process
Question The Assumptions In the beginning of a project, assumptions are made and new ideas are born founded on hunches, not data
That is a good thing, because you want that outside of the box creativity
But accepting these assumptions as the truth without verifying them is just gambling the whole project on a brainwave
These assumptions sometimes become a dogma within the company –– never questioned but also never proven to be right
Test those assumptions and various ideas early in the project and learn from them
Even if ideas are proven worthless, it could still lead to other insights and new ideas
Innovation means you wonder into unknown territories, so all you know could be worth being challenged
And this is not only true for new products but works evenly well if you are working on a new feature or improving an existing product
Iterate From The Start, And Get Consumer Feedback Quickly It happens quite often that weeks are spent on requirements and scoping of a project, resulting in a hefty backlog
Everything is pretty much set in stone, and it is hard to change direction if the first consumer response is timid
Therefore it is better to bring the concept as quickly as possible to real customers to get feedback and iterate on that
And work on the main concept here, don’t linger on the details –– if the concept doesn’t stick, it probably won’t be due to a detail! When doing this, speak to real or potential customers to get that feedback
Present your concept preferably in a form that is realistic enough for a consumer, even if it is still in the early stages
The person you show it to needs to get a pretty good sense of what the product or service is, how it could work and what the benefits for him or her would be
Not everyone can interpret abstract storyboards or wireframes the right way
Kick-off With An Intensive To kickstart a project without loads of requirements, we at Luminis do, as we call it, an Intensive with our client
It is short: It all happens in one day
With around five people (some designers and engineers from us and a few experts from the client, and perhaps even a end-user) we have a small and productive team
In the morning we start with the domain and marketplace, and the client discusses the potential he or she sees
In the afternoon we challenge assumptions, generate ideas and finish with some concepts or a first prototype at the end of the day
The Intensive not only helps to get us quickly into the context, it helps the client to get an outside perspective and some solutions that might work
And all within a day, making it fast and cheap
The results then can be shown to customers to get the first feedback
Fall In Love With The Problem, And Not The Solution Maybe the biggest reason the Intensive works is because it helps us and the client to understand the problem better – that is also why you should start with the marketplace and customer (and not that new technology everyone’s talking about)
The result of the Intensive itself is used to learn more about the problem
If the project continues after this kick-off session, we “throw” away the solution and continue with the problem
Falling in love with the problem helps you to have the right focus
It makes you consider what customers really want or are struggling with; that is where you should start
Do you already have feedback on existing products
Great, start from there
Can you observe the customer in the specific context you are interested in, even better
This helps you to see the bigger picture and it could even unexpectedly provide a solution
Falling in love with the solution is a trap
It limits the number of solutions you consider (because why look further if the team’s really happy with the solution on the table?), so it risks leaving a potentially better solution to be undiscovered
Loving a solution prevents you also from letting go of it when the response on it is too moderate or a better solution comes along
If an idea get’s the team really excited, it is worth putting it aside for a few days before jumping on it
Keep looking for other, maybe better, possibilities
The rush you get from that great idea could very well be worn off when you have another look at it two days later
Solving the right problem right is the goal to create truly great products
Fail Smart Finding the right solution and developing a great product won’t happen all the time
Some things may not work, and you and your team fail
It is important for all to expect this as a possible outcome and it shouldn’t be regarded as a negative event per se
When it happens, it does require a moment of reflection: Why did it go wrong, and did it happen as early and as cheaply as possible
And with answers to these questions, it is possible to improve the process
Keep in mind that you now only know what doesn’t work
Reflection is good but don’t let the failure limit the creativity or turn into a bureaucratic rule that becomes an obstacle in this or the next project
And reconsider sharing it with your colleagues and other teams
The next time the context could differ so much that it renders the learnings of the failure useless in that case
Process failures are more relevant to share than product failures
Make Great Products Whether if you work at a company that existed before the internet or work at a startup, the lessons described above could help you equally well
The concepts, processes and principles are valuable for all those involved in the development of digital products and services
A lot is about being open-minded, breaking through dogmas, and understanding the real issue the consumer has that requires a solution
Focussing on the problem, loving it, understanding it, and than solving it with a great product, that should keep you going and innovatingRecently, a customer reported that the performance of the web application we are building, an AngularJS front end using REST services from a Java back end, is unsatisfactory
I had a fun time finding out what the problem was, understanding what was causing it and thinking up solutions
I’d like to share the experience with you
What is the problem
“Bad performance” can mean many things
Actual response times are only part of the story; an application that shows feedback while gathering a response can make you wait longer before it is perceived as slow
When users know their bandwith is limited, for instance on a mobile device on a slower network, they anticipate a slower site
But of course, it is always possible that the services on which your site relies are just too slow
So, first order of business: ask the reporter what he means
What action did he perform, and how long did it take
How long did he expect it to take
It turned out there were several problem areas, not all of which were actually related to performance
I will describe two of these areas in more detail: loading of the AngularJS application loading of list views Loading of the AngularJS application Our AngularJS application has to perform several tasks before it can render the first view
This takes time, and during this time the end user is staring at a blank page
Finding out what happens To figure out what tasks are executed and which of these can most easily be sped up I opened a fresh Chrome window, trashed all the caches, opened the network panel and loaded the main page: This is interesting
There is a flurry of activity at the start, followed by 300 ms of silence, after which a lot more is loaded
What does it mean
Among the first items to be loaded is our JavaScript library, including AngularJS and other components
This is a large file, so it takes several hundreds of milliseconds to get loaded
After that, the AngularJS application is initializing (nothing happens) and after that, additional data is downloaded as the AngularJS application is building its first view
The largest of these is a translations file
The application won’t show anything until the translations are loaded, so the limiting path is: Loading of the JavaScript libraries Initialization of the AngularJS application Retrieving the translations file Improvements The initialization of the AngularJS application is not an easy one to speed up, so I’ll concentrate on the other two parts
The time it takes to download a JavaScript library is largely determined by its size
The library was already minified during the build
However, when a project is running for some time, it is not uncommon to have solved the same problem in two different ways, using two different libraries, or to end up with unused libraries
As it turned out, we were using two different libraries for file uploads
Choosing a single library for the uploads and removing libraries that are no longer used reduced the size of the final, minified JavaScript file and improved load time
The contents of the translations file were all used
However, since this is a plain text file, it was a prime candidate for zipped transfer
AngularJS handles zipped files out of the box, so the only thing we needed to do was add an @org.apache.cxf.annotations.GZIP annotation to the Apache CXF resource in the back end, which reduced the download time from several hundred milliseconds to ~10 ms
List views List views are always tricky, especially if there is no limit to the number of elements that may show up in the list, as was the case in some of our services
However, that turned out to not even be the biggest problem
Feedback: always When opening a page with a list, the application would show a spinner to indicate that the contents of the list were still being retrieved
The spinner would be removed, and the list shown, when the variable containing the list data was present on the scope: java <double-bounce-spinner ng-if="projects === undefined"></double-bounce-spinner> <div ng-if="projects !== undefined"> <ul> <li ng-repeat="project in projects"> <div><!-- project data here --></div> </li> </ul> </div> However, the controller that retrieves the projects had no error handling: java projectService.query() .then(function (projects) { $scope.projects = projects; }); So, if the back end service returned any error, the projects variable would not be set and the spinner would keep spinning - which made the testers conclude that the application was very slow! You should always provide feedback on the outcome of a back end call
At the very least, a spinner suggesting that data is retrieved should always be removed when a response is received
Showing the spinner should depend on whether a call is being performed, not an the presence of the result: java <double-bounce-spinner ng-if="loadingProjects"></double-bounce-spinner> java $scope.loadingProjects = true; projectService.query() .then(function(projects) { $scope.projects = projects; }) .finally(function() { $scope.loadingProjects = false; }); Even better would be to provide an error message if the back end returned an error: java $scope.loadingProjects = true; projectService.query() .then(function(projects) { $scope.projects = projects; }) .then(function(error) { // Process this to show an appropriate error message in the view }) .finally(function() { $scope.loadingProjects = false; }); Slow REST services Of course, there’s still the problem of REST services simply taking a long time to return a response
To monitor the response times for these services, we added simple metrics to all the REST services using Java Simon
This is a simple framework to gather performance data
You request a Stopwatch and use it to measure the call durations on the resource; it will then provide you with minumum, maximum and mean call durations: java @GET @Produces(MediaType.APPLICATION_JSON) public Response getProjects( final Split split = SimonManager.getStopwatch("getProjects").start(); final Response response; try { response = Response.ok(projectManagement.findAll()).build(); } finally { split.stop(); } return response; } java Stopwatch stopwatch = SimonManager.getStopwatch("getProjects"); long minimumDuration = stopwatch.min(); // minimum duration in nanoseconds long maximumDuration = stopwatch.max(); // maximum duration in nanoseconds long meanDuration = stopwatch.mean(); // mean duration in nanoseconds We then used Prometheus to provide access to the measurements
There are of course many reasons for a service to be slow, so I won’t go into details here, but this setup provided insight into which services are consistently slow so we could focus improvement efforts on those servicesIn this first of two blog posts I would like to discuss the book The Digital Transformation Playbook by David L
Rogers
The book is aimed mostly at (C-level) managers working at a company that needs to go through a digital transformation or is already undergoing it
Still, the book provides sufficient insights for developers and designers as well
The Digital Transformation Playbook often reads as an anthropological study of companies’ behaviours in the current digital age
This is not surprising, as Rogers (a faculty director of Columbia Business School) has been out into the field, consulting for companies such as Google, Toyota, Visa and GE on digital strategy
He has seen companies struggling with and responding to new technological opportunities this digital age has brought forth so far, and Rogers has collected these learnings and successful strategies in his book
Digital transformation is actually not about technology itself but about strategies on how to respond to technological developments
Reading The Playbook helps to understand the current behaviours of digital companies better, the strategies they pursue and why
These digital developments affect five domains of strategy: Customer networks
Customers are not a mass market any more but are active in a dynamic network and are often the key influencer (and not the company) due to social media and networks; Competition and platforms
Competition can come from anywhere as distinctions between industries become more fuzzy
The distinctions between partners and rivals also blur, with competitors working together in some areas and competing in others
Platforms bring different types of users easily and with low costs together, and contribute and receive different kinds of value; Data as assets
Data is key in value creation
It can be gathered easily and cheap, and when interlinked and made usable with analytical tools, is provides key business insights; Innovation by rapid experimentation
Moving away from shipping finalised products, and applying more testing and validation already during product development
Releasing minimum viable products, and iterating using user feedback; Adapting value propositions
Sticking to your value proposition is dangerous, a new competitor can come out of nowhere
The value proposition should be defined by the changing customer needs, so a company should always look for new opportunities and adapt
There are two take-aways that are repeated by Rogers throughout the book
The first one is to keep moving, improving, evolving
A company that does not respond to new digital technologies and the impact of it on its industry, will likely be overrun by a company that does
Like Andy Grove said, “only the paranoid survive” in this digital age
The second one is the realisation by Rogers that how ever urgent the transformation may be or how willing you are as an employee to pursue this, in the end it comes down to the willingness of the company’s board and culture to change
If these are limiting change, than those could be the biggest obstacles for a successful digital transformation
It is a pity that Rogers does not provide any advice here but that’s probably a topic worth a book on its own
With technological developments and innovations raging on, resulting in new digital possibilities and solutions, and shaping behaviour of consumers and companies in new ways, one can argue that this book will transform from a business strategy into a history book sooner than later
It is an observing book that does not provide universal truths, but truths with a best before date
This is not meant as a critique, however, as the book finds its strength especially in being observant, enlightening the reader in how the current business world is digitalising and transforming
Enlightening even those already active in this domain, like us as developers and designers
In a couple of weeks I’ll dive deeper into those learnings, and discuss how we as developers and designers can apply that in our daily workWhen developing or testing, having a database that comes pre-filled with relevant data can be a big help with implementation or a scenario walk-through
However, often there is only a structure dump of the production database available, or less
This article outlines the process of creating a Docker image that starts a database and automatically restores a dump containing a representative set of data
We use the PostgreSQL database in our examples, but the process outlined here can easily be applied to other databases such as MySQL or Oracle SQL
Dumping the database I will assume that you have access to a database that contains a copy of production or an equivalent representative set of data
You can dump your database using Postgres’ pg_dump utility: 1 pg_dump --dbname=postgresql://user@localhost:5432/mydatabase \ --format=custom --file=database.dmp We will be using the custom format option to create the dump file with
This gives us a file that can easily be restored with the pg_restore utility later on and ensures the file is compressed
In case of larger databases, you may also wish to exclude certain database elements from your dump
In order to do so you have the following options: The --exclude-table option, which takes a string pattern and ensures any tables matching the pattern will not be included in the dump file
The --schema option, which restricts our dump to particular schemas in the database
It may be a good idea to exclude pg_catalog – this schema contains among other things the table pg_largeobject, which contains all of your database’s binaries
See the Postgres documentation for more available options
Distributing the dump among users For the distribution of the dump, we will be using Docker
Postgres, MySQL and even Oracle provide you with prebuilt Docker images of their databases
Example 1: A first attempt In order to start an instance of a Postgres database, you can use the following docker run command to start a container based on Postgres: 1 docker run -p 5432:5432 --name database-dump-container \ -e POSTGRES_USER=user -e POSTGRES_PASSWORD=password \ -e POSTGRES_DB=mydatabase -d postgres:9.5.10-alpine This starts a container named database-dump-container that can be reached at port 5432 with user:password as the login
Note the usage of the 9.5.10-alpine tag
This ensures that the Linux distribution that we use inside the Docker container is Alpine Linux, a distribution with a small footprint
The whole Docker image will take up about 14 MB, while the regular 9.5.10 tag would require 104 MB
We are pulling the image from Docker Hub, a public Docker registry where various open source projects host their Docker images
Having started our Docker container, we can now copy our dump file into it
We first use docker execto execute a command against the container we just made
In this case, we create a directory inside the Docker container: 1 docker exec -i database-dump-container mkdir -p /var/lib/postgresql/dumps/ Following that, we use docker cp to copy the dump file from our host into the container: 1 docker cp database.dmp database-dump-container:/var/lib/postgresql/dumps/ After this, we can restore our dump: 1 docker exec -i database-dump-container pg_restore --username=user --verbose \ --exit-on-error --format=custom \ --dbname=mydatabase /var/lib/postgresql/dumps/database.dmp We now have a Docker container with a running Postgres instance containing our data dump
In order to actually distribute this, you will need to get it into a Docker repository
If you register with Docker Hub you can create public repositories for free
After creating your account you can login to the registry that hosts your repositories with the following command: 1 docker login docker.io Enter the username and password for your Docker Hub account when prompted
Having done this, we are able to publish our data dump container as an image, using the following commands: 1 2 docker commit database-dump-container my-repository/database-dump-image<br /> docker push my-repository/database-dump-image Note that you are able to push different versions of an image by using Docker image tags
The image is now available to other developers
It can be pulled and ran on another machine using the following commands: 1 docker pull my-repository/database-dump-image docker run -p 5432:5432 --name database-dump-container \ -e POSTGRES_USER=user -e POSTGRES_PASSWORD=password \ -e POSTGRES_DB=mydatabase -d my-repository/database-dump-image All done! Or are we
After we run the container based on the image, we still have an empty database
How did this happen
Example 2: Creating your own Dockerfile It turns out that the Postgres Docker image uses Docker volumes
This separates the actual image from data and ensures that the size of the image remains reasonable
We can view what volumes Docker has made for us by using docker volume ls
These volumes can be associated with more than one Docker container and will remain, even after you have removed the container that initially spawned the volume
If you would like to remove a Docker container, including its volumes, make sure to use the -v option: 1 docker rm -v database-dump-container Go ahead and execute the command, we will be recreating the container in the following steps
So how can we use this knowledge to distribute our database including our dump
Luckily, the Postgres image provides for exactly this situation
Any scripts that are present in the Docker container under the directory /docker-entrypoint-initdb.d/ will be executed automatically upon starting a new container
This allows us to add data to the Docker volume upon starting the container
In order to make use of this functionality, we are going to have to create our own image using a Dockerfile that extends the postgres:9.5.10-alpine image we used earlier: sql FROM postgres:9.5.10-alpine RUN mkdir -p /var/lib/postgresql/dumps/ ADD database.dmp /var/lib/postgresql/dumps/ ADD intialize.sh /docker-entrypoint-initdb.d/ The contents of initialize.sh are as follows: 1 pg_restore --username=user --verbose --exit-on-error --format=custom \ --dbname=mydatabase /var/lib/postgresql/dumps/database.dmp We can build and run this Dockerfile by navigating to its directory and then executing: 1 2 docker build --rm=true -t database-dump-image .<br /> docker run -p 5432:5432 --name database-dump-container \ -e POSTGRES_USER=user -e POSTGRES_PASSWORD=password -e POSTGRES_DB=mydatabase -d database-dump-image After starting the container, inspect its progress using docker logs -f database-dump-container
You can see that upon starting the container, our database dump is being restored into the Postgres instance
We can now again publish the image using the earlier steps, and the image is available for usage
Conclusions and further reading While working through this article, you have used a lot of important concepts within Docker
The first example demonstrated the usage of images and containers, combined with commands exec and cpthat are able to interact with running containers
We then demonstrated how you can publish a Docker image using Docker Hub, after which we’ve shown you how to build and run your own custom made image
We have also touched upon some more complex topics such as Docker volumes
After this you may wish to consult the Docker documentation to further familiarize yourself with the other commands that Docker offers
This setup still leaves room for improvement – the current process involves quite a lot of handwork, and we’ve coupled our container with one particular database dump
Please refer to the Github project for automated examples of this processPreviously, we’ve built a simple PV-DBOW-‘like’ model (https://amsterdam.luminis.eu/2017/02/21/coding-doc2vec/)
We’ve made a couple of choices, e.g., about how to generate training batches, how to compute the loss function, etc
In this blog post, we’ll take a look at the choices made in the popular gensim library
First, we’ll convince ourselves that we implemented indeed more or less the same thing :-)
Then, by looking at the differences, we’ll get ideas to improve and extend our own implementation (of course, this could work both ways ;-))
The first extension we are interested in is to infer a document vector for a new document
We’ll discuss how the gensim implementation achieves this
Disclaimer: You will notice that we’ll write this blog post in a somehwat dry, bullet-point style
You may use it for reference if you ever want to work on doc2vec
We plan to, anyway
If you see mistakes in our eyeball-interpretation of what gensim does, feel free to (gently) correct us; please refer to the same git commit version of the code against which we wrote this blog post, and use line numbers to point to code
Code walk through of gensim’s PV-DBOW We’ll start with the non-optimized Python module doc2vec (https://github.com/RaRe-Technologies/gensim/blob/8766edcd8e4baf3cfa08cdc22bb25cb9f2e0b55f/gensim/models/doc2vec.py)
Note that the link is to the specific version against which this blog post was written
To narrow it down, and to stay as close to our own PV-DBOW implementation, we’ll first postulate some assumptions: we’ll initialize the Doc2Vec class as follows d2v = Doc2Vec(dm=0, **kwargs)
That is, we’ll use the PV-DBOW flavour of doc2vec
We’ll use just one, unique, ‘document tag’ for each document
We’ll use negative sampling
The first thing to note about the Doc2Vec class is that is subclasses the Word2Vec class, overriding some of its methods
By prefixing methods with the class, we’ll denote which exact method is called
The super class object is then initialized as follows, in lines 640-643, by deduction: Word2Vec(sg=1, null_word=0, **kwargs) sg stands for Skip-Gram
Remember from elsewhere on the net that the Skip-Gram Word2Vec model is trained to predict surrounding words (for any word in a corpus)
Upon initialisation, Word2Vec.train() is called: a model is trained
Here, some parallelisation is taken care of that I will not go into at this point
At some point however, Doc2Vec._do_train_job() is called: in a single job a number of documents is trained on
Since we have self.sg = 1, Doc2Vec.train_document_dbow() is called there, for each document in the job
In this method, the model is trained to predict each word in the document
For this, Word2Vec.train_sg_pair() is used
Only, instead of two words, this method now receives the document tag and a word: the task is to correctly predict each word given the document tag
In this method, weights are updated
It seems, then, that at each stochastic gradient descent iteration, only one training example is used
Comparison of ours and gensim’s Doc2Vec implementation By just eyeballing the code, at first sight, the following similarities and differences stand out: Similarities: The network architecture used seems the same in both implementations: the input layer has as many neurons as there are documents in the corpus, there is one hidden layer, and the output layer has the vocabulary size
Neither implementation uses regularisation
Differences: One training example (document, term) per SGD iteration is used by gensim, whereas we allow computing the loss function over multiple training examples
All terms in a document are offered to SGD right after each other in gensim, whereas we generate batches consisting of several term windows from different documents
In gensim, the order in which training documents are offered is the same order each epoch; we randomize the order of term windows again each epoch
Inferring document vectors Given a new, unseen document, using Doc2Vec.infer_vector(), a document vector can be estimated anyway
How
Well, the idea is that we keep the word classifier that operates on the hidden space fixed
In other words, we keep the weights between the hidden layer and the output layer fixed
Now, we train a new mini-network with just one input neuron–the new document id–
We optimize the network such that the document gets an optimal position in the hidden space
In other words, again, we train the weights that connect this new document id to the hidden layer
How does gensim initialize the weights for the new input neuron
Randomly, set to small weights, just like it was done for the initial documents that were trained on
The training procedure consists of a fixed number of steps (we can choose how many)
At each step, all words in the document are offered as a training example, one after the other
Wrap-up We’ve seen that gensim’s implementation and ours do implement roughly the same thing, although there are a number of differences
This consolidates our position with our small ‘proof-of-concept’ implementation of doc2vec (https://amsterdam.luminis.eu/2017/02/21/coding-doc2vec/)
We’ve eyeballed how gensim’s doc2vec implementation manages to infer a document vector for an unseen document; now we are in a position to extend our own implementation to do the same
Of course, you can do it yourself, too!Just recently I’ve completed a UI redesign for a client’s existing application
The app is part of a bigger solution used at warehouse floors for picking orders
Throughout the years the application has grown with multiple customisations and fixes
When a customer opted for a new feature that was incompatible with this legacy and the UI, it became clear to the client that an overhaul was needed
As the designer for this project, the assignment for me was two-fold: Develop a UI that is flexible enough for customer-specific functionality, and create a clear interface that is usable for new users with as little explanation as possible while maintaining a low error-rate regarding picking
Our client already had some good ideas on the new app but I suggested to go into the field as well: How is the existing solution used and what do its users think about it
I hopped in a car with a sales rep to one of their long-term customers to gain these insights
Observing the users and the picking process was definitely helpful to understand the context better and see what is relevant in the app
However, the pickers worked lightning fast and had low error-rates, and even after talking to them, I only got one true issue with the current solution: An often-used button was so small that while working fast, it required an annoyingly slow and precise press by the picker, disrupting their flow
This is a crucial moment in the design process
Are you satisfied with your research
Visiting one customer and observing several end-users is in the end still n=1
Things like their picking process and their company culture determine a lot about what is observed during a visit, and are obviously very different at another company
It is always a good idea to have more observations to get a better sense of the product or service, and it is best to make sure the next customer or end-user differs enough from the first to cover the whole spectrum of the product’s users
In this case, the long-term customer had embraced the solution a long time ago, embedded it completely in their way of working and worked around problems they’d totally forgot about by now
So, the sales rep and I visited a second customer that was the opposite of this customer
Instead of being familiar with the solution, having permanent employees and running the warehouse for their own online shops, the second customer just started with this picking solution, they hired temporary workers and offered the warehouse services solely to other online shops
This second visit resulted in a very different afternoon
Being new to the app, they had several remarks and even suggestions for improving it
Not only did this visit confirm the necessity of the redesign as well, it also provided sufficient insights to make sure the new app would fit the domain and the use cases
For me, and likely most designers, starting the creative process with users, observations and tests is necessary to ground the solution in reality – to create a satisfying product or service that offers true improvement and fulfils a need
The experience of the above-mentioned project shows that context makes all the difference
Select a diverse range of users for your research: Don’t stick to one observation, you’re likely to miss out on a lot of insights for making a truly great productJava 9 brings modules as a core concept to the platform, but it’s more than just a language feature
With modules in Java 9, we can improve the design of code to increase maintainability and extensibility
As with every design principle, modularity requires thought and trade-offs to really reap the benefits
This session covers design practices for making codebases more maintainable and extensible
You will also find out about trade-offs to help you make the best choices
Topics include hiding implementations, using services for extensibility, API modules, avoiding cycles, optional dependencies, and dynamically loading modules
Familiarity with modules is helpful but not required
The speakers are the authors of Java 9 Modularity (O’Reilly)Soon after the release of elasticsearch it became clear that elasticsearch was good at more than providing search
It turned out that it could be used to store logs very effectively
That is why logstash was using elasticsearch
It contained standard parsers for apache httpd logs
To obtain the logs it had file monitoring plugins
It had plugins to extend and filter the content, and it had plugins to send the content to elasticsearch
That is Logstash in a nutshell back in the days
Of course the logs had to be shown, therefore a tool called Kibana was created
Kibana was a nice tool to create highly interactive dashboards to show and analyse your data
Together they became the famous ELK suite
Nowadays we have a lot more options in all these tools
We have Ingest node in elastic to pre-process documents before they move into elasticsearch, we have beats to monitor files, databases, machines, etc
And we have very nice and new Kibana dashboards
Time to re-investigate what the combination of Logstash, Elasticsearch and Kibana can do
In this blog post I’ll focus on Logstash
X-Pack As the company elastic has to make some money as well, they have created a product called X-Pack
X-Pack has a lot of features that sometimes span multiple products
There is a security component, by using this you can make users login in when using kibana and secure your content
Other interesting parts of X-Pack are machine learning, graph and monitoring
Parts of X-Pack can be used free of charge, you do need a license though
For other parts you need a paid license
I personally like the monitoring part so I regularly install X-Pack
In this blogpost I’ll also investigate the X-Pack features for Logstash
I’ll focus on out-of-the-box functionality and mostly what all these nice new things like monitoring and pipeline viewing bring us
Using the version 6 release candidate As elastic has already given us a RC1 of their complete stack, I’ll use this one for the evaluation
Beware though, this is still a release candidate, so not production ready
What does Logstash do If you never really heard about Logstash, let me give you a very short introduction
Logstash can be used to obtain data from a multitude of different sources
Than filter, transform and enrich the data
Finally store the data to again a multitude of datasources
Example data sources are relational databases, files, queues and websockets
Logstash ships with a large number of filter plugins, with these we can process data to exclude some fields
We can also enrich data, lookup information about ip addresses, or lookup records belonging to an id in for instance elasticsearch or a database
After the lookup we can add data to the document or event that we are handling before sending it to one or more outputs
Outputs can be elasticsearch, a database, but also queue’s like Kafka or RabbitMQ
In the later releases logstash started to add more features that a tool handling large amounts of data over longer periods need
Things like monitoring and clustering of nodes were introduced and also persisting incoming data to disk
By now logstash in combination with Kibana and Elasticsearch is used by very large companies but also by a lot of start ups to monitor their servers and handle all sorts of interesting data streams
Enough of this talk, let us get our hands dirty
First step install everything on our developer machines
Installation I’ll focus on the developer machine, if you want to install it on a server please refer to the extensive logstash documentation
First download the zip or tar.gz file and extract it to a convenient location
Now create a folder where you can store the configuration files
To make the files small and to show you that you can split them, I create three different files in this folder: input.conf, filters.conf and output.conf
The most basic configuration is one with a stdin for input, no filters and stdout for output
Below the contents for the two files html input { stdin{} } html output { stdout { codec => rubydebug } } Time to start logstash
Step into the downloaded and extracted folder with the logstash binaries and execute the following command
html bin/logstash -r -f ../logstashblog/ the -r, can be used during development for reloading the configuration on change
Beware, this does not work with the stdin plugin
With -f we tell logstash to load a configuration file or directory
In our case a directory containing the three mentioned files
When logstash is ready it will print something like this: html [2017-10-28T19:00:19,511][INFO ][logstash.pipeline ] Pipeline started {"pipeline.id"=>"main"} The stdin plugin is now waiting for input: [2017-10-28T19:00:19,526][INFO ][logstash.agent ] Pipelines running {:count=>1, :pipelines=>["main"]} Now you can type something and the result is the created document or event that went through the almost empty pipeline
The thing to notice is that we now have a field called message containing the text we entered
html Just some text for input { "@version" => "1", "host" => "Jettros-MBP.fritz.box", "@timestamp" => 2017-10-28T17:02:18.185Z, "message" => "Just some text for input" } Now that we know it is working, I want you to have a look at the monitoring options you have available using the rest endpoint
html http://localhost:9600/ { "host": "Jettros-MBP.fritz.box", "version": "6.0.0-rc1", "http_address": "127.0.0.1:9600", "id": "20290d5e-1303-4fbd-9e15-03f549886af1", "name": "Jettros-MBP.fritz.box", "build_date": "2017-09-25T20:32:16Z", "build_sha": "c13a253bb733452031913c186892523d03967857", "build_snapshot": false } You can use the same url with different endpoints to get information about the node, the plugins, stats and hot threads: http://localhost:9600/_node http://localhost:9600/_node/plugins http://localhost:9600/_node/stats http://localhost:9600/_node/hot_threads It becomes a lot more fun if we have a UI, so let us install xpack into logstash
Before we can run logstash with monitoring on, we need to install elasticsearch and kibana with X-pack installed into those as well
Refer to the X-Pack documentation on how to do it
The basic commands to install x-pack into elasticsearch and kibana are very easy
For now I disable security by adding the following line to both kibana.yml and elasticsearch.yml: xpack.security.enabled: false
After installing x-pack into logstash we have to add the following lines to the logstash.yml file in the config folder html xpack.monitoring.elasticsearch.url: ["http://localhost:9200"] xpack.monitoring.elasticsearch.username: xpack.monitoring.elasticsearch.password: Notice the empty username and password, this is required when security is disabled
Now move over to Kibana and check the monitoring tab (the heart shape figure) and click on logstash
In the first screen you can see the events, they could be zero, zo please enter some events
Now move to the pipeline tab
Of course with our basic pipeline, this is a bit stupid, but imagine what it will show later on
Time to get some real input
Import the Signalmedia dataset Signalmedia has provided a dataset you can use for research
More information about the dataset and how to obtain it can be found here
The dataset contains an exact amount of 1 million news documents
You can download the file as a file that contains a JSON document on each line
The JSON document has the following format: html { "id": "a080f99a-07d9-47d1-8244-26a540017b7a", "content": "KUALA LUMPUR, Sept 15 (MySinchew) -- The Kuala Lumpur City Hall today issued ...", "title": "Pay up or face legal action: DBKL", "media-type": "News", "source": "My Sinchew", "published": "2015-09-15T10:17:53Z" } We want to import this big file with all the JSON documents as separate documents into elasticsearch using logstash
The first step is to create a logstash input
Use the path to point to the file
We can use the logstash file plugin to load the file, tell it to start at the beginning and mark each line as a JSON document
The file plugin has more options you can use
It can also handle rolling files that are used a lot in logging
html input { file { path => "/Volumes/Transcend/signalmedia-1m.jsonl" codec => "json" start_position => beginning } } That is it, with the stdout plugin and the rubydebug codec this would give the following output
html { "path" => "/Volumes/Transcend/signalmedia-1m.jsonl", "@timestamp" => 2017-10-30T18:49:45.948Z, "@version" => "1", "host" => "Jettros-MBP.fritz.box", "id" => "a080f99a-07d9-47d1-8244-26a540017b7a", "source" => "My Sinchew", "published" => "2015-09-15T10:17:53Z", "title" => "Pay up or face legal action: DBKL", "media-type" => "News", "content" => "KUALA LUMPUR, Sept 15 (MySinchew) -- The Kuala Lumpur City Hall today issued ..." } Notice that besides the fields we expected: id, content, title, media-type, source and published we also got some additional fields
Before sending this to elasticsearch we want to clean it up
We do not need the path, host, @timestamp, @version
There is also something with the field id
We want to use the id field to create the document in elasticsearch, but we do not want to add it to the document
If we need the value of id in the output plugin later on, but we do not want to add it as a field to the document we can move it to the @metadata object
That is exactly what the first part of the filter does
The second part removes the fields we do not need
html filter { mutate { copy => {"id" => "[@metadata][id]"} } mutate { remove_field => ["@timestamp", "@version", "host", "path", "id"] } } With these filters in place the output of the same document would become: html { "source" => "My Sinchew", "published" => "2015-09-15T10:17:53Z", "title" => "Pay up or face legal action: DBKL", "media-type" => "News", "content" => "KUALA LUMPUR, Sept 15 (MySinchew) -- The Kuala Lumpur City Hall today issued ..." } Now the content is ready to be send to elasticsearch, so we need to configure the elasticsearch output plugin
When sending data to elastic you first need to think about creating the index and the mapping that goes with it
In this example I am going to create an index template
I am not going to explain a lot about the mappings as this is not an elasticsearch blog
But with the following code we insert the mapping template when connecting to elasticsearch and we can insert all documents
Do look at the way the document_id is created
Remember we talked about that @metadata and how we copied the id field into it
This is the reason why we did it
Now we use that value as the id of the document when inserting it into elasticsearch
html output { elasticsearch { index => "signalmedia" document_id => "%{[@metadata][id]}" document_type => "doc" manage_template => "true" template => "./signalmedia-template.json" template_name => "signalmediatemplate" } stdout { codec => dots } } Notice there are two outputs configured
The elasticsearch output of course, but also a stdout
This time not with the rubydebug codec, this would be way to verbose
We use the dots codec
This codec prints a dot for each document it parses
For completeness I also want to show the mapping template
In this case I positioned it in the root folder of the logstash binary, usually this would of course be an absolute path somewhere else
html { "index_patterns": ["signalmedia"], "settings": { "number_of_replicas": 0, "number_of_shards": 3 }, "mappings": { "doc": { "properties": { "source": { "type": "keyword" }, "published": { "type": "date" }, "title": { "type": "text" }, "media-type": { "type": "keyword" }, "content": { "type": "text" } } } } } Now we want to import all the million documents and have a look at the monitoring along the way
Let’s do it
Running a query Of course we have to prove the documents are now available in elasticsearch
So lets execute one of my favourite queries that makes use of the new significant text aggregation
First the request and then parts of the response
html GET signalmedia/_search { "query": { "match": { "content": "netherlands" } }, "aggs": { "my_sampler": { "sampler": { "shard_size": 200 }, "aggs": { "keywords": { "significant_text": { "field": "content", "filter_duplicate_text": true } } } } }, "size": 0 } Just a very small part of the response, I stripped out a lot of the elements to make it better viewable
Good to see that that see dutch as a significant word when searching for the netherlands and of course geenstijl
html "buckets": [ {"key": "netherlands","doc_count": 527}, {"key": "dutch","doc_count": 196}, {"key": "mmsi","doc_count": 7}, {"key": "herikerbergweg","doc_count": 4}, {"key": "konya","doc_count": 14}, {"key": "geenstijl","doc_count": 3} ] Concluding Good to see the nice ui options in Kibana
The pipeline viewer is very useful
In a next blog post I’ll be looking at Kibana and all the new and interesting things in thereWhen working with organization, mostly when giving Scrum training, I often get asked about certifications
And while I fundamentally disagree with the obsession that certain P&O departments have with certifications, they do have a certain merit
In the Netherlands there seems to be a focus on the CSM (Certified Scrum Master) certification by the Scrum Alliance
And generally that is the one my students ask about
And I agree CSM is very good indeed
The trainers need to be certified to give the training so they tend to be really good
I have worked with a number of Certified Scrum Trainers and they are great
So, no objections against CSM at all
But, I tend to prefer a lesser known certification: PSM (Professional Scrum Master)
Where CSM is a certification by the Scrum Alliance, PSM is offered Scrum.org which was founded by Ken Schwaber
Ken is one of the original forces behind Scrum so this is not a shady organization
Even though these are two different certifications, they have a lot in common: Both: • Fundamentally test knowledge (theoretical) of Scrum, plus some insight into the philosophy behind Scrum and the way to use it • Are granted based on an on-line, open book, multiple choice exam • Do not really require practical experience (but it helps) So, they seem to be roughly interchangeable in what they certify and how they do it
But there are a few differences: • CSM requires a course by a Certified Scrum Trainer, the exam is almost an afterthought
Typical costs are around $2000 per person
This means the level of the training is great, but it gives less flexibility
PSM has no such requirement, you can use any training you want (at your own risk) or take the exam if you think you already know all there is to know • CSM requires 25 out of 35 questions to be answered correctly (71%), PSM requires 68 out of 80 questions to be answered correctly (85%) • CSM has no time limit and can be continued at any time
PSM has a time limit of 60 minutes and questions need to be answered in a single session, • CSM exam is included in the Scrum course, PSM exam is charged separately ($150) • CSM is valid for 2 years and then requires renewal ($250), PSM remains valid So, CSM is clearly more expensive
But is it better
Not in my mind
The exam for PSM is harder so you need to be at a higher level of competence to get PSM
It is true that you are dependent on the level of the trainer when going for PSM, which is almost guaranteed with CSM
But there are a lot of good trainers around
And if you can’t find one, contact me! So, if I were in a P&O position, I would rather hire a PSM than a CSM
But to be honest, neither of those guarantee that the holder of the certification is a good Scrum Master
There is a real difference between knowledge of the theory and using that effectively in the real world
This is why am a big believer in experience-based certifications such as the Agile Master certification by the Agile Consortium, but that is something for a future blogWriting software is hard
Writing good software is harder
Writing good, simple software is the hardest
Writing good, simple software as a team is the hardest… est
You shouldn’t need several hours to understand what a method, class or package does
If you need a lot of mental effort to even start programming, you will run out of energy before you can produce quality
Reduce the cognitive load of your code and you will reduce its amount of bugs
“This is puristic nonsense!” you say
“Intellectual masturbation! Software should be modeled after real-world, mutable objects!” I’m not saying you should go on a vision quest and return as a hardcore functional programmer
That wouldn’t be very productive
Functional and object-oriented programming can complement each other greatly
I will show you how
But first, let’s get to the what and why of it
What
A pure function is a function that: …always returns the same result, given the same input
…does not have any side effects
Put simply: When calling a pure function with input A, it always returns B, no matter how often, when and where you call it
Also, it does nothing else
In Java, a pure function might look like this: public static int sum(int a, int b) { return a + b; } If a is 2 and b is 3, the result is always 5, no matter how often or how fast you call it, even if done so concurrently
As a counterexample, this would be an impure function: public static int sum(int a, int b) { return new Random().nextInt() + a + b; } Its output could be anything, no matter the input
It violates the first rule
If this would be part of a program, the program would become nigh impossible to reason about
An example of the second rule, a function that has side effects: public static int sum(int a, int b) { writeSomethingToFile(); return a + b; } Even though this method is static and always returns the same value, it is impure
It does more than it advertises
Even worse, it does so secretly
These examples may look simple and harmless
But impurity adds up quickly
Why
Let’s say you have a decent-sized program you want to add a new feature to
Before you can do so, you’ll have to know what it does
You will probably start by looking at the code, reading documentation, following method calls, et cetera
While you are doing this, you are creating a mental model of the program
You are trying to fit every possible flow of the data inside your mind
A human brain is not optimised for this task
After a few state changes here, some conditionals there, you start to get somewhat foggy
Now we have a breeding ground for bugs
So how can we prevent this
By reducing the complexity of our program, for starters
Pure functions can be of great help here
How
A great way to clarify complex code is breaking it up into smaller, more manageable pieces
Ideally, every bit has its own responsibility
This makes them easier to reason about, especially if we can test them in isolation
While untangling the spaghetti, you will notice you are now able to free up some of your intellectual capabilities
You are slowly getting at the core of the problem
Every piece that can stand on its own can be moved and you are left with the essence of your program
Finding that bug is much easier now
So is adding a cool new feature
To make things more concrete, here are some guidelines
Partition Firstly, partition the bigger pieces by their functionality
Think about the various parts of the solution and how they interconnect
Break Up Break up those parts into composable units
Define a function that can be used to filter items from a list, or an action that can be re-used for every single item
Maybe add helper functions that encapsulate logic that would otherwise end up inside deeply nested code
Document Write documentation when you feel you are ‘done’ implementing a unit
This will help you see the logic from a different perspective and reveals unforeseen edge cases
If necessary, add unit tests to further define the program’s intents
Rinse and repeat
The definition of done is personal and can change over time
Don’t overdo it
If it’s still not clear enough, you will find out later
If not during a code review, maybe when adding a new feature to the project
So far, so good, right
Theory always sounds great
But we need some convincing in the form of a practical example
An Example Let’s say we have a client that sells IoT devices
Customers can install these devices in their home and connect them to the internet
They can control these devices with an app on their mobile phone
Our client wants their customers to receive a push notification if their device has been offline for a while, so they have a chance to reconnect the device
If it stays offline, they want to push notifications periodically, to remind the customer to reconnect the device
But not too often, since they don’t want unstable devices to cause an endless stream of warnings
Also, our customer wants to be able to adjust the thresholds for sending these messages
At runtime, without having to pay us to do it for them
There should be at least one threshold, but there could be any number of them
Using our backend software, we can detect the online status of these IoT devices
If they go offline, we store the moment they went offline
Naive Implementation How can we implement this
This feature looks quite simple on paper, so we could just start and see how far we’d get
After doing some scaffolding we quickly get to the core of the problem: determining whether notifications should be sent for each offline device
Sounds simple, right
public void run() { // @todo Send notifications for offline devices } We add some for loops here
for (Map.Entry<Device, Instant> offlineDevice : offlineDevices.entrySet()) { for (Duration threshold : thresholds) { // ..
} } Some if statements there
if (firstThresholdWasPassed) { // ..
} Looking good! Wait, there’s a special case for the last threshold
if (i == thresholds.size()) { // ..
} Oh, and one for a single threshold
if (thresholds.size() == 1) { // ..
} Crap, we forgot to check if the notification for each threshold was already sent
In 3 places
if (!lastOfflineNotificationInstant.isPresent()) { // ..
} And what if it was sent before the device went offline
if (Duration.between(disconnectInstant, lastOfflineNotificationInstant.get()).isNegative()) { // ..
} Oh my
SonarLint is foaming at the mouth, telling us to reduce the cognitive complexity of 69, where 15 is allowed
The unit tests are getting quite complex as well, having to use an external Clock an whatnot
We cannot let our future selves see this, they’d kill us! Luckily, we have our three-step program
Let’s begin at 1
Partitioning by Functionality Now, to untangle this mess
Our first step is to partition the bigger pieces by functionality, to think about the various parts of the solution and how they interconnect
We could visualize our problem by drawing a timeline, starting at the instant a device went offline: On the timeline we can plot the various thresholds: Every time our job is running, we have arrived at a certain point on the timeline
If we have not yet passed a threshold, nothing needs to be done: If we do pass a threshold, we should send a notification and remember the time we sent it: Next time we run our job, we should take the sent notification into account
If we have passed a threshold that the user was already notified about, we do nothing: Only when passing another threshold, we can send another notification: And so on and so forth
How do we express this in code
It seems we need to know: Which threshold (if any) we passed last
Which threshold (if any) we sent the last notification for
That’s it, basically
If we know these, we can determine if we should send a notification
All the special cases and complexity are captured by these two core parts of our solution
Now we need to see if we can simplify our solution any further by breaking it up into composable parts
Breaking It Up Into Composable Units The two above statements look awfully familiar, no
That’s because they are
They both define a threshold or nothing
They both need similar input: The moment the device went offline
The threshold(s)
The point in time we are currently evaluating
Now that we know the input and output of our unit, we can define its signature
Using the fancy java.time API, we can express it as such: Optional calculateLastPassedThreshold(Instant start, Instant current, Duration[] thresholds); We can now use this function to get both thresholds we needed
Did I say function
Let’s make that a pure function: declare it static and make sure it’s deterministic and doesn’t create any side effects: static Optional<Duration> calculateLastPassedThreshold(Instant start, Instant current, List<Duration> thresholds) { Duration timePassed = Duration.between(start, current); if (timePassed.compareTo(thresholds.get(0)) <= 0) { return Optional.empty(); } for (int i = 0; i < thresholds.size(); i++) { if (timePassed.compareTo(thresholds.get(i)) <= 0) { return Optional.of(thresholds.get(i - 1)); } } return Optional.of(thresholds.get(thresholds.size() - 1)); } There! Aside from its breathtaking purity, it has another pro: it can be unit tested very easily
You don’t need mocks nor external clocks to test this part of the code
Can you feel the amount of space your brain just regained
When programming the rest of our solution, we can just trust that this function will do what it says
So what’s left
We can now determine, for each device, if we should send a notification
Again, we can isolate this part of our program, make it pure and test it thoroughly: static boolean shouldSendNotification(Instant jobStart, Instant deviceOffline, Instant lastNotification, List<Duration> thresholds) { Optional<Duration> lastPassedThreshold = calculateLastPassedThreshold(deviceOffline, jobStart, thresholds); if (!lastPassedThreshold.isPresent()) { return false; } if (lastNotification.isBefore(deviceOffline)) { return true; } Optional<Duration> lastPassedThresholdNotifiedAbout = calculateLastPassedThreshold(deviceOffline, lastNotification, thresholds); return !lastPassedThreshold.equals(lastPassedThresholdNotifiedAbout); } Or, more concisely: static boolean shouldSendNotification(Instant jobStart, Instant deviceOffline, Instant lastNotification, List<Duration> thresholds) { Optional<Duration> lastPassedThreshold = calculateLastPassedThreshold(deviceOffline, jobStart, thresholds); return lastPassedThreshold.isPresent() && (lastNotification.isBefore(deviceOffline) || !lastPassedThreshold.equals(calculateLastPassedThreshold(deviceOffline, lastNotification, thresholds))); } And when you know the device has no previously sent notifications, you can just use: static boolean shouldSendNotification(Instant jobStart, Instant deviceOffline, List<Duration> thresholds) { return calculateLastPassedThreshold(deviceOffline, jobStart, thresholds).isPresent(); } Now the only thing left is calling these functions for every offline device, every time our job runs: public void run() { Instant jobStart = Instant.now(); offlineDevices.entrySet().stream() .filter(offlineDevice -> pushNotificationService .getLastOfflineNotificationInstant(offlineDevice.getKey()) .map(instant -> shouldSendNotification(jobStart, offlineDevice.getValue(), instant, thresholds)) .orElseGet(() -> shouldSendNotification(jobStart, offlineDevice.getValue(), thresholds)) ) .forEach(offlineDevice -> pushNotificationService.sendOfflineNotification(offlineDevice.getKey())); } We’re bordering on iamverysmart-territory here
Not everyone likes this style of coding and it is arguably harder to parse than a more traditional style
So the least we can do is document our units
Documenting our solution Aside from playing nice with others, describing our code can also help us catch that last bug, or reveal an edge case we hadn’t thought of yet
Documentation in our case most obviously comes in the form of Javadoc, for example: /** * Checks whether a notification should be sent by determining which threshold has been passed last for the * calculated amount of time passed between the device going offline and the job running
* * @param jobStart The instant the job calling this function was started
* @param deviceOffline The instant the device went offline
* @param lastNotification The instant the last notification was sent
* @param thresholds The list of notification thresholds
* @return True if the notification should be sent, false if not
*/ static boolean shouldSendNotification(Instant jobStart, Instant deviceOffline, Instant lastNotification, List<Duration> thresholds) { // ..
} Unit tests can also be regarded as documentation since they can define the intended use of the program under test very precisely
In the case of pure functions, where you often need to test a list of varying input, parameterized tests can come in handy
While writing tests and documentation, we shift our thinking from the problem to the solution, and its users
By thinking from this other perspective, subtle problems in our code are highlighted and can be fixed on the spot
Afterthoughts It can be hard to convince others or even yourself of the value of this approach
Churning out functionality has more obvious business value than meticulously crafting software
When approaching existing code this way, you can end up wasting hours while adding no measurable (short-term) value
Try explaining that during the daily stand-up
On the other hand, preventing a bug is cheaper than fixing it, and the more clean your code is, the less time it takes to add a feature or find an error, especially if the code hasn’t been touched in a while
Somewhere along the line, the balance is tipped
On one side lies the holy grail, the unachievable perfect piece of software
On the other side, the unmaintainable spaghetti monster slithers, shitting on deadlines, ballooning budgets and destroying teams
Listen to your instincts, communicate your intentions, listen to your peers and learn from one another: making great software is an ongoing (team) effort
Source code All code referenced in this post is open source and can be found in its GitHub repository
Feel free to check it out, tinker with it and come up with better solutions
Feedback I’d love to hear your feedback in the form of pull requests, issues or comments! There are some nice discussions going on over here: reddit.com/r/programming reddit.com/r/java Or you can post your comments belowWhen looking at the definition of Agile frameworks like Scrum, it is clear that they are really meant to be used within single organizations
Organizations that either work with their internal IT department, or even better have parts of their IT embedded in the normal business
And while the benefits of working in an Agile way are very clear, most organizations find that it is very hard to extend this way of working to a situation where you work with external partners
The main difference between “internal” and “external” is that there is an implicit trust of the internal resources, but a distinct distrust of external parties
We trust the people we work with, but distrust those external companies that will bill us for every last cent
Whether or not this feeling is justified is beyond the point, but the feeling persists almost everywhere
The “remedy” is almost as universal: iron-clad contracts that explicitly specify what the external party is supposed to deliver, when that should be, milestones and often even penalties for missing deadlines
Now let’s count the number of Agile Values that we are violating in this way
I will do it for you: 4 out of 4
No way you can successfully work in an Agile way with an external partner this way
So, how do we solve this
At Luminis we face this situation daily
We want to help clients achieve their goals and believe that software it the most important asset they have in achieving those
We want to work together with them to help them succeed, we do not want to be in a position where we delivered what we promised but that turns out to be the wrong thing
In short, we want to be Agile and work with the client not against the client as is the case in traditional software development
The start of the solution is to explain a few things to the client: 1
Whatever you write down in your iron-clad contract is going to change
You know before you start that what you write down will be obsolete before the project ends
At the end of the day, do you want what you are going to write in the contract or something that will solve the real problem
2
Agile, more specifically Scrum, is not “do whatever you like”
It is agreeing per sprint what should be delivered, checking after (most often) two weeks, learning and improving all the time
You will never hear at the end of a project that there is a problem, you will hear it after a few weeks
This improves your control, and does not reduce it
3
Working Agile gives you control (previous point), but also means that you are able to change, extend, or reduce the scope at any point
You are the final decider on the backlog, you decide the priority nobody else
You can change your mind at any time
And if there is a blocking problem that means the project should not proceed, you can get out also
While it may seem that working in an Agile way means less control and more uncertainty, after all scope is going to be variable and the team decided what they can do, that is not the case at all
You gain control by frequent checks and you gain control by the flexibility of managing the backlog
Sure, you may not get what you wanted at the beginning, but you surely will get what you want at the end
The only “but” here is that you have to have a certain amount of trust that the external partner you work with is really going to do their best to help you
But ask yourself: if you do not trust in that, should you even work with that partner
Given that basic trust, working in an Agile way gives you a lot better chance of success in the end
There are a few ways to gain this level of trust
First of all, any client is free to talk to previous clients and get to know from them whether or not we are trustworthy
But secondly, we actively try to get at least one internal IT person to join our team
That person has inside knowledge of what we do and can therefore check whether we are really doing as we promised
Another frequently heard objection is that the specifications are not done at the beginning (and we do not want them to be!), so the Product Owner needs to spend a lot of time during the project and he/she does not have the time to perform that role
We mostly counter that by using a delegated Product Owner; one of our analysts and Agile experts that performs the daily interaction with the team
While they are with Luminis, they act as though they are part of the client company even if that conflicts with the interests of Luminis
The delegated Product Owner frequently communicates with the real Product Owner in the client company regarding specifications and priorities
This way the internal Product Owner remains in charge and control, but most of the day-to-day work is delegated
To sum up: • Given basic trust, working Agile with external partners has a lot of advantages • Gain trust by talking to previous clients, and try to have one team internal team member join the external team • Delegate the day-to-day tasks of the Product Owner We find that by using this structure we are able to really cooperate with our client, and together make the client successfulFor some time now, elasticsearch has been releasing versions of the new major release elasticsearch 6
At this moment the latest edition is already rc1, so it is time to start thinking about migrating to the latest and greatest
What backwards compatible issues will you run into and what new features can you start using
This blog post gives a summary of the items that are most important to me based on the projects that I do
First we’ll have a look at the breaking changes, than we move on to new features or interesting upgrades
Breaking changes Most of the breaking changes come from the elasticsearch documentation that you can of course also read yourself
Migrating indexes from previous versions As with all major release, only indexes created in the prior version can be migrated automatically
So if you have an index created in 2.x, migrated it to 5.x and now want to start using 6.x you have to use the reindex API to first index it into a 5.x index before migrating
Index types In elasticsearch 6 the first step is taken into indexes without types
The first step is to allow only a single type within a new index and be able to keep using multiple types in indexes migrated from 5.x
Starting with elasticsearch 5.6 you can prevent people from creating indexes with multiple types
This will make it easier to migrate to 6.x when it becomes available
By applying the following configuration option you can prevent people from making multiple types in one index java index.mapping.single_type: true More reasoning about why the types need to be removed can be found in elasticsearch documentation removal of types
Also if you are into parent-child relationships in elasticsearch and are curious what the implications of not being able to use multiple types are, check this documentation page parent-join
Yes will will get joins in elasticsearch :-), though with very limited use
Java High Level REST Client This was already introduced in 5.6, still good to know as this will be the replacement for the Transport client
As you might know I am also creating some code to use in Java Applications on top of the Low Level REST client for java that is also being used by this new client
More information about my work can found here: part 1 and part 2
Uniform response for create/update and delete requests At the moment a create request returns a response field created true/false, and a delete request returns found true/false
If you are someone trying to parse the response and using this field, you can no longer use this
Use the result field instead
This will have the value created or updated in case of the create request and deleted or not_found in case of the delete request
Translog change The translog is used to keep documents that have not been flushed to disk yet by elasticsearch
In prior releases the translog files are removed when elasticsearch has performed a flush
However, due to optimisations made for recovery having the translog could speed the recovery process
Therefore the translog is now kept for by default 12 hours or a maximum of 512 Mb More information about the translog can be found here: Translog
Java Client In a lot of java projects the java client is used
I have used it as well for numerous projects
However, with the introduction of the High Level Rest client for java projects should move away from the Transport Client
If you want/need to keep using it for now, there are some changes in packages and some methods have been removed
For me the one I used the most is the the order for aggregations, think about Terms.Order and Histogram.Order
They have been replaced by BucketOrder Index mappings There are two important changes that can affect your way of working with elastic
The first is the way booleans are handled
In indexes created in version 6, a boolean accepts only two values: true and false
Al other values will result in an exception
The second change is the _all field
In prior version by default an _all field was created in which all values of fields were copied as strings and analysed with the standard analyser
This field was used by queries like the query_string
There was however a performance penalty as we now had to analyse and index a potentially big field
Soon it became a best practice to disable the field
In elasticsearch 6 the field is disabled by default and it cannot be configured for indices created with elasticsearch 6
If you still use the query_string query, it is now executed agains each field
You should be very careful with the query_string query
It comes with a lot of power
Users get a lot of options to create their own query
But with great power comes great responsibilities
They can create very heavy queries as well
And they can queries that break without a lot of feedback
More information about the query_string
If you still want to give you users more control, but the query_string query is one step to far, think about creating your own search DSL
Some ideas can be found in one of my previous blog posts: Creating a search DSL and Part 2 of creating a search DSL
Booting elasticsearch Some things changed with the startup options
You cannot configure the user elasticsearch runs with if you use the deb or rpm packages and the elasticsearch.yml file location is now configured differently
Now you have to export the path where to find all configuration files (elasticsearch.yml, jvm.options and log4j2.properties)
You can expose an environment variable ES_PATH_CONF containing the path to the config folder
I use this regularly on my local machine
As I have multiple projects running often with different version of elasticsearch I have setup a structure where I put my config files in separate folders from the elasticsearch distributable
Find the structure in the image below
In the beginning I just copy the config files to my project specific folder
When I start the project with the script startNode.sh the following script is executed
java #!/bin/bash CURRENT_PROJECT=$(pwd) export ES_PATH_CONF=$CURRENT_PROJECT/config DATA=$CURRENT_PROJECT/data LOGS=$CURRENT_PROJECT/logs REPO=$CURRENT_PROJECT/backups NODE_NAME=Node1 CLUSTER_NAME=playground BASH_ES_OPTS="-Epath.data=$DATA -Epath.logs=$LOGS -Epath.repo=$REPO -Enode.name=$NODE_NAME -Ecluster.name=$CLUSTER_NAME" ELASTICSEARCH=$HOME/Development/elastic/elasticsearch/elasticsearch-6.0.0-rc1 $ELASTICSEARCH/bin/elasticsearch $BASH_ES_OPTS Now when you need additional configuration options, add them to the elasticsearch.yml
If you need more memory for the specific project, change the jvm.options file
Plugins When indexing pdf documents or word documents a lot of you out there have been using the mapper-attachments plugin
This was already deprecated, now it has been removed
You can switch to the ingest attachment plugin
Never heard about Injest
Injest can be used to pre process documents before they are being indexed by elasticsearch
It is a lightweight variant for Logstash, running within elasticsearch
Be warned though that plugins like the attachment mapper can be heavy on your cluster
So it is wise to have a separate node for Injest
Curious about what you can do to inject the contents of a pdf
The next few steps show you the commands to create the injest pipeline, send a document to it and obtain it again or create a query
First create the injest pipeline java PUT _ingest/pipeline/attachment { "description": "Extract attachment information", "processors": [ { "attachment": { "field": "data" } } ] } Now when indexing a document containing the attachment as a base64 encoded string in the field data we need to tell elasticsearch to use a pipeline
Check the parameter in the url: pipeline=attachment
This is the name used when creating the pipeline
java PUT my_index/my_type/my_id?pipeline=attachment { "data": "" } We could stop here, but how to get base64 encoded input from for instance a pdf
On linux and the mac you can use the base64 command for that
Below is a script that reads a specific pdf and creates a base64 ended string out of it
This string is than pushed to elasticsearch
java #!/bin/bash pdf_doc=$(base64 ~/Desktop/Programma.pdf) curl -XPUT "http://localhost:9200/my_index/my_type/my_id?pipeline=attachment" -H 'Content-Type: application/json' -d '{"data" : "'"$pdf_doc"'"}' Scripts If you are heavy into scripting in elasticsearch you need to check a few things
Changes have been made to the use of the lang attribute when obtaining or updating scripts, you cannot provide it any more
Also support for other languages than painless has been removed
Search and query DSL changes Most of the changes in this area are very specific
I am not going to sum them, please check the original documentation
Some of them I do want to mention as they are important to me
If you are constructing queries and it can happen you have an empty query, you can no longer provide an empty object { }
You will get an exception if you keep doing it
Bool queries had a disable_coord parameter, with this you could influence the score function to not use missing search terms as a penalty for the score
This option has been removed
You could transform a match query into a match_phrase query by specifying a type
This is no longer possible, you should just create a phrase query now if you need it
Therefore also the slop parameter has been removed from the match query
Calculating the score I the beginning of elasticsearch the score for a document based on an executed query was calculated using an adjusted formula for TF/IDF
It turned out that for fields containing smaller amounts of text TF/IDF was less ideal
Therefore the default scoring algorithm was replaced by BM25
Moving away from TF/IDF to BM25 has been the topic for version 5
Now with 6 they have removed two mechanisms in the scoring: Query Normalization and Coordination Factors
Query Normalization was always hard to explain during trainings
It was an attempt to normalise the scores of queries
Normalizing should make it possible to compare them
However, it did not work and you still should not compare scores of different queries
The Coordinating Factors were more a penalty when having multiple terms to search for and not all of them were found, the coordinating factor gave a penalty to the score
You could easily see this when using the explain API
That is it for the breaking changes, again there are more changes that you might want to investigate if you are really into all the elasticsearch details
Than have a look at the original documetation Next up, cool new features Now let us zoom in on some of the newer features or interesting upgrades
Sequence Numbers Sequence Numbers are now assigned to all index, update and delete operations
Using this number a shard that went offline for a moment can ask the primary shard for all operations after a certain sequence number
If the translog is still available (remember that we mentioned in the beginning that the translog was now kept around for 12 hours and or 512 Mb by default) the missing operations can be send to the shard preventing a complete refresh of all the shards contents
Test Normalizer using analyse endpoint One of the most important parts of elastic is configurating the mapping for your documents
How do you adjust the terms that you can search for based on the provided text
If you are not sure and you want to try out a specific tokeniser and filters combination you can use the analyze endpoint
Have a look at the following code sample and response where we try out a whitespace tokeniser with a lowercase filter
java GET _analyze { "tokenizer": "whitespace", "filter": ["lowercase"], "text": ["Jettro Coenradie"] } { "tokens": [ { "token": "jettro", "start_offset": 0, "end_offset": 6, "type": "word", "position": 0 }, { "token": "coenradie", "start_offset": 7, "end_offset": 16, "type": "word", "position": 1 } ] } As you can see we now get two tokens and the uppercase characters are replaced by their lowercase counterparts
What if we do not want the text to become two terms, but we want it to stay as one term
Still we would like to replace the uppercase characters with their lowercase counterparts
This was not possible in the beginning
However, with the introduction of normalizer, a special analyser for fields of type keyword it became possible
In elasticsearch 6 we now have the functionality to use the analyse endpoint for normalisers as well
Check the following code block for an example
java PUT people { "settings": { "number_of_replicas": 0, "number_of_shards": 1, "analysis": { "normalizer": { "name_normalizer": { "type": "custom", "filter": [ "lowercase" ] } } } } } GET people/_analyze { "normalizer": "name_normalizer", "text": ["Jettro Coenradie"] } { "tokens": [ { "token": "jettro coenradie", "start_offset": 0, "end_offset": 16, "type": "word", "position": 0 } ] } LogLog-Beta Ever heard about HyperLogLog or even HyperLogLog++
Well than you must be happy with LogLog-Beta
Some background, elasticsearch comes with a Cardinality Aggregation which can be used to calculate or better estimate the amount of distinct values
If we wanted to create an exact value, we would have to create a map of values with all unique values in there
This would require an extensive amount of memory
You can specify a threshold under which the amount of unique values would be close to exact
However the maximum value for this is 40000
Before elasticsearch used the HyperLogLog++ algorithm to estimate the unique values
With the new algorithm called LogLog-Beta there are better results with lower error margins and still the same performance
Significant Text Aggregation For some time the Significant Terms Aggregation has been available
The idea behind this aggregation is to find terms that are common to a specific scope and less common to a more general scope
So imagine we are looking for users of our website that place more orders in relation to pages they visit out of logs with page visits
You cannot calculate them by just counting the number of orders
You need to find those users that are more common to the set of orders than to the set of page visits
In the prior version this was already possible with terms, so not analysed fields
By enabling field-data or doc_values you could use small analysed fields
But for larger text fields this was a performance problem
Now with the Significant Text Aggregation we can overcome this problem
It also comes with an interesting functionality to deduplicate text (think about emails with the original text in a reply, or retweets)
Sounds a bit to vague
Ok, lets have an example
In elasticsearch documentation they use a dataset from Signal Media
As it is an interesting dataset to work with, I will also use it
You can try it out yourself as well
I downloaded the file and imported it into elasticsearch using logstash
This gist should help you
Now on to the query and the response java GET signalmedia/_search { "query": { "match": { "content": "rain" } }, "aggs": { "my_sampler": { "sampler": { "shard_size": 200 }, "aggs": { "keywords": { "significant_text": { "field": "content", "filter_duplicate_text": true } } } } }, "size": 0 } So we are looking for documents with the word rain
Now in these documents we are going to lookup terms that occur more often than in the global context
java { "took": 248, "timed_out": false, "_shards": { "total": 3, "successful": 3, "skipped": 0, "failed": 0 }, "hits": { "total": 11722, "max_score": 0, "hits": [] }, "aggregations": { "my_sampler": { "doc_count": 600, "keywords": { "doc_count": 600, "bg_count": 1000000, "buckets": [ { "key": "rain", "doc_count": 544, "score": 69.22167699861609, "bg_count": 11722 }, { "key": "showers", "doc_count": 164, "score": 32.66807368214775, "bg_count": 2268 }, { "key": "rainfall", "doc_count": 129, "score": 24.82562838569881, "bg_count": 1846 }, { "key": "thundery", "doc_count": 28, "score": 20.306396677050884, "bg_count": 107 }, { "key": "flooding", "doc_count": 153, "score": 17.767450110864743, "bg_count": 3608 }, { "key": "meteorologist", "doc_count": 63, "score": 16.498915662650603, "bg_count": 664 }, { "key": "downpours", "doc_count": 40, "score": 13.608547008547008, "bg_count": 325 }, { "key": "thunderstorms", "doc_count": 48, "score": 11.771851851851853, "bg_count": 540 }, { "key": "heatindex", "doc_count": 5, "score": 11.56574074074074, "bg_count": 6 }, { "key": "southeasterlies", "doc_count": 4, "score": 11.104444444444447, "bg_count": 4 } ] } } } } Interesting terms when looking for rain: showers, rainfall, thundery, flooding, etc
These terms could now be returned to the user as possible candidates for improving their search results
Concluding That is it for now
I haven’t even scratched all the new cool stuff in the other components like X-Pack, Logstash and Kibana
More to comeFrom Wednesday October 11th to Thursday October 12th, the World Summit AI 2017 conference was held in Amsterdam
It had a terrific line-up, with top professors in the field of AI, like Stuart Russell and Yann LeCun, and ‘our own’ Max Welling
Most of these people were actually there in person
But it wasn’t all science
Large companies like ING, Accenture, and smaller companies like BrainCreators (they were new for me) also had a large presence, with talks on the main stage, or with stands in front of the main arena
In this blog post, I’ll briefly discuss some of my favorite talks, in non-chronological order
On the second day, Meredith Whittaker, leader of Google’s open research group and co-founder of AI Now Institute, New York University, focused on current applications of ‘AI-technologies’ in the domain of human labour
And on what’s wrong with them, mostly
My summary of the problems she outlined would be that machine learning techniques are widely applied without a solid statistical methodology
The simplest example she gave
A soap dispenser that would not give soap to everybody
It obviously has not been tested on people with varied skin color
The field of statistics could have helped here
Who are the intended users of the soap dispensers
People
So, let’s fine-tune, or ‘train’, the soap dispensers on a representative sample of this population
Not just on a bunch of people you know, who happen to be similar to you
The same principles play a decisive role in any machine learning application
And, sadly, they are often not applied correctly, as Meredith very eloquently pointed out
She said much more about this, and there is still much more to be said about it
There certainly are many opportunities for improvement in this area
Talking about the second conference day, who opened it
None lesser than Stuart Russell, one of the authors of, can I say, the AI Bible
It turns out he is a talented comedian as well
He had the whole audience in the main stage laughing
Even the end of the human species did not seem so bad when he talked about it
His main message
“I cannot fetch coffee if I’m dead”
Currently, machine learning algorithms are programmed to optimise some quantity
In technical terms, they optimise an ‘objective function’
The problem
Humans can’t state very well what they want
Having a super-intelligent AI (if we ever succeed in creating it) optimise a function that was formulated by humans may lead to problems
And we might not be able to stop this AI
It will quickly figure out that it needs to stay alive in order to optimise its objective
And therefore, it will not allow us to switch it off
Being super-intelligent, it may actually succeed in this
Almost as a side point, professor Russell offered one possible avenue towards a solution: make algorithms uncertain about their objective function
Program AI so that it will put the interests of humans first, even if it is still unsure about what those interests are
If a human switches it off, it should therefore always gladly accept
Could there be dilemma’s and difficulties here as well
Well, certainly there were a good couple of laughs with the follow-up scenarios he offered
In his TED talk (https://www.ted.com/talks/stuart_russell_how_ai_might_make_us_better_people/footnotes?language=en) he discusses the same subject matter, so go ahead and watch that right after this blog post
On the first day, one of the stand-out talks for me was the one by Laurens van der Maaten, now at Facebook, famous for the T-SNE algorithm
If you don’t want to know what that is, you may well skip this paragraph, because his talk was one of the more technical of the conference
But it was a strong talk, leading up to a novel and original combination of deep learning techniques and symbolic AI approaches
The setting was the task of visual question answering: an algorithm gets as input an image and a question about it, and has to produce a correct answer
Admittedly, the scope was narrowed down a bit further: the images in question were generated images of geometrical objects, and the questions were often about characteristics such as color, texture, size, and shape
In this artificial world, however, the proposed solution performed very well indeed: better than humans
It seems these days you cannot sell anything less anymore, no
The sentence containing the question was processed by an LSTM-based sequence-to-sequence model
The output
A small computer program, a combination of primitive functions such as ‘count’, ‘filter color’, ‘filter’ shape
How are these functions executed
Well, they are themselves trained neural networks
An impressive composition of trained neural networks to achieve something bigger! A pre-print of the paper (with a nice picture of the architecture, showing everything I just wrote) is available here: https://arxiv.org/pdf/1705.03633.pdf With this, I’d like to leave you now, perhaps we’ll add more summaries here later!A few months ago we had a techday at Luminis Amsterdam
These days are great to explore, learn, innovate or just doing something cool
For a while now I have been reading about creating a smart mirror with a raspberry pi
So my idea on this techday was to find out what we need to do to create a smart mirror for our new office
Ideas went all over the place.
we need facial recognition, machine learning, some sort of custom greetings message.
Well enough enthusiasm! Since creating a smart mirror also includes some carpentry, we decided to focus on the software part first 😉 So I started off with reading an issue of the official Raspberry Pi magazine (https://www.raspberrypi.org/magpi/issues/54/)
This issue mentioned an open source modular smart mirror platform MagicMirror²
That sounded exactly what we were looking for! Installation Installing the platform was relatively easy
Just one command to execute and we should be good to go! curl -sL https://raw.githubusercontent.com/MichMich/MagicMirror/master/installers/raspberry.sh | bash We had some small issues with the installation of some node modules, but removing them and restarting the installation process was enough to finish it
Modules MagicMirror² comes with a few default modules
But there are already a lot of third party modules
Installing one of these modules is also really easy
We just needed to clone the module from github in the modules folder, change the config of that module, add the module to the MagicMirror config file and that’s it! The newly installed module will be picked up automatically when restarting the platform
So we decided that we wanted to show a few things: Luminis logo Luminis blog feed Buienrader – information about the rain Employee birthday calendar Creating our own module Because we wanted to learn how we can created our own module, we took the most simple option (display a logo) to build
An detailed description of how to create your own module can be found here
But basically to create a simple module you need a javascript file that registers the module and override some functions
javascript Module.register("luminis",{ // Default module config
defaults: { text: "Welkom bij Luminis" }, // Override dom generator
getDom: function() { var wrapper = document.createElement("div"); var textWrapper = document.createElement("span"); textWrapper.innerHTML = this.config.text; wrapper.appendChild(textWrapper); var logoImgElement = document.createElement("img"); logoImgElement.setAttribute("src", "some-url"); wrapper.appendChild(logoImgElement); return wrapper; } }); So we configured some other modules and started the MagicMirror: We were happy with the result and at the end of the day we did a demo for our colleagues
Even without a mirror, the screen looks cool enough to just hang it somewhere in the office! But we decided to we will create a mirror when we will move to our new office
Creating the mirror After a few weeks I have started with creating a mirror for myself
First I created a frame to support the monitor, hide the raspberry pi and the cables
I ordered a white power cable for the monitor to match with the wall 😉 , added some paint to the frame and used a wall mount to hang the monitor
The result is pretty cool
Maybe I will build a frame that looks a bit nicer, but to make it look better I probably have to let it create by someone else 🙂In my previous blog post I wrote about creating your own search DSL using Antlr
In that post I discussed the Antlr language for constructs like AND/OR, multiple words and combining words
In this blog post I am showing how to use the visitor mechanism to write actual elasticsearch queries
If you did not read the first post yet, please do so
It will make it easier to follow along
If you want the code, please visit the github page
https://amsterdam.luminis.eu/2017/06/28/creating-search-dsl/ Github repository What queries to use In the previous blog post we ended up with some of the queries we want to support apple apple OR juice apple raspberry OR juice apple AND raspberry AND juice OR Cola “apple juice” OR applejuice Based on these queries we have some choices to make
The first query seems obvious, searching for one word would become a match query
However, in which field do you want to search
In Elasticsearch there is a special field called the _all field
In the example we are using the _all field, however it would be easy to create a query against a number of specific fields using a multi_match
In the second example we have two words with OR in between
The most basic implementation would again be a match query, since the match query by default uses OR if you supply multiple words
However, the DSL uses OR to combine terms as well as and queries
A term in itself can be a quoted term as well
Therefore, to translate the apple OR juice we need to create a boolean query
Now look at the last example, here we use quotes
One would expect quotes to keep words together
In elasticsearch we would use the Phrase query to accomplish this
As the current DSL is fairly simple, creating the queries is not that hard
But a lot more extensions are possible that can make use of more advance query options
Using wildcards could result in fuzzy queries, using title:apple could look into one specific field and using single quotes could mean an exact match, so we would need to use the term query
Now you should have an idea of the queries we would need, let us have a look at the code and see Antlr DSL in action
Generate json queries As mentioned in the introduction we are going to use the visitor to parse the tree
Of course we need to create the tree first
Below the code to create the tree
json static SearchdslParser.QueryContext createTreeFromString(String searchString) { CharStream charStream = CharStreams.fromString(searchString); SearchdslLexer lexer = new SearchdslLexer(charStream); CommonTokenStream commonTokenStream = new CommonTokenStream(lexer); SearchdslParser parser = new SearchdslParser(commonTokenStream); return parser.query(); } AS mentioned in the previous posts, the parser and the visitor classes get generated by Antlr
Methods are generated for visiting the different nodes of the tree
Check the class SearchdslBaseVisitor for the methods you can override
To understand what happens, it is best to have a look at the tree itself
Below the image of the tree that we are going to visit
We visit the tree from the top
The first method or Node that we visit is the top level Query
Below the code of the visit method
json @Override public String visitQuery(SearchdslParser.QueryContext ctx) { String query = visitChildren(ctx); return "{" + "\"query\":" + query + "}"; } Every visitor generates a string, with the query we just visit all the possible children and create a json string with a query in there
In the image we see only a child orQuery, but it could also be a Term or andQuery
By calling the visitChildren method we continue to walk the tree
Next step is the visitOrQuery
json @Override public String visitOrQuery(SearchdslParser.OrQueryContext ctx) { List<String> shouldQueries = ctx.orExpr().stream().map(this::visit).collect(Collectors.toList()); String query = String.join(",", shouldQueries); return "{\"bool\": {" + "\"should\": [" + query + "]" + "}}"; } When creating an OR query we use the bool query with the should clause
Next we have to obtain the queries to include in the should clause
We obtain the orExpr items from the orQuery and for each orExpr we again call the visit method
This time we will visit the orExpr Node, this node does not contain important information for us, therefore we let the template method just call the visitChildren method
orExpr nodes can contain a term or an andQuery
Let us have a look at visiting the andQuery first
json @Override public String visitAndQuery(SearchdslParser.AndQueryContext ctx) { List<String> mustQueries = ctx.term().stream().map(this::visit).collect(Collectors.toList()); String query = String.join(",", mustQueries); return "{" + "\"bool\": {" + "\"must\": [" + query + "]" + "}" + "}"; } Notice how closely this resembles the orQuery, big difference in the query is that we now use the bool query with a must part
We are almost there
The next step is the Term node
This node contains words to transform into a match query, or it contains a quotedTerm
The next code block shows the visit method of a Term
json @Override public String visitTerm(SearchdslParser.TermContext ctx) { if (ctx.quotedTerm() != null) { return visit(ctx.quotedTerm()); } List<TerminalNode> words = ctx.WORD(); String termsAsText = obtainWords(words); return "{" + "\"match\": {" + "\"_all\":\"" + termsAsText + "\"" + "}" + "}"; } private String obtainWords(List<TerminalNode> words) { if (words == null || words.isEmpty()) { return ""; } List<String> foundWords = words.stream().map(TerminalNode::getText).collect(Collectors.toList()); return String.join(" ", foundWords); } Notice we first check if the term contain a quotedTerm
If it does not contain a quotedTerm we obtain the words and combine them into one string
The final step is to visit the quotedTerm node
json @Override public String visitQuotedTerm(SearchdslParser.QuotedTermContext ctx) { List<TerminalNode> words = ctx.WORD(); String termsAsText = obtainWords(words); return "{" + "\"match_phrase\": {" + "\"_all\":\"" + termsAsText + "\"" + "}" + "}"; } Notice we parse this part into a match_phrase query, other than that it is almost the same as the term visitor
Finally we can generate the complete query
Example “multi search” && find && doit OR succeed && nothing json { "query": { "bool": { "should": [ { "bool": { "must": [ { "match_phrase": { "_all": "multi search" } }, { "match": { "_all": "find" } }, { "match": { "_all": "doit" } } ] } }, { "bool": { "must": [ { "match": { "_all": "succeed" } }, { "match": { "_all": "nothing" } } ] } } ] } } } In the codebase on github there is also a Jackson JsonNode based visitor, if you don’t like the string based approach
That is about it, I am planning on extending the example further
If I have added some interesting new concepts I’ll get back to you with a part 3On this site, recently, we featured a blog post [12] that used Doc2vec [4]
What is Doc2vec
Where does it come from
What does it do
Why use doc2vec, instead of other algorithms that do the same
What implementations exist
Where can I read more about it
If you, like me, are curious about these questions, read on
So what is Doc2vec and where does it come from
In recent years some Google papers were published by Tomas Mikolov and friends about a neural network that could be trained to produce so-called paragraph vectors [1, 2, 3, 9]
The authors did not release software with their research papers
So others have tried to implement it
Doc2vec is an implementation of paragraph vectors by the authors of gensim, a much used library for numerical methods in the field of natural language processing (NLP)
The name is different, but it is the same algorithm: doc2vec just sounds better than paragraph vectors
It is also a nicer name, because doc2vec builds upon the same neural network architectures that underly those other famous algorithms that go by the name word2vec
If you don’t know word2vec, Google it, there are plenty of resources where you can learn about it
Resources to learn about doc2vec, however, are just a bit less abundant, so in this post we’ll google it for you
First, what does doc2vec do
Well, it gives you vectors of a fixed length–to be determined by you–that can represent text fragments of varying size, such as sentences, paragraphs, or documents
It achieves this by training a small neural network to perform a prediction task
To train a network, you need labels
In this case, the labels will come from the texts themselves
After the network has been trained, you can re-use a part of it, and this part will give you your sentence / paragraph / document vectors
These vectors can then be used in various algorithms, including document classification [12]
One of the success factors for using doc2vec will be the answer to this: the task you are using the doc2vec vectors for, is it related to the way doc2vec was trained, in a useful way
What benefits does doc2vec offer over other methods
There are many ways to represent sentences, paragraphs or documents as a fixed size vector
The simplest way is to create a vocabulary of all the words in a corpus, and represent each document by a vector that has an entry for each word in the vocabulary
But such a vector would be quite large, and it would be quite sparse, too (it would contain many zeroes)
Some algorithms have difficulty working with sparse and high dimensional vectors
doc2vec yields vectors of a more manageable size, as determined by you
Again, there are many algorithms that do this for you, such as LDA [18], LSI [19], or Siamese CBOW [17], to name a recent one by a former colleague
To argue for the one or the other, what researchers would normally do is implement the prediction task they care about with several algorithms, and then measure which algorithm performed best
For example, in [9] paragraph vectors are compared to LDA for various tasks; the authors conclude that paragraph vectors outperform LDA
This does not mean that doc2vec will always be best for your particular application
But perhaps it is worth trying out
Running experiments with doc2vec is one way of learning about what it does, when you can use it, when it works well, and when it is less useful
In terms of implementations, we’ve already mentioned the Doc2Vec class in gensim [4]
There’s also an implementation in deeplearning4j [15]
And Facebook’s fastText may have an implementation, too [16]
Since I like working with Tensorflow, I’ve googled “doc2vec tensorflow” and found a promising, at first sight clean and consise implementation [13]
And a nice discussion about a few lines of Tensorflow code as well, with the discussion shifting to the gensim implementation [11]
Implementations in other low level neural network frameworks may exist
Zooming in just a bit, it turns out that doc2vec is not just one algorithm, but rather it refers to a small group of alternative algorithms
And these are of course extended in new research
For example, in [9], a modified version of a particular doc2vec algorithm is used, according to a blog post about the paper [10]
In that blog post, some details on the extension in [9] are given, based on correspondence with the authors of [9]
An implementation of that extension is believed not to be there yet by the author of [10]
In general, it may be impossible to recreate exactly the same algorithms as the authors of the original papers used
Rather, studying concrete implementations is another way of learning about how doc2vec algorithms work
A third way of learning more is reading
If you know a bit about how neural networks work, you can start by checking the original papers [1, 2, 3, 9]
There are some notes on the papers, too, in blogs by various people [10, 14]
The papers and the blog posts leave some details to the reader, and are not primarily intended as lecture material
The Stanford course on deep learning for NLP has some good lecture notes on some of the algorithms leading up to doc2vec [7], but doc2vec itself is not covered
There are enough posts explaining how to use the gensim Doc2Vec class [5, 6, 8, 12]
Some of these posts do include some remarks on the workings of Doc2Vec [5, 6, 8] or even perform experiments with it [6, 8, 12]
But they do not really drill down to the details of the neural net itself
I could not find a blog post explaining the neural net layout in [13], or reporting on experiments with [13]
Now that you have come this far, wouldn’t it be nice to set out to take a look at how doc2vec, the algorithm, works
With the aim to add some detail and elaboration to the concise exposition in the original papers
And perhaps we can add and discuss some working code, if not too much of it is needed! Stay tuned for more on this
References Quoc Le and Tomas Mikolov
Distributed Representations of Sentences and Documents
http://arxiv.org/pdf/1405.4053v2.pdf Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean
Efficient Estimation of Word Representations in Vector Space
In Proceedings of Workshop at ICLR, 2013
https://arxiv.org/pdf/1301.3781.pdf Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean
Distributed Representations of Words and Phrases and their Compositionality
In Proceedings of NIPS, 2013
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf https://radimrehurek.com/gensim/models/doc2vec.html https://rare-technologies.com/doc2vec-tutorial/ https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb http://cs224d.stanford.edu/lecture_notes/LectureNotes1.pdf https://ireneli.eu/2016/07/27/nlp-05-from-word2vec-to-doc2vec-a-simple-example-with-gensim/ Andrew N
Dai, Christopher Olah, Quoc V
Le
Document Embedding with Paragraph Vectors, NIPS 2014
https://arxiv.org/pdf/1507.07998v1.pdf http://building-babylon.net/2015/06/03/document-embedding-with-paragraph-vectors/ https://groups.google.com/forum/#!topic/gensim/0GVxA055yOU https://amsterdam.luminis.eu/2016/11/15/machine-learning-example/ https://github.com/wangz10/tensorflow-playground/blob/master/doc2vec.py https://blog.acolyer.org/2016/06/01/distributed-representations-of-sentences-and-documents/ https://deeplearning4j.org/doc2vec https://github.com/facebookresearch/fastText/issues/26 Tom Kenter, Alexey Borisov, Maarten de Rijke
Siamese CBOW: Optimizing Word Embeddings for Sentence Representations
ACL 2016
http://aclweb.org/anthology/P/P16/P16-1089.pdf https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation https://en.wikipedia.org/wiki/Latent_semantic_analysisAs an (elastic)search expert, I regularly visit customers
For these customers I often do a short analysis of their search solution and I give advice about improvements they can make
It is always interesting to look at solutions customers come up with
At one of my most recent customers I noticed a search solution based on a very extensive search DSL (Domain Specific Language) created with Antlr
I knew about Antlr, but never thought about creating my own search DSL
To better understand the options of Antlr and to practice with creating my own DSL I started experimenting with it
In this blog post I’ll take you on my learning journey
I am going to create my own very basic search DSL
Specifying the DSL First we need to define the queries we would like our users to enter
Below are some examples: tree – This is an easy one, just one word to search for tree apple – Two words to look up tree apple AND sell – Find matching content for tree apple, but also containing sell
tree AND apple OR juice – Find matching content containing the terms tree and apple or containing the term juice
“apple tree” OR juice – Find content having the terms apple and tree next to each other in the right order (Phrase query) or having the term juice
These are the combinations we need to make
In the next sections we setup our environment and I explain the basics of Antlr that you need to understand to follow along
Setting up Antlr for your project There are lots of resources about setting up your local Antlr environment
I personally learned most from tomassetti
I prefer to use Maven to gather the required dependencies
I also use the Maven Antlr plugin to generate the Java classes based on the Lexar and Grammar rules
I also installed Antlr using Homebrew, but you do not really need this for this blog post
You can find the project on Github: https://github.com/jettro/search-dsl I generally just load the Maven project into IntelliJ and get everything running from there
If you don’t want to use an IDE, you can also do this with pure Maven
java proj_home #> mvn clean install proj_home #> mvn dependency:copy-dependencies proj_home #> java -classpath "target/search-dsl-1.0-SNAPSHOT.jar:target/dependency/*" nl.gridshore.searchdsl.RunStep1 Of course you can change the RunStep1 into one of the other three classes
Antlr introduction This blog post does not have the intention to explain all ins and outs of Antlr
But there are a few things you need to know if you want to follow along with the code samples
Lexer – A program that takes a phrase and obtains tokens from the phrase
Examples of lexers are: AND consisting of the characters ‘AND’ but also the specials characters ‘&&’
Another example is a WORD consisting of upper or lowercase characters and numbers
Tokens coming out of a Lexer contain the type of the token as well as the matched characters by that token
Grammar – Rules that make use of the Lexer to create the syntax of your DSL
The result is a parser that creates a ParseTree out of your phrase
For example, we have a grammar rule querythat parses a phrase like tree AND apple into the following ParseTree
The Grammar rule is: query : term (AND term)+ ;
ParseTree – Tree by Antlr using the grammar and lexer from the provided phrase
Antlr also comes with a tool to create a visual tree
See an example below
In this blog post we create our own parser of the tree, there are however two better alternatives
The first is using the classic Listener pattern
The other is the Visitor pattern
Listener – Antlr generates some parent classes to create your own listener
The idea behind a a listener is that you receive events when a new element is started and when the element is finished
This resembles how for instance the SAX parser works
Visitor – Antlr generates some parent classes to create your own Visitors
With a visitor you start visiting your top level element, then you visit the children, that way you recursively go down the tree
In a next blog post we’ll discuss the visitor pattern in depth
Search DSL Basics In this section we are going to create the DSL in four small steps
For each step we have a StepXLexerRules.g4 and a StepXSearchDsl.g4 file containing the Antlr lexer and grammar rules
Each step also contains a Java file with the name RunStepX
Step 1 In this step we want to write rules like: apple apple juice apple1 juice java lexer WORD : ([A-z]|[0-9])+ ; WS : [ \t\r\n]+ -&amp;gt; skip ; grammar query : WORD+ ; In all the Java examples we’ll start the same
I’ll mention the rules here but will not go into depth in the other steps
java Lexer lexer = new Step1LexerRules(CharStreams.fromString("apple juice")); CommonTokenStream commonTokenStream = new CommonTokenStream(lexer); Step1SearchDslParser parser = new Step1SearchDslParser(commonTokenStream); Step1SearchDslParser.QueryContext queryContext = parser.query(); First we create the Lexer, the Lexer is generated by Antlr
The input is a stream of characters created using the class CharStreams
From the Lexer we obtain a stream of Tokens, which is the input for the parser
The parser is also generated by Antlr
Using the parser we can obtain the queryContext
Notice the method query
This is the same name as the first grammar rule
In this basic example a query consists of at least one WORD and a WORD consists of upper and lower case characters and numbers
The output for the first step is: html Source: apple WORDS (1): apple, Source: apple juice WORDS (2): apple,juice, Source: apple1 juice WORDS (2): apple1,juice, In the next step we are extending the DSL with an option to keep words together
Step 2 In the previous step you got the option to search for one or multiple words
In this step we are adding the option to keep some words together by surrounding them with quotes
We add the following lines to the lexer and grammar
java 1 2 3 4 5 6 7 8 lexer QUOTE : ["]; grammar query : term ; term : WORD+|quotedTerm; quotedTerm : QUOTE WORD+ QUOTE ; Now we can support queries like apple “apple juice” The addition to the lexer is QUOTE, the grammar becomes slightly more complex
The query now is a term, a term can be multiple WORDs or a quoted term consisting of multiple WORDs surrounded by QUOTEs
In Java we have to check from the termContext that is obtained from the queryContext if the term contains WORDs or a quotedTerm
That is what is shown in the next code block
java Step2SearchDslParser.TermContext termContext = queryContext.term(); handleTermOrQuotedTerm(termContext); private void handleTermOrQuotedTerm(Step2SearchDslParser.TermContext termContext) { if (null != termContext.quotedTerm()) { handleQuotedTerm(termContext.quotedTerm()); } else { handleWordTokens(termContext.WORD()); } } private void handleQuotedTerm(Step2SearchDslParser.QuotedTermContext quotedTermContext) { System.out.print("QUOTED "); handleWordTokens(quotedTermContext.WORD()); } Notice how we check if the termContext contains a quotedTerm, just by checking if it is null
The output then becomes html Source: apple WORDS (1): apple, Source: "apple juice" QUOTED WORDS (2): apple,juice, Time to take the next step, this time we make it possible to specify to make it explicit to query for one term or the other
Step 3 In this step we make it possible to make it optional for a term to match as long as another term matches
Example queries are: apple apple OR juice “apple juice” OR applejuice The change to the Lexer is just one type OR
The grammar has to change, now the query needs to support a term or an orQuery
The orQuery consists of a term extended with OR and a term, at least once
java 1 2 3 4 5 6 lexer OR : 'OR' | '||' ; grammar query : term | orQuery ; orQuery : term (OR term)+ ; The handling in Java is straightforward now, again some null checks and handle methods
java 1 2 3 4 5 if (queryContext.orQuery() != null) { handleOrContext(queryContext.orQuery()); } else { handleTermContext(queryContext.term()); } The output of the program then becomes: html Source: apple WORDS (1): apple, Source: apple OR juice Or query: WORDS (1): apple, WORDS (1): juice, Source: "apple juice" OR applejuice Or query: QUOTED WORDS (2): apple,juice, WORDS (1): applejuice, In the final step we want to make the OR complete by also adding an AND
Step 4 In the final step for this blog we are going to introduce AND
With the combination of AND we can make more complicated combinations
What would you make from one AND two OR three OR four AND five
In my DSL I first do the AND, then the OR
So this would become (one AND two) OR three OR (four AND five)
So a document would match if it contains one and two, or four and five, or three
The Lexer does change a bit, again we just add a type for AND
The grammar has to introduce some new terms
It is good to have an overview of the complete grammar
java query : term | orQuery | andQuery ; orQuery : orExpr (OR orExpr)+ ; orExpr : term|andQuery; andQuery : term (AND term)+ ; term : WORD+|quotedTerm; quotedTerm : QUOTE WORD+ QUOTE ; As you can see, we introduced an orExpr, being a term or an andQuery
We changed an orQuery to become an orExpr followed by at least one combination of OR and another orExpr
The query now is a term, an orQuery or an andQuery
Some examples below
apple apple OR juice apple raspberry OR juice apple AND raspberry AND juice OR Cola “apple juice” OR applejuice The java code becomes a bit boring by now, so let us move to the output of the program immediately
html Source: apple WORDS (1): apple, Source: apple OR juice Or query: WORDS (1): apple, WORDS (1): juice, Source: apple raspberry OR juice Or query: WORDS (2): apple,raspberry, WORDS (1): juice, Source: apple AND raspberry AND juice OR Cola Or query: And Query: WORDS (1): apple, WORDS (1): raspberry, WORDS (1): juice, WORDS (1): Cola, Source: "apple juice" OR applejuice Or query: QUOTED WORDS (2): apple,juice, WORDS (1): applejuice, Concluding That is it for now, of course this is not the most complicated search DSL
You can most likely come up with other interesting constructs
The goal for this blogpost was to get you underway
In the next blog post I intend to discuss and show how to create a visitor that makes a real elasticsearch query based on the DSLModern day software programming is an increasingly wide and complex field
An ‘ideal’ full-stack developer would probably be one that started developing applications at a young age, developing knowledge and understanding of a broad number of technologies
She or he would have received formal education in different programming paradigms in several different languages, as well as in (relational) databases, communication protocols, hardware, security strategies, front-end technologies, etcetera
Having first learned what a string and an integer are some 3 years ago, I am not such a developer
I know many other colleagues and friends for which software development was a taste that they acquired at a later age
There could be several reasons why software development is a popular choice of career switch
It is a relatively young profession, so perhaps some people are only now ‘discovering’ it
Perhaps people have a (probably unwarranted) fear that the profession is boring, or too difficult
Whatever the reason, people who want to learn building software applications professionally today will initially have to find a way through an overwhelming number of technologies and concepts
You have little choice in this case, but to simply start somewhere
For me that somewhere was (primarily) ASP.Net and C#
After following some (open) university courses about object-oriented programming (in Java) and subsequently landing a job as junior C# developer, I quickly felt somewhat competent in basic programming in C#
Classes, interfaces, inheritance, and those sorts of things
The C# and Java languages of course have their differences (that I won’t discuss here), but I would argue that they are conceptually similar
Some knowledge of object-oriented and class based programming was a narrow window of entrance into a wide field of technologies
To contribute to the development process of full web applications, I had to at least develop some basic ability to (amongst others) work with things like HTML and HTTP/Ajax
And: Javascript
Superficially, Javascript seems somewhat like C# (except for some obvious differences like strong typing and classes)
But Javascript has been a somewhat rough experience for me so far
After taking some time recently to get more familiar with the core concepts of plain Javascript as a language however, things start making more sense
Let’s now zoom in on those aspects of Javascript that may be most confusing to C# and or Java developers! Javascript is actually, in many ways, very much different from C# or Java and I believe that a basic, structured, overview of the most fundamental differences might help junior Javascript developers, like me, with the following things: Some understanding of the inner workings of popular libraries we tend to use like Typescript, various module loaders, Angular, etc
More awareness of good and bad coding practices when writing Javascript
Detecting and understanding ‘bugs’ in Javascript code that we could not understand if we treat it like it was, say, C#
Powerful features that are unique to Javascript and that we can use to our advantage
Every data container in Javascript is either an ‘object’ or a ‘primitive’ (and that’s all) Primitives in Javascript are not objects and they are always 1 of the following 5 things: number, string, bool, null and undefined
String, bool and null are somewhat similar to how they work in C#/Java
One notable thing though, is that Javascript has no stack and heap separation in terms of memory allocation
In other words, there is no such thing as a struct in Javascript
Thus, the implication of data being captured in either an object or a primitive is minimal in memory terms
Also, the ‘under-water’ way in which Javascript enables methods to be called on a string literal (e.g
string.length), is completely different from C# and Java
I’ll come back to that later when talking about object prototypes
Javascript numbers are always 64-bit floating point variables
There is only the one type of number (so no double, decimal, etc.)
All objects declared in the Javascript language always have this exact structure: { key1: value1, key2: value2, key3: value3, // ..etc, key100: value100, // …etc } The values in this case can be primitives, or other objects
Note that functions in Javascript are objects themselves, just like arrays
Compared to C#/Java objects, Javascript objects have no type (by default) and do not need to instantiate a class
The undefined primitive is the value that variables receive when they are declared but have not yet received a value
Somewhat confusingly, you can also assign the value undefined manually, like so: var x = undefined; var y = {key1: undefined}; Doing this is considered bad practice but it is good to understand the difference between an undefined variable, a variable with the value null and a ‘unknown’ variable
In Java and C#, declared but unassigned variables have a default value (null for reference types, 0 for Integer, false for bool etc.)
In Javascript this default value is undefined for everything
Null is typically used in Javascript when you want to explicitly not assign any value to something (a method could typically return null for example, as opposed to undefined)
Finally, when a variable that has not been declared at all is used during execution of the code, the compiler will throw an unknown variable exception
Program flow in Javascript is single threaded, starts with a root ‘this’ object and flows from top to bottom in the second phase of its execution
Ok, that header is certainly a mouthful
Please bear with me and I’ll explain
Program flow in Javascript is fundamentally different from C# and Java and some insight into how it works is very helpful in understanding when things in your code happen and why
Code execution (compilation) of a Javascript app proceeds in two phases
In the first phase, the Javascript compiler prepares the ‘execution context(s)’
In the second phase those execution contexts are, well, executed
I.e
it is translated by the compiler into code that the computer can understand
The first of the execution contexts that is prepared by the compiler is called the global execution context
This is the outer scope of your Javascript app
It always comes with a global object that is called ‘this’
In the case of Javascript executed in a web-page, ‘this’ is (in the global scope) normally the window object
Furthermore, each ‘function’ in your app gets its own execution context
These execution contexts are prepared for execution in the order in which they are nested under the outer scope, and from top to bottom
Because this may be easier to follow with some visual example, I refer to the following post that I found on the web and that explains this topic very well: https://cedric-dumont.com/2015/10/31/compilation-and-execution-phases-of-simple-javascript-code The ‘this’ variable for each execution context, by default, points to whatever object is currently in scope
That essentially means that ‘this’ initially points to the object on which the function which encapsulates the current execution context is called
The nature of ‘this’ in Javascript is extremely confusing, especially for developers used to C# or Java
However, the following document does a fair job of explaining it: https://developer.mozilla.org/nl/docs/Web/JavaScript/Reference/Operators/this In the second phase, once all execution contexts are prepared, the code on them is then executed for each context in sequential order and on a single thread
Javascript code, unlike C# and Java applications, has no classes or methods that can start new threads
Variables that have been declared but that have not received a value at the time of code execution will have the default value (undefined), as explained earlier
Note that this can be for any reason: You may have forgotten to assign the variable, or the variable may get assigned only after a promise has resolved and that promise has not returned yet
The same holds true for variables that are assigned to a function
The following code for example will throw an exception because x is undefined, just as if x were any other object (Remember, there is only one type of object in Javascript)
… x(); var x = function(){ console.log(‘Called x!’);}; … By contrast, the following code will work, and show ‘Called x!’ on the console: … x(); function x() { console.log(‘Called x!’);}; … This last type of function is evaluated as soon as the execution context in which it resides is generated and will be placed on that scope as an object-function called x
The new keyword in Javascript is used when creating new instances of objects using constructor functions
Not when simply creating a new object by defining it and assigning it to a variable
The ‘new’ keyword, much like the ‘this’ keyword is a strange beast in Javascript
Perhaps this is even more true because it was introduced to Javascript at a later point in time, with the intention to make the instantiation of new objects look and feel like regular Java
What makes it confusing is that, in practice, it does something completely different and is not comparable with the new keyword in C# or Java
Let’s start by explaining what ‘new’ does in Javascript
Consider the following ‘constructor’ function and assume that you placed this function on the global execution context: function Person(first, last, age, eye) { this.firstName = first; this.lastName = last; this.age = age; this.eyeColor = eye; } The reason this type of function is called a constructor function is that it can be used to create new instances of objects that look like this: { firstname: ‘firstname’ lastname: ‘lastname’, age: number, eyecolor: ‘value’, __Proto__: { contructor: function Person(first, last, age, eye) } To accomplish this, we need to simply write: var x = new Person(‘first’, ‘last’, number, ‘eye’); What we certainly do not want to do is write the following: var x = Person(‘first’, ‘last’, number, ‘eye’)
If you fail to see why this is, you should probably read the former paragraph about program flow and the ‘this’ keyword once more
Because it is an easy mistake for a programmer to make, declaring the Person function with a capital P is an unusual but important convention here
Normally functions and variables in Javascript always start with a lowercase letter
(Incidentally, another difference with Java and C#)
I will talk about the _Proto_ (‘prototype’) object a bit more in the last paragraph
For now, it is important to see that objects you create with a ‘new’ keyword retain a reference to (a copy of) the function that created them via this prototype object
All ‘children’ that you create off the constructor function have a reference to the same prototype object
This is the basis for prototypical inheritance in Javascript
A completely different mechanism to inheritance as we know it in C# and Java
Using object prototypes in Javascript to your advantage Each object in Javascript has a hidden reference to a prototype object, with Object.prototype always being at the end of that reference chain
All properties and methods of an object’s prototype, are accessible by its ‘children’ as well
Let’s say we create a new object in the normal way, like so: var normalObject = { quality: ‘normal’, type: ’object’ }; This normalObject has, by default, a reference to Object.prototype and that is why you can call a method like ‘toString()’ on it
Primitives do not have a prototype object, but strangely you can call methods on them
This is made possible due to Javascript very briefly creating the necessary object representation of that primitive, use that objects’ String.prototype methods, then garbage-collect the object immediately after
For example: ‘test’.charAt(0) will be briefly converted to new String(‘test’).charAt(0) upon executing the code
Because all child objects made by a constructor function share the same prototype object, you can do funny things, that are somewhat unexpected when you are used to Java/C#
You can, for example, extend commonly used object prototypes like Array.protoype, Window.prototype ans String.prototype (each of which has its own reference to Object.prototype), with you own methods*
This feels almost like extension methods in C#
You can also create an object with the new operator, then change its prototype to an entirely different object, to create various interesting effects
It is fun to play around with and I would encourage the reader to experiment, to get a good feeling of object behavior in Javascript
*If you are using typescript, remember to also extend the type-definition interface, to get this to work
Closing words Javascript is an interesting language that has its weaknesses but certainly has a lot of strengths as well
Because Javascript is the backbone of nearly all client-side code in web-applications, often with complex frameworks like typescript and angular built on top of it, I believe there is a lot of value in understanding how it works on a fundamental level for nearly every developer
While it looks somewhat like Java and C# on a superficial level, I hope I have given a good impression of the points at which it is very different
I also hope that this write-up was of some use to other people learning Javascript as a secondary languageAt my current project, we’re developing an application based on Spring Boot
During my normal development cycle, I always start the application from within IntelliJ by means of a run configuration that deploys the application to a local Tomcat container
Spring boot applications can run perfectly fine with an embedded container, but since we deploy the application within a Tomcat container in our acceptance and production environments, I always stick to the same deployment manner on my local machine
After joining the project in March one thing always kept bugging me
When I started the application with IntelliJ, it always took more than 60 seconds to start the deployed application, which I thought was pretty long given the size of the application
My teammates always said they found it strange as well, but nobody bothered to spend the time to investigate the cause
Most of us run the entire application and it’s dependencies (MongoDB and Elasticsearch) on their laptop and the application requires no remote connections, so I always wondering what the application was doing during those 60+ seconds
Just leveraging the logging framework with the Spring boot application gives you a pretty good insight into what’s going on during the launch of the application
In the log file, there were a couple of strange jumps in time that I wanted to investigate further
Let’s take a look at a snippet of the log: java 2017-05-09 23:53:10,293 INFO - Bean 'integrationGlobalProperties' of type [class java.util.Properties] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying) 2017-05-09 23:53:15,829 INFO - Cluster created with settings {hosts=[localhost:27017], mode=MULTIPLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms', maxWaitQueueSize=500} 2017-05-09 23:53:15,830 INFO - Adding discovered server localhost:27017 to client view of cluster 2017-05-09 23:53:16,432 INFO - No server chosen by WritableServerSelector from cluster description ClusterDescription{type=UNKNOWN, connectionMode=MULTIPLE, serverDescriptions=[ServerDescription{address=localhost:27017, type=UNKNOWN, state=CONNECTING}]}
Waiting for 30000 ms before timing out 2017-05-09 23:53:20,992 INFO - Opened connection [connectionId{localValue:1, serverValue:45}] to localhost:27017 2017-05-09 23:53:20,994 INFO - Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, version=ServerVersion{versionList=[3, 4, 2]}, minWireVersion=0, maxWireVersion=5, maxDocumentSize=16777216, roundTripTimeNanos=457426} 2017-05-09 23:53:20,995 INFO - Discovered cluster type of STANDALONE 2017-05-09 23:53:21,020 INFO - Opened connection [connectionId{localValue:2, serverValue:46}] to localhost:27017 2017-05-09 23:53:21,293 INFO - Checking unique service notification from repository: Now what’s interesting about the above log is that it makes a couple of multi-second jumps
The first jump is after handling the bean ‘integrationGlobalProperties’
After about 5 seconds the application logs an entry when it tries to setup a connection to a locally running MongoDB instance
I double checked my settings, but you can see it’s really trying to connect to a locally running instance by the log messages stating it tries to connect to ‘localhost’ on ‘27017’
A couple of lines down it makes another jump of about 4 seconds
In that line, it is still trying to set up the proper MongoDB connection
So in it takes about 10 seconds in total to connect to a locally running (almost empty) MongoDB instance
That can’t be right?! Figuring out what’s was going on wasn’t that hard
I just took a couple of Thread dumps and a small Google query which led me to this post on the IntelliJ forum and this post on StackOverflow
Both posts point out a problem similar to mine: a ‘DNS problem’ with how ‘localhost’ was resolved
The time seems to be spent in java.net.InetAddress.getLocalHost()
The writers of both posts have a delay up to 5 minutes or so, which definitely is not workable and would have pushed me to look into this problem instantly
I guess I was ‘lucky’ it just took a minute on my machine
Solving the problem is actually quite simple as stated in both posts
All you have to do is make sure that your /etc/hosts file also contains the .local domain entry for ‘localhost’ entries
While inspecting my hosts file I noticed it did contain both entries for resolving localhost on both IPv4 and IPv6
java 127.0.0.1 localhost ::1 localhost However, it was missing the .local addresses, so I added those
If you’re unsure what your hostname is, you can get it quite easily from a terminal
Just use the hostname command: java $ hostname and it should return something like: java Jeroens-MacBook-Pro.local In the end, the entries in your host file should look something like: java 127.0.0.1 localhost Jeroens-MacBook-Pro.local ::1 localhost Jeroens-MacBook-Pro.local Now with this small change applied to my hosts file, the application starts within 19 seconds
That 1/3 of the time it needed before! Not bad for a 30-minute investigation
I wonder if this is related to an upgraded macOS or if it exists on a clean install of macOS Sierra as well
The good thing is that this will apply to other applications as well, not just Java applicationsThe Scrum Retrospective is the closing ritual of a Sprint
This is the moment for the team to reflect on the past Sprint
For good collaboration and productivity of the team it is important to take time to improve these things
By letting the team come up with their own action points of improvement, you’re ensured that they’ll find them important and will implement them
There are many ways to implement the Retrospective
In this blog I’ll describe a way I developed and tested in practice with my team
I’ll describe the method in detail and present alternatives on some points
Important to the Scrum Retrospective is to provide space for all team members to express themselves and to express the things they are occupied with
To this end, a safe environment must be created, but also literally the space and time for anyone to have their say
First, I’ll outline some important preconditions
By setting these, you’ll have a good foundation
This makes the execution go more smoothly
Only a good conversation leader is still necessary to guide everything
The program for this Retrospective consists of a relatively short individual part and the rest is collective
The solo part is done in silence first
This way everyone can first think for themselves how the Sprint has progressed and what he (and she) considers important for the Retro
The common part is to understand each other’s views and experiences and then jointly come to action points that everyone can commit to
Suppose you’ll have 1 hour, that is 60 minutes, for the Retro, then the program can be laid out as follows
If you have another timebox, you can of course change the program accordingly
There are in total five rounds
Somebody can monitor the timebox of the rounds with a timer
Set the stage: grading In order to get everyone in an active and open attitude, each team member gives a rating for the last sprint
This is also called team happiness
This can be useful to compare different Sprints and to monitor progress in the experience
In addition to a number, another scale may be used, such as five stars or bad, moderate, sufficient, good, excellent
If a grade feels too much like reporting, then you can also ask for a single word to describe the Sprint
The latter also promotes creativity
Gather data: argumenting Next, everyone writes his reasons for his grade on post-its
These are observations and can be both positive and negative
It is useful to use different colors of post-its for positive and negative observations
Take 10 minutes for this
Only if everyone is done earlier you could stop earlier
It’s important to do this individually and in silence
This way, the team members don’t influence each other
During this round everyone has the maximum freedom to express themselves on post-its
Next, everyone orders his post-its in order of importance
The most important one is on top
Both positive and negative are stacked together
Generate insights: discussing themes In this round everyone stands up for an active attitude
They all stand around a blackboard, flip over or a glass wall
At least it has to be surface on which post-its can be stuck
Everyone can start a new theme in turn
It goes like this
The first team member hangs up his most important post-it
He explains this at the same time
Anyone may ask questions to clarify the arguments, observations, experience, problem, cause or issue
After that, everyone can stick a post-it that belongs to the same theme, next to it and read it and, if necessary, clarify it
This will continue until everyone has hung his post-its on this first theme
Then another team member may hang his most important (from the remaining post-its) as the second theme
The rest of the process goes the same as with the first theme
Then another one can start a third theme
This will continue until everyone’s post-its are finished or the timebox has expired
A timebox of 20 minute is appropriate
You may choose to let everybody stick and read their remaining post-its after the expiration of the timebox
However, they are not discussed further in detail
Sometimes it can be difficult to determine whether a new post-it belongs to the same theme or that it is actually a new theme
Usually this becomes apparent after there are hanging multiple post-its related to a theme
Then you can decide to split the theme
Let this happen dynamically and don’t be too rigid about it
Grouping by topic are only required to determine the topics to be discussed in the next round
At the end of this round, all post-its with observations are grouped by theme
Based on the number of notes per subject, it is clear how important it is
As a team, you’ll choose the most important or most important two to take to the next round
This also depends on whether two subjects have about the same number of notes or that a subject jumps out
You could also choose to determine the priority of the subjects by giving each team member a number of votes and holding a ballot
I ain’t in favor of this, because this way the team members can influence each other
My statement is simple, if someone has not written a note about a subject, he doesn’t deem that topic important enough
With a voting round, everyone can influence or convince each other to choose a topic
Decide what to do: form action points In this final round we’ll become concrete
We are going to formulate action points
These must be formulated SMART
These are agreements with the team that team members can hold each other to
Take 30 minutes for this round
For this round we will sit again at the meeting table
We will discuss the most important 1 to 3 themes to formulate up to 3 action points for them
Three is indeed a proven number to work on in the next sprint (and afterwards!)
Too much will lead to loss of focus
It’s important to formulate these action points concrete and SMART, and especially discuss the boundaries of the agreements to make them clear to everyone
Close the Retrospective: wrapping up Thank everyone for their contribution and ask for a quick review of this format
With this last feedback, the Retro itself can be improved
Hopefully, you will find this blog post useful and will put this whole format or partly into practice
I would like to hear your experiencesYou want to give your site visitors the best user experience
To accomplish this, your visitors can search as well as browse your content
Search is only relevant when results are meaningful to the visitor
In this talk, Jettro will introduce you to the concepts like precision, recall and relevancy
Jettro uses Examples to show concepts in the context of search, autocomplete, suggestions and grouping of results
Used technologies are: Angular, Java and elasticsearch
After the presentation you’ll have a good idea about the steps you need to take te present your visitors relevant resultsBuilding a story map adds an extra dimension to your otherwise flat product backlog
It’s well suited to keep a clear, user-facing overview of your product requirements while maintaining the benefits of an ordinary product backlog
In this talk, I will explore the value of building a story map alongside your product backlog
How it will visualize larger scenarios and guide your release schedule in a sensible and agile fashion
I will approach its use from both a new product perspective as well as existing product development
Frustrated by the limitations of a classic product backlog
Looking for a better way to organize your requirements without casting them in stone
This talk is for you!Bol.com is the number one online retailer in the Netherlands, and serves more than five million customers in the Netherlands and Belgium
Oracle’s search engine Endeca has been used for many years to handle millions of requests, but with the current and planned growth, bol.com is starting to reach the limits of Endeca
In January 2016, bol.com started with the migration towards Elasticsearch as the new search engine for the webshop
Now one year later, some parts of the webshop are fully migrated to Elasticsearch, with more to be added in the near future
This talk is about the journey of migrating from Endeca to Elasticsearch and all the technical/functional challenges that come with switching from the current search engine to ElasticsearchAt one of my former projects at a client of Luminis I got the opportunity to work with Apigee
Apigee is a platform for APIs
In this blog I’ll describe in an overview its features
In Apigee APIs can be easily created with shared modules, called policies, and flows
Because of this it is easy to arrange authentication and authorization, but also other things like protection against traffic spikes
Apigee is still a work-in-progress, so not all specifications are well documented and the version control system can still be improved a lot
That’s why we stored the versions in a git repository on Stash and deployed it with Jenkins to Apigee
On the other side, the versions of the APIs themselves can be disclosed through a segment in the path
A great advantage to base the architecture on this platform is uniformity in APIs and their maintainability
Analyses and statistics belong to the standard functionality
The consumers and the API (Proxies) can be connected, so filtering is possible on basis of both
There are a lot of standard policies with functionality available to compose a flow for an API
These consist of traffic management, security, mediation and extensions
The mediation policies are for conversion, extraction, and changing the request/response message
The extensions can be used for enclosing Java, JavaScript, Python and also to do call-outs to other APIs inside or outside Apigee, statistics and logging
Because of this the work for simple APIs and APIs with underlying applications can be limited to configuration in Apigee
On the other side it is also possible to put in some more code for fine-tuning or advanced capabilities
This can be done neatly in a management console, but can also be done directly in XML-files
By working with API Proxy’s in Apigee, several security measurements can be taken for all passing traffic, like authentication, authorization, validations to prevent code injections, and prevent spikes in traffic
It’s also possible to share variables within flows and environments
Besides that, there are also caching possibilities
By caching key-value-pairs and responses a higher performance can be achieved
Adding this layer and disclosing data like this offers many advantages, like re-usability of APIs and a central place to locate them
This is ideal for omnichannel, where companies want to disclose data through all channels, like websites, social media, apps, etc
Not only did we have front-end components on the website as consumers of our APIs, but also several apps
What can be done exactly in this layer, can vary from a simple proxy to complete applications
The proxy can be made for analyses and statistics, authentication, authorization, (first) validations, security measurements, and/or caching
This can also vary per API or API Proxy
In the context of microservices it can have advantages in terms of security and governance to disclose them through this platform
All in all, Apigee is a nice platform if you want to work with APIsLast week was our Luminis conference called Devcon 2017
For the third time we organise this conference with around 500 attendees and 20 speakers
This year we wanted to have a mobile app with the conference program and information about speakers
The app was created with the Ionic framework and the backend is a spring boot application
Before and during the conference we wanted to monitor the servers
We wanted to monitor the log files, hardware statistics and uptime
I used a number of beats to collect data, store the data in elasticsearch and show nice dashboards with Kibana
In this blog post I’ll explain to you the different beats
I’ll show you how to set up a system like this yourselves and I’ll show you some of the interesting charts that you can create from the collected data
Beats Beats is the platform for single-purpose data shippers
They install as lightweight agents and send data from hundreds or thousands of machines to Logstash or Elasticsearch
~ Elastic homepage Beats is a library to make it easier to build single purpose data shippers
Elastic comes with a number of beats themselves, but the community has already been creating their own beats as well
There are 4 beats I have experience with, for our monitoring wishes we needed three of them
Filebeat – Used to monitor files, can deal with multiple files in one directory, has module for files in well known formats like nginx, apache https, etc
Metricbeat – Monitors the resources of our hardware, think about used memory, used cpu, disk space, etc Heartbeat – Used to check endpoint for availability
Can check times it took to connect and if the remote system is up Packetbeat – Takes a deep dive into the packets going over the wire
It has a number of protocols to sniff like http, dns and amp
It also understand the packets being sent to applications like: MySql, MongoDB and Redis
The idea behind a beat is that it has some input, a file or an endpoint, and an output
The output can be elasticsearch, logstash but also a file
All beats come with default index templates that tell elasticsearch to create indexes with the right mapping
They also come with predefined Kibana dashboards that are easy to install
Have a look at the image below for an example
That should give you an idea of what beats are all about
In the next section I’ll show you the setup of our environment
The backend and the monitoring servers I want to tell you about the architecture of our application
It is a basic spring boot application with things like security, caching and spa enabled
We use MySql as a datastore
The app consists of two main parts
The first being an API for the mobile app
The second a graphical user interface created with Thymeleaf
The GUI is created to enable us to edit the conference properties like the speakers, the talks, used twitter tags, etc
We installed FileBeat and MetricBeat on the backend
We had a second server, this server was running elasticsearch
This second server is also the host for HeartBeat
The next image shows an overview of the platform
All beats were installed using the debian packages and using the installation guides from the elastic website
As the documentation is thorough I won’t go into details here
I do want to show the configurations that I used for the different beats
FileBeat Filebeat was used to monitor the nginx access logs files
Filebeat makes use of modules with predefined templates
In our case we use the nginx module
Below the configuration that we used
code filebeat.modules: - module: nginx output.elasticsearch: hosts: ["10.132.29.182:9200"] As we have been doing with logstash for a while, we want to enhance the log lines with things like browser extraction, gea enhancements
Elastic these days has an option to use ingest for this purpose
More information about how to install these ingest components can be found here
https://www.elastic.co/guide/en/elasticsearch/plugins/5.3/ingest-user-agent.html https://www.elastic.co/guide/en/elasticsearch/plugins/5.3/ingest-geoip.html An example of the filebeat dashboard in Kibana is below
MetricsBeat Next step is monitoring the CPU, load factor, memory usage per process
Installing MetricBeat is easy when using the debian package
Below the configuration I used on the server
code metricbeat.modules: - module: system metricsets: - cpu - load - filesystem - fsstat - memory - network - process enabled: true period: 30s processes: ['.*'] - module: mysql metricsets: ["status"] enabled: true period: 30s hosts: ["user:password@tcp(127.0.0.1:3306)/"] output.elasticsearch: hosts: ["10.132.29.182:9200"] As you can see, this one is bigger than the filebeat config
I configure he metrics to measure and how often to measure
In this example we measure every 30 seconds
We have two modules, the system module and the mysql module
With mysql module we get specific metrics about the mysql process
Below an idea of the available metrics
Interesting to see the amount of commands and threads
code { "_index": "metricbeat-2017.04.06", "_type": "metricsets", "_id": "AVtFR6tN2PYBRsZuE_1r", "_score": null, "_source": { "@timestamp": "2017-04-06T21:59:36.211Z", "beat": { "hostname": "devcon-api-2017", "name": "devcon-api-2017", "version": "5.3.0" }, "metricset": { "host": "127.0.0.1:3306", "module": "mysql", "name": "status", "rtt": 3186 }, "mysql": { "status": { "aborted": { "clients": 24, "connects": 2 }, "binlog": { "cache": { "disk_use": 0, "use": 0 } }, "bytes": { "received": 91999863, "sent": 447340795 }, "command": { "delete": 281, "insert": 2802, "select": 161874, "update": 7 }, "connections": 68, "created": { "tmp": { "disk_tables": 3126, "files": 6, "tables": 27404 } }, "delayed": { "errors": 0, "insert_threads": 0, "writes": 0 }, "flush_commands": 1, "max_used_connections": 11, "open": { "files": 22, "streams": 0, "tables": 311 }, "opened_tables": 7794, "threads": { "cached": 6, "connected": 3, "created": 13, "running": 1 } } }, "type": "metricsets" }, "fields": { "@timestamp": [ 1491515976211 ] }, "highlight": { "metricset.module": [ "@kibana-highlighted-field@mysql@/kibana-highlighted-field@" ] }, "sort": [ 1491515976211 ] } Another example report was already shown in the introduction of this post
HeartBeat This beat can be used to monitor the availability of other services
I used it to monitor the availability of our backend as well as our homepage
Configuration is as easy as this
code # Configure monitors heartbeat.monitors: - type: http urls: ["https://amsterdam.luminis.eu:443"] schedule: '@every 60s' - type: http urls: ["https://api.devcon.luminis.amsterdam:443/monitoring/ping"] schedule: '@every 30s' dashboards.enabled: true output.elasticsearch: hosts: ["localhost:9200"] You see both monitors, they check a url every 30 seconds or 60 seconds in the first monitor
With this beat you can explicitly enable the kibana dashboards in the configuration
An example dashboard is presented in the image below
Not so funny to see that our website has been unavailable for a moment
Luckily this was not our mobile app backend
Custom dashboards Of course you can use all the prefabricated dashboard
You can however still create you own dashboards, combine the available views that you need to analyse your platform
In my case I want to create a new view
I want to have an indication of the usage of urls of the api over time
Kibana 5.3 comes with a new chart type called heatmap chart
In the next few images I am stepping through the creation of this chart
At the end I’ll also do an analysis of the chart for the conference day
Our heatmap shows the amount of calls to specific url
The darker the block, the more hits
First we create a new visualisation, choose the heatmap
Next we need to chose the index to take the documents from
In our case we use the filebeat index
Next choose the x-axis
We create buckets for each time period, so a date histogram is what we need
Now we need a sub aggregation, divide the histogram buckets per url
So we add a subaggregation in the interface and chose the y-Axis
The sub aggregation can be a terms aggregation, use the field nginx.access.url
Have a look at the image above, we see a number of urls we are not interested in
The have very little hits or are not interesting for our analysis
We can do two things to filter out specific urls
One of them is excluding values
These are under the advanced trigger
Open it by clicking it, bottom left
Than we can enter a minimum doc count of 2
We can also exclude a number of url with the pattern
Finally let us have a better look at a larger version of this chart
In this chart we can see a few things
First of all the url /monitoring/ping
This is steady throughout the whole time period, which is no surprise as we call this url each 30 seconds using the heartbeat plugin
The second row with url /api/conferences/1is a complete json representation of the program, session details as well as speaker information
Most used at the beginning of the conference and it stopped around 17:00 when the last sessions started
At the beginning of the day people marked their favourite sessions
People were not consistent in rating session which can be found in the last line
Than there is the url /icon-256.png
This is an image used by android devices when a push notification was received
The darks green blocks were exactly the moments when we send out a push notification
Wrapping up The test with using beats for monitoring felt really good
I like how easy they are to setup and the information you can obtain using the different kind of beats
Will definitely use them more in the futureIn the first part of this series, we’ve briefly looked at the technologies that make up the SMACK stack
In this post, we’ll take a closer look at one of the fundamental layers of the stack: Apache Mesos
What is Apache Mesos
Apache Mesos is an open source cluster manager that provides efficient resource isolation and sharing across distributed applications or frameworks
Mesos manages a cluster of machines (virtual or physical) and can also be seen as a distributed systems kernel, because the Mesos kernel runs on every machine in the cluster and provides applications (e.g., Hadoop, Spark, Kafka, etc) with API’s for resource management and scheduling across an entire data center or cloud environment
One of the ideas behind Mesos is to deploy multiple distributed systems within the same shared pool of cluster nodes in order to increase resource utilization
Let’s get to know Mesos a little better by looking at some of the core Mesos concepts
Architecture By examining the Mesos architecture we will get to know the most important concepts to understand when dealing with Mesos
As you can see in the above figure, a Mesos architecture consists of a master daemon that manages agentdaemons running on nodes in the cluster
Apache Mesos also uses Apache ZooKeeper to operate
ZooKeeper acts as the master election service in the Mesos architecture and stores state for the Mesos nodes
Frameworks Next to the masters and agents, Mesos has the concept of frameworks
Frameworks within Mesos are responsible for running tasks on the Mesos agent nodes
A Mesos framework consists of two major components: a scheduler that registers with the Mesos master to be offered resources an executor process that is launched on slave nodes to run the framework’s tasks Mesos agents will notify the Mesos master about their available resources
Based on that information, the Mesos master determines how many resources to offer to each framework
After the Mesos master has decided which resources to offer, the scheduler then selects which of the offered resources to use
When a framework accepts offered resources, it passes Mesos a description of the tasks it wants to launch
Now that we went over the concept of frameworks, schedulers, and executors, it’s interesting to point out that most of the components that make up the SMACK stack are available as a Mesos framework
Spark, Kafka, and Cassandra are all available as a framework for Mesos
Mesos has support for a lot of different frameworks, you can find a more extensive list of available frameworks on the Mesos frameworks documentation page
Jobs Almost all data processing platforms will have the need for two different kinds of jobs: scheduled/periodic jobs – you can think of periodic batch aggregations or reporting jobs long-running jobs – stream processing or long running application jobs For the first kind of jobs, you can use the Chronos framework
Chronos is a distributed and fault-tolerant scheduler that runs on top of Apache Mesos and can be used for job orchestration
If you’re familiar with Linux you can compare it to a distributed version of cron
However, compared to regular cron, Chronos has a number of advantages
For instance, it allows you to schedule your jobs using ISO8601 repeating interval notation, which enables more flexibility in job scheduling
Next to that Chronos also supports arbitrarily long dependency chains
Jobs can be triggered by the completion of other jobs
This can very useful at times
For the long-running jobs, Mesos has the Marathon framework
Marathon is a fault tolerant and distributed ‘init’ system and can be used to deploy and run applications across a Mesos cluster
Marathon has many features that simplify running applications in a clustered environment, such as high-availability, node constraints, application health checks, an API for scriptability and service discovery
When it comes to application deployment Mesos has built in support for Blue-Green deployments and it also adds scaling and self-healing to the already big feature sets
In case a machine in the cluster dies, Marathon will make sure that the application is automatically spawned elsewhere in the cluster to make sure the application is always on and meets the preconfigured amount of instances
Marathon can be used to run any kind of executable process and is also often used as a PaaS solutions for running containers
Other Mesos frameworks can also be launched from Marathon
In combination with the self-healing ability, Mesos and Marathon make a very robust platform for running any kind of application
Getting some hands-on experience You can build Mesos from source, but probably the easiest way to getting some hands-on experience with Mesos in the context of the SMACK stack is by installing DC/OS
DC/OS stands for the open source DataCenter Operating System developed by Mesosphere, the company which is also actively working on Apache Mesos, and DC/OS is built on top of Apache Mesos
When experimenting with new technologies I always like to use Vagrant or Docker
Luckily Mesosphere has released a DC/OS vagrant project which we can easily use, but before we get started, make sure you have Git, Vagrant, VirtualBoxinstalled
The first step is installing the vagrant-host manager plugin, which will alter our /etc/hosts file with some new hostnames, so we can easily connect to the instances
apache $ vagrant plugin install vagrant-hostmanager Now let’s clone the dcos vagrant project: apache $ git clone https://github.com/dcos/dcos-vagrant Let’s configure a 3 node installation by copying the correct Vagrant configuration: apache $ cd dcos-vagrant $ cp VagrantConfig-1m-1a-1p.yaml VagrantConfig.yaml Now that we’ve configured everything, we can spin up our new DC/OS VMs
apache $ vagrant up During the installation, it might ask you for a password
You will need to enter your local machines password because vagrant is trying to alter your /etc/hosts file
It might take a little while before everything is up and running, but once it’s done you can navigate to http://m1.dcos/
If everything is setup alright you should be able to log in and see the DC/OS dashboard
As you can see from the screenshot above we currently have no tasks or services running, but now we’re all set to go and continue with our deep dive into the SMACK stack
Let’s install Marathon in DC/OS, so we can get some sense of what it takes to install a Mesos Framework
Go to Universe -> Packages and install Marathon as a Mesos framework
Once installed Marathon you should be able to see you now have running tasks on the dashboard
And by going to Services you should be able to see the marathon deployment happening within the Mesos cluster
Now if you want you can also open the service and go to the Marathon UI to start creating groups or applications, but we’ll leave that for next time
Marathon allows you to deploy processes and application containers and through the UI you can also easily scale these applications
Mesos frameworks like Marathon, Kafka, Cassandra and Spark can also be easily scaled from within DC/OS
It’s just a matter of a few clicks
In the above example, we installed Mesos via DC/OS in Vagrant, but you can also run DC/OS in the cloud or on premise
See the DC/OS get started page for more setup options
Summary Mesos is the fundamental layer of the SMACK stack that allows the applications to be run efficiently
It has all the features available to do proper auto recovery and can meet the scaling requirements required when you hit high load traffic on your architecture
Installing other parts of the stack is almost trivial with DC/OS and the Mesos frameworks for Kafka, Spark and Cassandra
In the next part of this series, we will dive deeper into Cassandra and start adding Cassandra to our local SMACK stack for storage of our data at handUser interface (or UI) guidelines are intended for designers and developers to ensure a suiting and consistent user experience of a specific product, product range or brand
The guideline can be a Word document, but these days it is more common to find UI guidelines on a Wiki or in another online format
Guidelines are an important resource, but there is a problem
Designers are great in designing, but usually lack the ability to document their thinking in a way others can continue their work along the lines of their original concept
I am a designer and I believe I’ve found a way
Creativity is my trade as a designer
It thrives in moderate chaos; being overly organised kills my ability to think outside boxes and shift angles to come up with original solutions
“To think the unthinkable one’s mind needs to wander freely.” I know how that sounds, but it’s true nonetheless
I am blessed to work with people that make up for my lack of organization
Colleagues that have the patience to put up with my chaos and turn my designs into actual products
But in writing UI guidelines I’m on my own
Although writing is in itself a creative process, you need to know what you want to say before you can start writing
Before I can write UI guidelines I need to analyse my own design process and decisions
It is sometimes a bit worrying that for some design decisions the only thing that seems to come to mind is “this solution just felt right to me.” Of course I can’t write that down
I have to backtrack my (mostly intuitive) design process to find out why I took those decisions
I have to turn intuition into reasoning
Having written and contributed to multiple UI guidelines I found that sticking to these following 6 rules enables me to write a decent guideline, bypassing my lack of organizational skills
Design a guideline like you would design any other product
A UI guideline is a utility; it is intended for a specific task and for a specific type of people
In order to know what to write and how to publish your writing, you first need to know who you are writing for and how these people will use your guideline
In that respect a guideline is not different from any other product I design
It helps a lot to involve the intended audience into your writing process
Ask them what they expect and need
Try to find out what they already know
Let them review and comment on your work
A guideline is never complete, so focus on doing enough
You can go on forever in describing every little UI detail and the harmony of it all, but you have to stop at some point
It is a tricky balance: write too little and people will not have enough to hold on to, write too much and people will not be able to grasp the big picture or how to apply the separate parts
My advise is to start small, see how it is understood and applied and then add more if needed
A guideline is a living thing, facilitate the process
New things will get added in the future, things will get changed
Common solutions today might be deprecated tomorrow
You need a solid process in writing and maintaining the guideline to keep ensuring it’s quality in the future
Make sure users can comment on (parts of) the guideline and suggest changes or additions
Keep users posted on new versions of the guideline and what has changed
Not just write the rules, but also the reasoning
You can’t cover everything in a limited set of rules
You need to make your audience smart enough to come up with solutions for unforeseen situations
So back up every rule with design principles
These design principles are what connects your rules, patterns and elements with the product vision
Also: spending an hour or two face-to-face with your intended audience to explain the basic UI concept can be more effective than weeks of writing
Write guidelines while you are designing
Design principles and patterns emerge during the process of designing a user interface, so you best frame them when you identify them
Framing design principles and patterns also helps you in designing a better, more coherent product, even if you are not intending to write actual guidelines for the product
Add examples
Lots of them
Because examples really show how the separate parts of your guideline come together and make a user interface
It makes your guideline more accessible and if done right really proves the value of your design efforts
You could even add your own design files so they can be used as design templates to get people up and running fastIn two previous posts, we googled doc2vec [1] and “implemented” [2] a simple version of a doc2vec algorithm
You could say we gave the specifications for a doc2vec algorithm
But we did not actually write any code
In this post, we’ll code doc2vec, according to our specificication
Together, these three blog posts give some understanding of how doc2vec works, under the hood
Understanding by building
We’ve made a Jupyter notebook [3], and we’ll walk you through it by highlighting some key parts
As a starting point for the notebook, we’ve used the word2vec implementation [4] from the Vincent Vanhoucke Udacity course
We’ve modified basically everything in that a bit, and added some new stuff
But the idea remains the same: a minimal code example for educational purposes
What we’ll do We’ll get and preprocess some documents from a well known Reuters corpus
We’ll train a small doc2vec network on these documents
While we train it, the document vectors for each document are changing
Every so many steps, we use the document vectors
First, we visualise them with T-SNE in a two-dimensional space
We color code the document vectors with class labels (the news categories from the Reuters corpus), to see if document vectors belong to the same class are getting closer to each other
Second, we train a simple linear classifier using the document vectors as input, to predict class labels
We’ll call this prediction task the end-to-end task
We’ll observe the following: 1
The doc2vec network it’s loss decreases as it trains
This is a good sign
2
If we train doc2vec longer, the performance for the end-to-end task increases
This is another good sign
3
The two-dimensional visualisation with color-coded document vectors is not very informative
This is too bad, but not a problem
In the end, it’s just a visualisation
Preprocessing The first step is preprocessing
The following one liner reads in the entire Reuters data set in memory
If your data is very big, this is not advisable of course
But: early optimization is the root of all evil 🙂 python ..
fileid2words = {fileid: [normalize(word) for word in word_tokenize( reuters.raw(fileid)) if accept(word)] \ for fileid in reuters.fileids() if accept_doc(fileid)} ..
In words: we iterate over Reuters files, and accept some of the documents
For each document, we accept some of the words
And the words we do accept, we normalise
Let’s take a closer look at each step
javascript def accept_doc(fileid): return fileid.startswith('training/') \ and np.random.random() * 100 < PERCENTAGE_DOCS We only accept documents from the training set of the Reuters corpus
And we select a random percentage of these documents according to the hyperparameter PERCENTAGE_DOCS that we have set by hand at the top of the notebook
python def accept(word): # Accept if not only Unicode non-word characters are present return re.sub(r'\W', '', word) != '' We refuse words that consist entirely of non-word characters
The words that are refused here are taken out of the token stream before anything happens
You can play with this and refuse other words, too
For example, stopwords like ‘the’, ‘and’, etc
This may or may not be a good idea
One way of learning more about it is to play with it and see what happens to your performance in the end-to-end task
python def normalize(word): return word.lower() And we lowercase all tokens
This is a first step to reduce data sparsity of natural language
There are other ideas, too
For example, replacing numbers with a special NUMBER token, or spelling out numbers with words, so that ‘123’ becomes ‘one two three’
There is always a tradeoff: normalising tokens may lead to some information loss
After we have obtained the dictionary fileid2words, we build our vocabulary: python ..
count = [['__UNK__', 0], ['__NULL__', 0]] count.extend([(word, count) for word, count in collections.Counter( [word for words in fileid2words.values() \ for word in words]).most_common( VOCAB_SIZE - 2 + REMOVE_TOP_K_TERMS)[ REMOVE_TOP_K_TERMS: ] if count >= MIN_TERM_FREQ]) ..
Here, first we flatten the dictionary, our entire dataset, to just a sequence of tokens (words)
Then we count the occurence of all the unique words
We add these counts to the counts of two special tokens: __UNK__ and __NULL__
We use the most common words as our vocabulary
We’ll remove the top k most common terms, because these tend to be stopwords
And we require the word frequency to be higher than a certain minimum
That is because we can hardly expect our network to predict a term that would only occur, say, once in the whole corpus
Occurences for words that did not end up in our vocabulary will later on be replaced with the __UNK__ (unknown) token
So at this point no words will be taken out of the corpus anymore, they will only be replaced
One thing that you can try is if it works better to really remove stopwords from the corpus
Don’t worry about the __NULL__ token, it is only used when our documents are too short to even fit a single text window in (remember that in doc2vec, we try to predict words from fixed size text windows that occur in a document)
That will not happen often in the Reuters corpus
Training the network In Tensorflow, training a network is done in two steps
First, you define the model
You can think of the model as a graph
Second, you run the model
We’ll take a look at the first step, how our model is defined
First: the input
python # Input data dataset = tf.placeholder(tf.int32, shape=[BATCH_SIZE]) labels = tf.placeholder(tf.int32, shape=[BATCH_SIZE, 1]) The dataset is defined as a placeholder with the shape of a simple array that contains ints
When we run the model, it will contain document ids
Nothing more, nothing less
It will contain as many document ids as we want to feed the stochastic gradient descent algorithm in a single iteration
The labels placeholder is also a vector
It will contain integers that represent words from the vocabulary
So, basically, for each document id we want to predict a word that occurs in it
In our implementation, we make sure that a batch contains one or more text windows
So if we use a text window of size 8, a batch will contain one or more sequences of eight consecutive words
Next, we take a look at the weights in our neural network: python # Weights embeddings = tf.Variable( tf.random_uniform([len(doclens), EMBEDDING_SIZE], -1.0, 1.0)) softmax_weights = tf.Variable( tf.truncated_normal( [vocab_size, EMBEDDING_SIZE], stddev=1.0 / np.sqrt(EMBEDDING_SIZE))) softmax_biases = tf.Variable(tf.zeros([vocab_size])) You can think of embeddings as the transpose of the matrix from our previous post [2]
In its rows, it has a document vector of length EMBEDDING_SIZE for each document id
This document vector is also called an “input vector”
You can also think of embeddings as the weights between the input layer and the middle layer of our small doc2vec network
When we run the session, we will initialize the embeddings variable with random weights between and 
The softmax_weights are the weights between the middle layer and the output layer of our network
You can also think of them as the matrix from our previous post
On its rows, it has an “output vector” of length EMBEDDING_SIZE for each word in the vocabulary
When we run the model in our session, we will initialize these weights with (truncated) normally distributed random variables with mean zero and a standard deviation that is inversely proportional to EMBEDDING_SIZE
Why are these variables initialized using a normal distribution, instead of with a uniform distribution like we used for the embeddings
The short answer is: because this way of initialisation has apparently worked well in the past
You can try different initialisation schemes yourself, and see what it does to your end-to-end performance
The long answer; well, perhaps that’s food for another blog post
The softmax_biases are initialised here with zeroes
In our previous post, we mentioned that softmax biases are often used, but omitted them in our final loss function
Here, we used them, because the word2vec implementation we based this notebook on used them
And the function we use for negative sampling wants them, too
The activation in the middle layer, or, alternatively, the estimated document vector for a document id is given by embed: python embed = tf.nn.embedding_lookup(embeddings, dataset) tf.nn.embedding_lookup will provide us with fast lookup of a document vector for a given document id
Finally, we are ready to compute the loss function that we’ll minimise: python loss = tf.reduce_mean( tf.nn.sampled_softmax_loss( softmax_weights, softmax_biases, embed, labels, NUM_SAMPLED, vocab_size)) Here, tf.nn.sampled_softmax_loss takes care of negative sampling for us
tf.reduce_mean will compute the average loss over all the training examples in our batch
As an aside, if you take a look at the complete source code in the notebook, you’ll notice that we also have a function test_loss
That function does not use negative sampling
It should not, because negative sampling underestimates the true loss of the network
It is only used because it is faster to compute than the real loss
When you run the notebook, you will see that the training losses it prints are always lower than the test losses
One other remark about the test loss is the following: the test examples are taken from the same set of documents as the training examples! This is because in our network, we have no input to represent a document that we have never seen before
The text windows that have to be predicted for a given document id are different though, in the test set
A two-d visualisation of the document vectors with T-SNE As we train our network, as we minimise our loss function, we keep making small changes to all the weights in the network
You can think of the weights between the embedding layer and the output layer as weights that affect where the decision boundaries of our output neurons are located
And you can think of the weights between our input layer and the embedding layer as weights that determine where our documents are projected in the embedding space
These weights help determine our document vectors
We train our network to predict words from text windows sampled from documents
So, intuitively, as we train longer, the documents change location in the embedding space in such a way that it becomes easier to predict the words that occur in these documents
And now here comes an important point
If we can predict which words occur in a document, then perhaps we can also say something about the topics in a document, or about the genre or category of the document
This is in fact an important motivation for using doc2vec as a step in a document classification algorithm
Okay
So, as we train to predict words, our document vectors keep changing location in the embedding space
If we want to use the document vectors as input for a document classifier, we hope that documents with the same class are located close to each other in the embedding space
If we want to make a visualisation that will show if this is happening, we need to use a dimensionality reduction algorithm, because we can’t visualise our embedding space if it has more than three dimensions
Because we are interested in the distance between document vectors, we use T-SNE
T-SNE is an algorithm that aims to project points close to each other that were also close to each other in the original high-dimensional space
And the other way around for points that are far apart in the high-dimensional space
And after we visualise the document vectors in 2D, we color code the points: purple points have the most common Reuters class as one of the class labels
In our experiments this was usually the class ‘earn’
The rest of the points does not have this label
Sadly, in experiments we did not see a clear purple cluster yet
Often, it looked something like this: However, it is hard to measure the quality of the document vectors by visualising them like this
To do that more quantitavely, we need to perform the end-to-end task that want to used the document vectors for
Predicting the most common class label from document vectors The end-to-end task that we’ll address in this post is to classify for each document whether or not it has the class label for the most common Reuters class (‘earn’, in most of the Reuters samples in my experiments)
This is a binary classification task
As input we’ll use the document vectors that are learned by our doc2vec network
As algorithm we’ll use a linear classifier, because the inventors of doc2vec specifically mention that paragraph vectors are suitable as input for any kind of linear classification algorithm
It makes sense, too: doc2vec itself has only one classification layer on top of the embedding layer
Each neuron in its output layer has an activation that is just a weighted sum, just a linear function, of the document vector in the embedding layer
And then all that happens after that is the softmax function
Predicting the class label of a document is easier than predicting the words in a document
So our first try will just be to use a simple linear classifier for document classification as well
Just to show that the classification algorithm we use does not have to be a neural network, we’ll use a support vector machine with a linear kernel
As an evaluation metric, we calculate precision and recall for both classes; then we calculate the harmonic mean of these two for each class: this is the F1 score
Then we take the weighted average of the F1 scores of the two classes: weighted by the number of observations of both classes
We report scores on a random subset of the Reuters training set
We do not report scores on the Reuters test set yet, because we are only in an exploratory phase yet here
In general, you should use test data sparingly, in this post, we’ll not use it at all! Every so many steps of training, we pause training of the network, and we use the current document vectors to train a classifier
In one of hour experiments, where we trained doc2vec on all documents in the Reuters training set, we got the following F1 results: Number of SGD iterations (steps) average F1 PV-DBOW training loss PV-DBOW test loss 30K 0.53 2.6 4.6 60K 0.57 2.4 4.3 90K 0.60 2.3 4.2 100K 0.61 2.2 4.2 What you can see here in the third and fourth column is that the training loss and test loss of our doc2vec network slowly decreases as we are training
We are slowly getting better at predicting the words from the document ids
But what is more important is that you can see in the second column that the performance on the end-to-end task (document classification) is also increasing as we are training the network longer
That means we are on the right track, even if the absolute performance is not yet that impressive
Now that we have walked you through the main steps of our Jupyter notebook [3], go ahead and run it yourself! All you need is a working installation of Python 3, numpy, pandas, sklearn, Tensorflow and Jupyter Notebook
pandas is not essential, we only use it to output some descriptive statistics quickly
You can implement alternatives for all of the many choices we had along the way
You can try different values for the configuration variables at the top of the notebook
It should be quite possible to improve the performance that we got in our first experiments: good luck!
In the beginning, do set `PERCENTAGE_DOCS` to a lower value, e.g., something like 5 percent
100K steps of training on all training set documents in the Reuters dataset took about an hour on a single desktop computer with 8 cores
You don’t want to wait that long just to try out if everything works
References 1
https://amsterdam.luminis.eu/2017/01/16/googling-doc2vec/ 2
https://amsterdam.luminis.eu/2017/01/30/implementing-doc2vec/ 3
https://github.com/luminis-ams/blog-doc2vec/blob/master/pvdbow.ipynb 4
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynbAt Luminis Amsterdam, Search is one of our main focus points
Because of that, we closely keep an eye out for upcoming features
Only a few weeks ago, I noticed that the following pull request (“Add field collapsing for search request”) was merged into the Elasticsearch code base, tagged for the 5.3/6.x release
This feature allows you to group your search results based on a specific key
In the past, this was merely possible by using a combination of an ‘aggregation’ and ‘top hits’
Now a good question would be: ‘why would I want this?’ or ‘what is this grouping you are talking about?’
Imagine having a website where you sell Apple products
MacBook’s, iPhones, iPad’s etc… Let’s say because of functional requirements, we have to create separate documents for each variant of each device
(eg
separate documents for iPad Air 2 32GB Silver, iPad Air 2 32GB Gold etc..) When a user searches for the word ‘iPad’, having no result grouping, will mean that your users will see search results for all the iPads you are selling
This could mean that your result list looks like the following: iPad Air 2 32GB Pink iPad Air 2 128GB Pink iPad Air 2 32GB Space Grey iPad Air 2 128GB Space Grey .
.
.
.
.
iPad Pro 12.9 Inch 32GB Space Grey Ipad Case with happy colourful pictures on it
Now for the sake of this example, let’s say we only show 10 products per page
If our user was really looking for an iPad case, he wouldn’t see this product, but instead, would be shown a long list of ‘the same’ iPad
This is not really user-friendly
Now, a better approach would be to group all the Ipad Air 2 products in one, so that it would take only 1 spot in the search results list
You would have to think of a visual presentation in order to notify the user that there are more variants of that same product
As mentioned before, grouping of results was already possible in older versions of Elasticseach, but the downside of this old approach was that it would use a lot of memory when computing this on big data sets, plus that paginating result was not (really) possible
An example: java GET shop/_search { "size": 0, "query": { "match": { "title": "iPad" } }, "aggs": { "collapse_by_id": { "terms": { "field": "family_id", "size": 10, "order": { "max_score": "desc" } }, "aggs": { "max_score": { "max": { "script": "_score" } }, "top_hits_for_family": { "top_hits": { "size": 3 } } } } } } We perform a Terms aggregation on the family_id, which results in the grouping we want
Next, we can use top_hits to get the documents belonging to that family
All seems well
Now let’s say we have a website where users are viewing 10 products per page
In order for users to go to the next page, we would have to execute the same query, up the number of aggregations to 20 and remove the first 10 results
Aggregations use quite some processing power, so having to constantly aggregate over the complete set will not be really performant when having a big data set
Another way would be to eliminate the first page results by executing a query with for page 2 combined with a filter to eliminate the families already shown
All in all, this would be a lot of extra work in order to achieve a field collapsing feature
Now that Elasticsearch added the field collapsing feature, this becomes a lot easier
You can download my gist( with some setup for if you want to play along with the example
The gist contains some settings/mappings, test data and the queries which I will be showing you in a minute
Alongside query, aggregations, suggestions, sorting/pagination options etc.
Elasticsearch has added a new ‘collapse’ feature: java GET shop/_search { "query": { "match": { "title": "Ipad" } }, "collapse": { "field": "family_id" } } The simplest version of collapse only takes a field name on which to form the grouping
If we execute this query, it will generate the following result: java "hits": { "total": 6, "max_score": null, "hits": [ { "_index": "shop", "_type": "product", "_id": "5", "_score": 0.078307986, "_source": { "title": "iPad Pro ipad", "colour": "Space Grey", "brand": "Apple", "size": "128gb", "price": 899, "family_id": "apple-5678" }, "fields": { "family_id": [ "apple-5678" ] } }, { "_index": "shop", "_type": "product", "_id": "1", "_score": 0.05406233, "_source": { "title": "iPad Air 2", "colour": "Silver", "brand": "Apple", "size": "32gb", "price": 399, "family_id": "apple-1234" }, "fields": { "family_id": [ "apple-1234" ] } } ] } Notice the total amounts in the query response, showing the total amount of documents that were matched against the query
Our hits only return 2 hits, but if we look at the ‘fields’ section of the result, we can see our two unique family_id’s
The best matching result for each family_id is returned in the search results
It is also possible to retrieve the documents directly for each family_id by adding an inner_hits block inside collapse: java GET shop/_search { "query": { "match": { "title": "iPad" } }, "collapse": { "field": "family_id", "inner_hits": { "name": "collapsed_by_family_id", "from": 1, "size": 2 } } } You can use ‘from:1’ to exclude the first hit in the family, since it’s already the returned parent of the family Which results in: java "hits": { "total": 6, "max_score": null, "hits": [ { "_index": "shop", "_type": "product", "_id": "5", "_score": 0.078307986, "_source": { "title": "iPad Pro ipad", "colour": "Space Grey", "brand": "Apple", "size": "128gb", "price": 899, "family_id": "apple-5678" }, "fields": { "family_id": [ "apple-5678" ] }, "inner_hits": { "collapsed_family_id": { "hits": { "total": 2, "max_score": 0.078307986, "hits": [ { "_index": "shop", "_type": "product", "_id": "6", "_score": 0.066075005, "_source": { "title": "iPad Pro", "colour": "Space Grey", "brand": "Apple", "size": "256gb", "price": 999, "family_id": "apple-5678" } } ] } } } }, { "_index": "shop", "_type": "product", "_id": "1", "_score": 0.05406233, "_source": { "title": "iPad Air 2", "colour": "Silver", "brand": "Apple", "size": "32gb", "price": 399, "family_id": "apple-1234" }, "fields": { "family_id": [ "apple-1234" ] }, "inner_hits": { "collapsed_family_id": { "hits": { "total": 4, "max_score": 0.05406233, "hits": [ { "_index": "shop", "_type": "product", "_id": "2", "_score": 0.05406233, "_source": { "title": "iPad Air 2", "colour": "Gold", "brand": "Apple", "size": "32gb", "price": 399, "family_id": "apple-1234" } }, { "_index": "shop", "_type": "product", "_id": "3", "_score": 0.05406233, "_source": { "title": "iPad Air 2", "colour": "Space Grey", "brand": "Apple", "size": "32gb", "price": 399, "family_id": "apple-1234" } } ] } } } } ] } Paging was an issue with the old approach, but since documents are grouped inside the search results, paging works out of the box
Same way as it does for normal queries and with the same limitations
A lot of people in the community have been waiting for this feature and I’m excited that it finally arrived
You can play around with the data set and try some more ‘collapsing’ (eg by color, brand, size etc..)
I hope this gave you a small overview of what’s to come in the upcoming 5.3/6.x releaseLast week my colleague Piet claimed: “You shouldn’t need several hours to understand what a method, class or package does”
Since unit tests are written in classes and methods, the same holds here
Welcome to the next episode of “reducing mental effort for software developers”
In this post I will lay out how AssertJ can help to reduce the mental effort needed while reading and writing test code, and as a bonus how it reduces the effort needed for understanding results of failing tests
AssertJ is a library that provides fluent assertions for Java
Before I dive into the fluent part, let’s start with some examples of assertions
Suppose you want to check that a String is of a certain value
In JUnit this will be done in the following way: Clean code reads like well-written prose java assertEquals("expected", "result"); In natural language this statement can be described as: “assert that expected and result are equal”
The same check with AssertJ can be done with: java assertThat("result").isEqualTo("expected"); Comparing to JUnit, the two values are in a reversed order
With assertThat() you specify which value you want to check, followed by isEqualTo() you specify to which value it should comply
Now the statement is expressed in a way closely to that of natural language
If you would strip the punctuation marks and “de-CamelCase” it, you’ll get the sentence: “assert that result is equal to expected”
My English may not be perfect, but this statements sounds a lot more like a sane and natural sentence
Because the Strings of these two examples are unequal, these tests will fail with the message: java org.junit.ComparisonFailure: Expected :expected Actual :result Sometimes I come across unit tests where expected and result are swapped like this: java assertEquals("result", "expected"); This is correct, but can be confusing when you’ve broken some tests and reading the message: java org.junit.ComparisonFailure: Expected :result Actual :expected In this example it’s quite obvious that something is wrong in the test, but imagine that in more obscure situations you’ll need a lot more mental effort before you find out what’s wrong and why the test is failing
AssertJ does not offer bullet proof protection against these kind of programming errors, but it will reduce the chance
A bell should ring when you read or write: java assertThat("expected").isEqualTo("result"); We don’t want to know if our expectation is correct! We want to know if the result is correct, i.e
that it meets our expectation
These equals checks are simple examples to make a clear difference between plain JUnit and the fluent assertions of AssertJ
The real power of fluent kicks in when applying multiple assertions in one single statement
For example: java assertThat(alphabet) .isNotNull() .containsIgnoringCase("A") .startsWith("abc") .endsWith("xyz"); As we’ve seen before, this statement reads like natural language
In JUnit on the other hand the equivalent test will read like: java assertNotNull(alphabet); assertTrue(alphabet.toUpperCase().contains("A")); assertTrue(alphabet.startsWith("abc")); assertTrue(alphabet.endsWith("xyz")); Apart from needing four separate statements, we now discover that JUnit provides quite a limited API
Bluntly, JUnit can check that something is true/false or that something is null (or not)
Using only JUnit we can’t say: “check that this String contains the character A”
We have to use the contains method of Java’s String class, and then check that its result is true
Let’s zoom in on the example of contains()
The JUnit the test: java assertTrue("abc".contains("A")); will fail with the message: java java.lang.AssertionError at org.junit.Assert.fail(Assert.java:86) at org.junit.Assert.assertTrue(Assert.java:41) at org.junit.Assert.assertTrue(Assert.java:52) at assertj.Strings.contains_junit(StringsTest.java:34) ..
This does not give away any information about what is wrong
Something went wrong with contains, but what String was tested
And what did we expect it to contain
When this happens while running the test in your IDE you hopefully can click somewhere so that it jumps to the line where it failed (line 34 in StringsTest.java) so you can find the error by looking at the assertion statement
But when reading the test results report from a Continuous Integration server on the other hand you have no context..
With Fluent Assertions the same test would be written as: java assertThat("abc").contains("A"); Because we exactly tell what we want to test (that “abc” contains the character A), AssertJ has enough information to tell us what went wrong
So this test fails with the message: java java.lang.AssertionError: Expecting: <"abc"> to contain: <"A"> Both in your IDE as on the CI server this will save a lot of time and mental effort because you see what’s wrong in a glance
We’ve now seen how we can write better readable tests which give more information when a test fails
Until now I only gave examples with Strings, but AssertJ provides API’s for more data types
All examples can be found on AssertJ’s website, but let me highlight another commonly used data type
Collections Suppose we want to test this List of Strings: java List numberList = Arrays.asList("One", "Two"); In JUnit this will look like: java assertEquals(Arrays.asList("Two"), numberList); And this fails with the message: java Expected :[Two] Actual :[One, Two] Using AssertJ the same would look like: java assertThat(numberList).containsExactly("Two"); and this fails with the message: java Actual and expected should have same size but actual size was: <2> while expected size was: <1> Actual was: <["One", "Two"]> Expected was: <["Two"]> So AssertJ tells us that the size is incorrect
Nice, we do not have the scan all the elements to find out what the difference is ourselves
Another example where the size is equal, but the ordering is different
JUnit’s: java assertEquals(Arrays.asList("Two", "One"), numberList); will fail with: java Expected :[Two, One] Actual :[One, Two] While AssertJ’s: java assertThat(numberList).containsExactly("Two", "One"); will fail with: java Actual and expected have the same elements but not in the same order, at index 0 actual element was: <"One"> whereas expected element was: <"Two"> In these examples the lists only contained two elements, but when the list is larger, it will get hard to find out which element is missing, or to see the difference
A last example where the difference in Collections is a bit more obscure
Suppose we want to check if the following List of numbers correctly counts up: java List largeNumberList = Arrays.asList(1, 2, 2, 4, 5); JUnit's: java assertEquals(Arrays.asList(1, 2, 3, 4, 5), largeNumberList); will fail with: java Expected :[1, 2, 3, 4, 5] Actual :[1, 2, 2, 4, 5] Unless you become happy from playing a game of spot the difference this results in needless occupation of your mental capacity
And that while AssertJ's: java assertThat(largeNumberList).containsExactly(1, 2, 3, 4, 5); fails with: java Expecting: <[1, 2, 2, 4, 5]> to contain exactly (and in same order): <[1, 2, 3, 4, 5]> but could not find the following elements: <[3]> In a glance we see what is wrong
Again, when Collections tends to be larger in size, these kind of failure messages are only getting more helpful
Why not Hamcrest
Well fair point
Hamcrest core has been included in JUnit since version 4.4 and tests using the hamcrest API look a lot more like AssertJ than that they look like plain JUnit
Also the failure messages are better than in Plain JUnit
But in my opinion Hamcrest does both these jobs not as well as AssertJ
Let’s compare the two
Comparing Strings with Hamcrest: java assertThat("abc", containsString("A")); fails with: Expected: a string containing "A" but: was "abc" At least we see the expected (containing “A”) and actual ( “abc” ) here, so that’s better than JUnit
At this point Hamcrest still reads like natural language just like the Fluent Assertions
But let’s get back on the example with multiple assertions on the letters of the alphabet String
With Fluent Assertions we saw: java assertThat("abc") .isNotNull() .startsWith("abc") .endsWith("xyz"); which fails with: java Expecting: <"abc"> to end with: <"xyz"> The equivalent in Hamcrest will look like: java assertThat("abc", allOf( is(notNullValue()), startsWith("abc"), endsWith("xyz"))); and fails with: java Expected: (is not null and a string starting with "abc" and a string ending with "xyz") but: a string ending with "xyz" was "abc" Decide for yourself which failure message requires less effort to understand what is tested and what went wrong
As we can see in the test itself Hamcrest provides a prefix notation like API to perform multiple assertions
This requires the reader to create a mental model of a stack with the operators like allOf() and is() while understanding the different assertions
With the given example this may sound exaggerated, but in more complex situations this requires quite some mental effort
As I said in the beginning only hamcrest-core is part of JUnit, which is quite limited
When you want to test collections for example you need to add hamcrest-all to your project
And when already adding an extra dependency to your project anyway, why not choose assertj
Last release of Hamcrest dates back to 2012, while AssertJ is more actively developed (may 2017) and supports Java8 features
Last reason why I think AssertJ is the best, the only and nothing but the best is code completion
Additional advantage of its Fluent API is that we can simply use code completion to explore all the possibilities
Without the the need for memorizing the whole API or the need for cheat sheets
Getting Started The website of AssertJ is full of examples and instructions on how to include AssertJ in your project
For an extensive set of examples see the assertj-examples tests project on Github
When you’re using Eclipse, see this tip to get code completion
You could do the same for Mockito by the way 😉 While the examples in this post were in Java with the AssertJ library, the same ideas apply for other languages
See for example fluentassertions.com for .NET
After reading this, I hope you’re even more devoted to create code that is simple and direct
Or as Grady Booch, author of Object Oriented Analysis and Design with Applications, said:Elasticsearch is a search solution based on Lucene
It comes with a lot of features to enrich the search experience
Some of these features have been recognised as very useful in the analytics scene as well
Interacting with elasticsearch mainly takes place using the REST endpoint
You can do everything using the different available endpoints
You can create new indexes, insert documents, search for documents and lots of other things
Still some of the things are not available out of the box
If you need an analyser that is not available by default, you can install it as a plugin
If you need security, you can install a plugin
If you need alerting, you can install it as a plugin
I guess you get the idea by now
The plugin extension option is nice, but might be a bit hard to begin with
Therefore in this blog post I am going to write a few plugins
I’ll point you to some of the resources I used to get it running and I want to give you some inspiration for your own ideas for cool plugins that extend the elasticsearch functionality
Bit of history In the releases prior to version 5 there were two type of plugins, site and java plugins
Site plugins were used extensively
Some well known examples are: Head, HQ, Kopf
Also Kibana and Marvel started out as a site plugin
It was a nice feature, however not the core of elasticsearch
Therefore the elastic team deprecated site plugins in 2.3 and the support was removed in 5
How does it work The default elasticsearch installation already provides a script to install plugins
You can find it in the binfolder
You can install plugins from repositories but also from a local path
A plugin comes in the form of a jar file
Plugins need to be installed on every node of the cluster
Installation is as simple as the following command
bin/elasticsearch-plugin install file:///path/to/elastic-basic-plugin-5.1.2-1-SNAPSHOT.zip In this case we install the plugin from our own hard drive
The plugins have a dependency on the elastic core and therefore need to have the exact same version as the elastic version you are using
So for each elasticsearch release you have to create a new version of the plugin
In the example I have created the plugin for elasticsearch 5.1.2
Start with our own plugin Elastic uses gradle internally to build the project, I still prefer maven over gradle
Luckily David Pilato wrote a good blog post about creating the maven project
I am not going to repeat all the steps of him
Feel free to take a peek at the pom.xml I used in my plugin
Create BasicPlugin that does nothing The first step in the plugin is to create a class that starts the plugin
Below is the class that has just one functionality, print a statement in the log that the plugin is installed
javascript public class BasicPlugin extends Plugin { private final static Logger LOGGER = LogManager.getLogger(BasicPlugin.class); public BasicPlugin() { super(); LOGGER.warn("Create the Basic Plugin and installed it into elasticsearch"); } } Next step is to configure the plugin as described by David Pilato in his blog I mentioned before
We need to add the maven assembly plugin using the file src/main/assemblies/plugin.xml
In this file we refer to another very important file, src/main/resources/plugin-descriptor.properties
With all this in place we can run maven to create the plugin in a jar
mvn clean package -DskipTests In the folder target/releases you’ll now find the file elastic-basic-plugin-5.1.2-1-SNAPSHOT.zip
Which is a jar file in disguise, we could change the extension to jar, there is no difference
Now use the command from above to install
If you get a message that the plugin is already there, you need to remove it first bin/elasticsearch-plugin remove elastic-basic-plugin Then after installing the plugin you’ll find the following line in the log of elasticsearch when starting [2017-01-31T13:42:01,629][WARN ][n.g.e.p.b.BasicPlugin ] Create the Basic Plugin and installed it into elasticsearch This is of course a bit silly, let us create a new rest endpoint that checks if the elasticsearch database contains an index called jettro
Create a new REST endpoint The inspiration for this endpoint came from another blog post by David Pilato: Creating a new rest endpoint
When creating a new endpoint you have to extend the class org.elasticsearch.rest.BaseRestHandler
But before we go there, we first initialise it in our plugin
To do that we implement the interface org.elasticsearch.plugins.ActionPlugin and implement the method getRestHandlers
javascript public class BasicPlugin extends Plugin implements ActionPlugin { private final static Logger LOGGER = LogManager.getLogger(BasicPlugin.class); public BasicPlugin() { super(); LOGGER.warn("Create the Basic Plugin and installed it into elasticsearch"); } @Override public List<Class<
extends RestHandler>> getRestHandlers() { return Collections.singletonList(JettroRestAction.class); } } Next is implementing the JettroRestAction class
Below the first part, the constructor and the method that handles the request
In the constructor we define the endpoint url patterns that this endpoint supports
The are clear from the code I think
Functionality wise, if you call without an action or with another action than exists, we return a message, if you ask for existence we return true or false
This handling is done in the prepareRequest method
javascript public class JettroRestAction extends BaseRestHandler { @Inject public JettroRestAction(Settings settings, RestController controller) { super(settings); controller.registerHandler(GET, "_jettro/{action}", this); controller.registerHandler(GET, "_jettro", this); } @Override protected RestChannelConsumer prepareRequest(RestRequest request, NodeClient client) throws IOException { String action = request.param("action"); if (action != null && "exists".equals(action)) { return createExistsResponse(request, client); } else { return createMessageResponse(request); } } } We have two utility classes that transform data into XContent: Message and Exists
The implementations of the two methods: createExistsResponse and createMessageResponse, can be found here
Time to re-install the plugin, first build it with maven, remove the old one and install the new version
Now we can test it in a browser or with curl
I personally use httpie to do the following requests
This way we can create our own custom endpoint
Next we dive a little bit deeper into the heart of elastic
We are going to create a custom filter that can be used in an analyser
Create a custom Filter The first part is registering the Filter in the BasePlugin class
We need to extend the interface org.elasticsearch.plugins.AnalysisPlugin and override the method getTokenFilters
We register a factory class that instantiates the filter class
The registration is done using a name that can later on be used to use the filter
The method looks like this javascript @Override public Map<String, AnalysisModule.AnalysisProvider<TokenFilterFactory>> getTokenFilters() { return Collections.singletonMap("jettro", JettroTokenFilterFactory::new); } The implementation of the factory is fairly basic javascript public class JettroTokenFilterFactory extends AbstractTokenFilterFactory { public JettroTokenFilterFactory(IndexSettings indexSettings, Environment environment, String name, Settings settings) { super(indexSettings, name, settings); } @Override public TokenStream create(TokenStream tokenStream) { return new JettroOnlyTokenFilter(tokenStream); } } The filter we are going to create has a bit strange functionality
It only accepts tokens that are the same as jettro
All other tokens are removed
javascript public class JettroOnlyTokenFilter extends FilteringTokenFilter { private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class); public JettroOnlyTokenFilter(TokenStream in) { super(in); } @Override protected boolean accept() throws IOException { return termAtt.toString().equals("jettro"); } } Time to test my fresh created filter
We can do that using the analyse endpoint curl -XGET 'localhost:9200/_analyze' -d ' { "tokenizer" : "standard", "filter" : ["jettro"], "text" : "this is a test for jettro" }' The response now is {"tokens":[{"token":"jettro","start_offset":19,"end_offset":25,"type":"","position":5}]} Concluding That is it, we have created the foundations to create a plugin, thanks to David Pilato, we have written our own _jettro endpoint and we have created a filter that only accepts one specific word, jettro
Ok, I agree the plugin in itself is not very useful, however the construction of the plugin is re-useable
Hope you like it and stay tuned for more elastic plugin blogs
We’re working on an extension to the synonyms plugin and have some ideas for other pluginsSometimes I need a small tool to perform a simple task for me
One recent example is a simple test client that sends a SOAP request that in an integrated environment would be sent by an external party
The client needs to send an ID and the outcome (accepted/rejected) of a request that was sent in a previous stage
Now, there are several ways to achieve this
You could write a full web-based GUI, or, on the other end of the spectrum, use a tool like Boomerang
But if you’re already using Maven, you should also consider using the Exec Maven plugin
The Exec plugin is an easy way to execute some code; it is lot faster to set up than a standalone application, but you are still able to access normal Maven dependencies
Setting it up You can use this in an existing project or create a new project
Since my client is basically test code, I created a new project that depends on the project that contains the application code
java <project> <modelVersion>4.0.0</modelVersion> <parent> <groupId>eu.luminis.mvnexec</groupId> <artifactId>parent</artifactId> <version>1.0.0</version> </parent> <artifactId>mvn-exec-example</artifactId> <name>Exec Maven plugin example</name> <dependencies> <dependency> <groupId>eu.luminis.mvnexec</groupId> <artifactId>services</artifactId> <version>1.0.0</version> </dependency> </dependencies> </project> Add an executable class to the project that performs the action you need
You can use the standard arguments array if you need paramaters
My class takes three parameters: a customer ID, the outcome, and the endpoint URL
It then performs a SOAP call and prints the results to the console
java package eu.luminis.mvnexec.client; import eu.luminis.mvnexec.services.*; import javax.xml.ws.BindingProvider; import java.math.BigInteger; public class SoapClient { private final LuminisService service; public static void main(final String[] args) { final String url = args[2]; final SoapClient client = new SoapClient(url); final ServiceRequest request = new ServiceRequest(); request.setCustomerId(BigInteger.valueOf(Long.parseLong(args[0]))); request.setOutcome(Outcome.valueOf(args[1])); final ServiceResponse response = client.updateCustomer(request); System.out.println("Status: " + response.getStatus()); System.out.println("Message: " + response.getMessage()); } public SoapClient(final String endpointUrl) { service = new LumninisService(); this.endpointUrl = endpointUrl; } public ServiceResponse updateCustomer(final ServiceRequest request) { return getPort().updateCustomer(request); } private LuminisPort getPort() { final LuminisPort port = service.getLuminisPort(); final BindingProvider bp = (BindingProvider) port; bp.getRequestContext().put(BindingProvider.ENDPOINT_ADDRESS_PROPERTY, endpointUrl); return port; } } Add the Exec plugin to your pom; if you want to use arguments, you must specify them in the configuration of the plugin
With the classpath tag you automatically add all project dependencies to the classpath
java <project> ..
<build> <plugins> <plugin> <groupId>org.codehaus.mojo</groupId> <artifactId>exec-maven-plugin</artifactId> <version>1.3.2</version> <executions> <execution> <goals> <goal>exec</goal> </goals> </execution> </executions> <configuration> <executable>java</executable> <arguments> <argument>-classpath</argument> <classpath /> <argument>eu.luminis.mvnexec.client.SoapClient</argument> <argument>${customerId}</argument> <argument>${outcome}</argument> <argument>${serviceUrl}</argument> </arguments> </configuration> </plugin> </plugins> </build> ..
</project> Running your code Run the code by calling the exec:exec goal and providing the parameters you defined in the pom
You must provide all parameters that you defined! java mvn exec:exec -DcustomerId=1234 -Doutcome=REJECTED -DserviceUrl=http://localhost:8080/services/soap If one of the parameters is (almost) always the same for the user, you could define the value as a property in the user's Maven settings file (user_home/.m2/settings.xml) so you don't have to provide it every time you execute the code
java <settings> <profiles> <profile> <id>inject-services-url</id> <properties> <serviceUrl>http://localhost:8080/services/soap</serviceUrl> </properties> </profile> </profiles> <activeProfiles> <activeProfile>inject-services-url</activeProfile> </activeProfiles> </settings> You can still override the property by providing it to the Exec goal as beforeFor a scrum master introducing or implementing agile in a team, should be a balance between implementing concepts and practices at a pace that does not overwhelm the team and demonstrating the benefits as early as possible to build momentum and motivate the team to do even more
Moreover, S/he should take it into consideration that each organisation has its own way of implementing agile
The work culture is totally different
The mentalities of individuals are different
Even the stability of the organisation/team plays a big role in how successfully you can implement agile
I started working at an organisation a year ago, where Agile was introduced 2 years back
I was selected for my technical knowledge and my experience as a Scrum master in another government organisation
Problems The team practiced its own version of scrum
It was still in the transition from waterfall to agile process
Few of the issues the team was facing were: Clash between IT and Business managers
Communication problems among team members and also with the Product owner
The Scrum master was already replaced twice in 6 months
The motive of the sprints seemed to be – Just get this done, with least effort
Dealing with legacy code
Sometimes, as old as 6 years
None of the sprints completed in last 10 months
Too many s in the team
Members leaving and joining
Analysis The team comprises of 6 people
It was a mix of developers, testers, business analyst and dev-ops
The developers and the dev-ops person report to the IT manager
The business analyst, the tester and the product owner report to a business manager
The vision was considered different
The team was tugged and pulled between the 2 managing bodies
This led to communication problems within the team and also with the product owner
It was not only limited to the communication but often led to heated discussions and arguments
No scrum master was able to contain the team and make them feel that they have a common goal
In the first retrospective meeting, I facilitated as a scrum master, I was baffled by the words that were thrown at each other
It was no simple task to bring this team together
The developers were not speaking their mind for various reasons
I understand, it could be organisational, personal or motivational
Since they did not speak in the beginning of a proposal, the goal was never reached, keeping the integrity of the system
It failed for different reasons
The scrum master was a business analyst(Not that I am against it)
The motive of the sprints seemed to be – Just do it, asap
Since the team members were not speaking their minds, the scrum master and the product owner never realised what is and what is not good for the team
Hence, the team members did not get the feeling of working towards a common goal, and were not motivated to do better
The team had gaps in the agile approach: The backlog was too volatile
The preference changed a lot
The team would refine the stories at lengths and in the next sprint, they might discover that the story is at the bottom of the priority list
By the time the team got to do it, the information was not only stale but forgotten too
It had to be refined again
The requirements/acceptance criteria of a story were not very well defined
Often the team members had to find out the problem with the users
This resulted in blind estimates and often under estimation
The stories were not picked up in priority
Team members picked up the stories they wanted to work on
There were more than 50% of the stories in progress
Reviews were done at the last moment, which delayed closing f the stories, which led to incomplete sprints
Approach In every team, there are introverts and extroverts
In meetings, the same people spoke up and the output was not well balanced
I wanted to get everyone’s opinion on what improvements they seek in themselves, others and also in the team before I started my job
I did one on one meetings with every member of the team, which helped me gain useful insight
Clash between IT and Business managers I scheduled a bi-weekly meeting with the Product owner(PO) to understand his expectations and share any concerns that team may have
Also, with the Product manager(PM), I scheduled monthly meetings for giving and receiving feedback on the performance of the team, and also, knowing the expectation from the IT management
There were a few movements in the organisation
A scrum coach joined to guide us to iron the details on the process
We were introduced to DIM session, where the team comes together along with the Po, to discuss issues
A few goals for the sprint were provided
Speed, quality, focus, motivation… The team collectively scored them as bad, ok or in good state
They would also pick a goal which matters to/bothers them most
The PO was also a part of the discussion and understand that some effort will be put into the area which team is bothered by
This helped us inform the PO and make him aware of our concerns about the quality of work we are delivering
He understood the issue team faced and agreed that we spend some time in cleaning the code we touch
Also, we slipped in improving test cases for the code, we work on
All members bought into it with time
And so far we have the freedom to add sub-tasks for cleaning up the code, improving what we can and also adding some test cases
Communication problems The DIM session helped bridge the gap in communication not only with the PO but within the team as well
We were working together to a common goal and looking out for each other
Fortunately, a third party came in for integrating with our services and adding value to our systems
I did not see a use case
Although all they asked for was some data for POC, which was not a lot of work, it wasn’t good for the motivation of the team
It was a decision thrust by the management
Not discussed with the team and didn’t look like our opinion mattered
I introduced “speaking up” and “standing your ground” to the team
Initially, I was all alone in the field, but with every meeting, more and more team members openly joined me to say “no” to what’s not good for the team
I was fortunate to be in the Scrum master position at that time, and be able to speak for the team
Also, relieved that it was the right decision to make
I would have introduced “accepting” as well to the team if I was proven wrong
Killing ego within the team is also a vital part of working together, in my opinion
Legacy and Motivation The event above, helped us gain our PO’s faith in us
We were given margin in estimates to improve code, add tests, and get rid of the legacy code
What had been a house of cards, started to turn into a stable wall
The production issues decreased
The re-work decreased
The team could work on new features
The team was sent department-wide kudos mails and the motivation rose up
Volatile Backlog The DIM session and our performance, both led us to communicate to the PO, that the backlog was too volatile
The stories are forgotten by the time we reach them and all our efforts turn into vain
As a result, the PO started fixing a sprint ahead, more or less
Also, we requested him to plan related stories in a sprint, so that we don’t have to work on a wide codebase
This would help the team to review each other’s work easily, share knowledge on the legacy code, and keep track of a minimum number of releases and still deliver value – lots of it, which he happily did
All communication and understanding reflected in the velocity of the sprint and in the quality we delivered
Working of the team Apart from the improvements done by the PO, the team had to put in effort in their working process as well
Slowly, with every retrospective more and more changes/adaptations were discussed with the team as improvement points
One improvement was a solution for a topic that the team was bothered with the most, that sprint
And the other was something related to the outcome for goal in the DIM session
A few improvements introduced in the team, over the period of 10 months were: The stories are to be picked up in priority
If one doesn’t have the knowledge, they were encouraged to peer program
No judgments were made
Each story should not be more than 50% of the total sprint goal
If it is so, it has to be broken down into smaller chunks
No more than 2 stories should be in progress at a given time, for the complete team of 3 developers and a tester
No more than 2 sub-tasks should be in the name of one person at a given instant
Before picking up a new sub task, the QA/review column should be checked
At least 2 people should review the code and put +1/+2 in the heading so that it is clear that it has been done
More to do There is still room for improvement
Apart from the agile processes, there are some problems which are hard to handle
There is instability in the organisation, namely, lay offs, team shuffling, other team changes
Every time the team is changed, the team goes back to forming stage of the agile process
It has been difficult to manage
It is difficult for a team to go through the process and achieve performing stage, every 3 months
Outcome The team has been showing continuous improvement with a rise in velocity
The PO is a happy PO
The PM even congratulated the team to have turned him around
There is a sense of working together, instead of against each other
The team is motivated and aims high
Individuals also seek growth and work also on their own skills
Both the business and IT management is very happy with the team performance
Impediment list is getting shorter by the day
The team is almost self-organising and managing
It is easier to gel new team members into the team when the team is clear about it’s working standards
Reflections Every situation is different so of course there is no “one way” to begin or improve on agile practices
The team should be flexible enough to try out practices and see what suits them best
Occasionally, the team would fall back into old habits
It is the job of scrum master to adjust his approach and choose to remind the team and adjust the ways to support the team
Note:-Every time a team member goes to a meeting on a wider platform with other teams, they always come back with a smile and proudly declare – Our team is really doing good
It’s the best! That’s what a scrum master always wants to hearOver the last few years I have been getting more and more fed up with some so-called Agile evangelists
Somehow making an Agile organization is the magic to curing all problems
Agile is so great that it seems to be a goal in and of itself
Let me put it bluntly: I think that is total rubbish
Do not get me wrong, I love almost everything about Agile
I’ve been an Agile coach for most of the last decade and helped many organizations make the transition
And I agree that it works
But let’s not kid ourselves, Agile is not the goal, it is a way of solving business problems
It is a process improvement that should prove it works, and if not it needs to be adjusted or just thrown out completely
Organizations invest in lots of things ranging from new factories and marketing campaigns to coaching from people like me
For a marketing campaign it is totally logical to measure its effectiveness and take decisions as to its future based on that
If a factory creates great products and everybody there is happy, but we lose a lot of money on it every day, guess how long it stays open
So why should Agile be any different
Agile, while being really great should be treated no different
It should prove that it is a success; not only based on believe but in cold, hard, figures
I had a conversation a while back with a fellow Agile Coach
He had been engaged in an Agile transformation that got cancelled by management
During the discussion the reasons became clear; while he thought things were going fine (the teams were adopting Scrum just fine, the process was in place, people were reasonably happy), management was not convinced and decided to stop investing in what they perceived to be a money pit
The failure here was not in Agile itself, but in failing to establish what success means, failing to measure, and failing to show management in their terms that things are working
If you don’t do that things will fall apart quickly, as it did in his case
To avoid problems like this starting out thinking about how results will be proven and presented is crucial
We need to define what a success means for a specific situation
What problem are we trying to solve
How can we measure the current state and can we measure how things are progressing as we go
How can we present results in a clear and concise manner that is easy to understand
In short: Agile is great, and I firmly believe it works in almost any setting
But if you cannot prove it does, your Agile transformation will, and should, be cancelledIn my previous blog post, I tried to give some intuition on what neural networks do
I explained that when given the right features, the neural network can generalize and identify regions of the same class in the feature space
The feature space consisted of only 2 dimensions so that it could be easily visualized
In this post, I want to look into a more practical problem of text classification
Specifically, I will use the Reuters 21578 news article dataset
I will describe a classification algorithm for this dataset that will utilize a novel feature extraction algorithm for text called doc2vec
I will also make the point that because we use machine learning, which means the machine will do most of the work, the same algorithms can be used on any kind of text data and not just news articles
The algorithm will not contain any business logic that is specific to news articles
Especially the neural network is a very reusable part
In machine learning theorem a neural net is known as a universal approximator
That means that it can be used to approximate many interesting functions
In practical terms, it means you can use the same neural network architecture for image data, text data, audio data and much more
So trying to understand one application of a neural network can help you understand much more machine learning applications
Training the Doc2vec model In the previous post, I explained how important it is to select the right features
Doc2vec is an algorithm that extracts features from text documents
As the name implies it converts documents to vectors
How exactly it does that is beyond the scope of this blog (do see the paper at: https://arxiv.org/pdf/1405.4053v2.pdf) but its interface is pretty simple
Below is the python code to create vectors from a collection of documents: python # Load the reuters news articles and convert them to TaggedDocuments taggedDocuments = [TaggedDocument(words=word_tokenize(reuters.raw(fileId)), tags=[i]) for i, fileId in enumerate(reuters.fileids())] # Create and train the doc2vec model doc2vec = Doc2Vec(size=doc2vec_dimensions, min_count=2, iter=10, workers=12) # Build the word2vec model from the corpus doc2vec.build_vocab(documents) # Build the doc2vec model from the corpus doc2vec.train(document (for the complete script see: reuters-doc2vec-train.py) To get some intuition on what doc2vec does let’s convert some documents to vectors and look at their properties
The following code will convert documents from the topic jobs and documents from the topic trade to document vectors
With the help of dimensionality reduction tools (PCA and TSNE) we can reduce these high dimensional vectors to 2 dimensions
See scripts/doc2vec-news-article-plot.py for the code
These tools work in such a way that coordinates in the high dimensional space that are far apart are also far apart in the 2-dimensional space and vice versa for coordinates that are near each other
(see the source code at: doc2vec-news-article-plot.py) What you see here are the document classes, red for the “job” topic documents and blue for the “trade” topic documents
You can easily see that there are definitely regions with more red than blue dots
By doing this we can get some intuition that the features we selected can be used to make a distinction between these 2 classes
Keep in mind that the classifier can use the high dimensional features which probably show a better distinction than this 2-dimensional plot
Another thing we can do is calculate the similarity between 2 doc vectors (see the similarity function of doc2vec for that: gensim.models.doc2vec.DocvecsArray#similarity)
If I pick 50 job vectors their average similarity to each other is 0.16
The average similarity between 50 trade vectors is 0.13 If we now look at what the average similarity between 50 job vectors and 50 trade vectors we get a lower number: 0.02
We see that the trade vectors are farther apart from the job vectors than that they are from each other
We get some more intuition that our vectors contain information about the content of the news article
There is also a function that given some example vectors finds the top n similar documents, see gensim.models.doc2vec.DocvecsArray#most_similar
This can also be useful to see if your trained doc2vec model can distinguish between classes
Given a news article, we expect to find more news articles of the same topic nearby
Training the classifier Now that we have a trained doc2vec model that can create a document vector given some text we can use that vector to train a neural network in recognizing the class of a vector
Important to understand is that the doc2vec algorithm is an unsupervised algorithm
During training, we didn’t give it any information about the topic of the news article
We just gave it the raw text of the news article
The models we create during the training phase will be stored and will later be used in the prediction phase
Schematically our algorithm looks like this (for the training phase): For the classifier, we will use a neural network that will train on all the articles in the training set (the reuters dataset is split up in a training and test set, the test set will later be used to validate the accuracy of the classifier)
The code for the classifier looks like this: python model = Sequential() model.add(Dense(input_dim=doc2vec_dimensions, output_dim=500, activation='relu')) model.add(Dropout(0.3)) model.add(Dense(output_dim=1200, activation='relu')) model.add(Dropout(0.3)) model.add(Dense(output_dim=400, activation='relu')) model.add(Dropout(0.3)) model.add(Dense(output_dim=600, activation='relu')) model.add(Dropout(0.3)) model.add(Dense(output_dim=train_labels.shape[1], activation='sigmoid')) model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy']) (for the complete script see: reuters-classifier-train.py) This code will build a neural network with 3 hidden layers
For each topic in the Reuters dataset, there will be an output neuron returning a value between 0 and 1 for the probability that the news article is about the topic
Keep in mind that a news article can have several topics
So the classifier can indicate the probability of more than 1 topic at once
This is achieved by using the binary_crossentropy loss function
Schematically the neural network will look like this: Given a doc vector, the neural network will give a prediction between 0 and 1 for each topic
After the training phase both the model of the doc2vec algorithm and for the neural network will be stored so that they later can be used for the prediction phase
Dimensionality When using a neural network it’s important not to have too few dimensions or too many
If the number of dimensions is too low the coordinates in the feature space will end up too close to each other which makes it hard to distinguish them from each other
Too many dimensions will cause the feature space to be too large, the neural network will have problems to relate data points of the same class
Doc2vec is a great tool that will create vectors that are not that large, the default being 300 dimensions
In the Doc2vec paper, it’s mentioned that this is the main advantage over techniques that create a dimension for every unique word in the text
This will create in the 10s or 100s thousand dimensions
Prediction phase For the prediction phase, we load the trained doc2vec model and the trained classifier model
When we feed the algorithm the text of a news article the doc2vec algorithm will convert it to a doc vector and based on that the classifier will predict a topic
During the training phase, I withheld a small set of news articles from the training of the classifier
We can use that set to evaluate the accuracy of the predictions by comparing the predicted topic with the actual topic
Here are some predicted topics next to their actual topics: title: AUSTRALIAN FOREIGN SHIP BAN ENDS BUT NSW PORTS HIT predicted: [‘ship’] – actual: [‘ship’] title: INDONESIAN COMMODITY EXCHANGE MAY EXPAND predicted: [‘coffee’] – actual: [‘coffee’, ‘lumber’, ‘palm-oil’, ‘rubber’, ‘veg-oil’] title: SRI LANKA GETS USDA APPROVAL FOR WHEAT PRICE predicted: [‘grain’, ‘wheat’] – actual: [‘grain’, ‘wheat’] title: WESTERN MINING TO OPEN NEW GOLD MINE IN AUSTRALIA predicted: [] – actual: [‘gold’] title: SUMITOMO BANK AIMS AT QUICK RECOVERY FROM MERGER predicted: [‘earn’] – actual: [‘acq’] title: SUBROTO SAYS INDONESIA SUPPORTS TIN PACT EXTENSION predicted: [‘acq’] – actual: [‘tin’] title: BUNDESBANK ALLOCATES 6.1 BILLION MARKS IN TENDER predicted: [‘interest’, ‘money-fx’] – actual: [‘interest’, ‘money-fx’] Conclusion Hopefully, by now I’ve given some intuition on what machine learning is
First, your data needs to be converted to meaningful feature vectors with just the right amount of dimensions
You can verify the contents of your feature vectors by: Reducing them to 2 dimensions and plot them on a graph to see if similar things end up near each other Given a datapoint, find the closest other data points and see if they are similar You need to divide your dataset in a training and a test set
Then you can train your classifier on the training set and verify it against the test set
While this is a process that takes a while to understand and getting used to, the very interesting thing is that this algorithm can be used for a lot of different use cases
This algorithm describes classifying news articles
I’ve found that using exactly the same algorithm on other kinds of predictions, sentiment prediction for example, works exactly the same
It’s just a matter of swapping out the topics with positive or negative sentiment
I’ve used the algorithm on other kinds of text documents: user reviews, product descriptions and medical data
The interesting thing is that the code changes required to apply these algorithms on other domains are minimal, you can use exactly the same algorithms
This is because the machine itself learns the business logic
Because of that the code doesn’t need to change
Understanding the described algorithm is not just learning a way to predict the topic of a news article
Basically, it can predict anything based on text data as long as you have examples
For me as software engineer, this is quite surprising
Usually, code is specific to an application and cannot be reused in another application
With machine learning, I can make software that can be applied in multiple very different domains
Very cool! Source code https://github.com/luminis-ams/blog-text-classification Further reading For more practical tips on machine learning see the paper “A Few Useful Things to Know about Machine Learning” at: https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf About doc2vec: https://radimrehurek.com/gensim/models/doc2vec.html https://deeplearning4j.org/word2vecCryptography is often shrouded like it is a black art
While you should stay away from implementing your own crypto, there’s plenty of components available in the Java ecosystem that can help you
But, what are they, and what to all those terms mean
In this session, we will unpack the jargon and show you different types of cryptography
We will visit some of the common pitfalls, talk about relative performance, and take a peek at new developments such as attribute based encryption
To top it all off, we go into the core Java APIs that available to help you
You will leave this session with a newfound appreciation of the tools available, and have a better understanding of which parts are too hot too handleAs I wrote before, Agile needs to prove its worth just like any other business improvement effort
However, that is easily said and harder to actually implement
What does it mean that it “works”, that there are “actual benefits”
This is a hard question to answer, however, I will give it a try
Traditionally there are two ways of dealing with Agile improvements
The first is to just ignore measuring at all
Agile is great in and of itself, and is a goal by itself
If Agile works than you have success
Needless to say this works for some companies but not for most others
The second approach is to start measuring in detail, and there are dozens of possible metrics to track
The issue then becomes how you can actually judge based on these metrics whether things got better, and how you can possibly keep track of them
This approach works in principle, but it is hard for busy managers to deal with
And ultimately, who cares whether your CPI, focus factor, or any of the other metrics changed
As a starting Agile consultant, I had the pleasure to work with one of the geniuses of our business, Ian Spence
He thought me a great way of dealing with this problem that I still use at each and every company work with
The solution begins with remember the old saying “You can get better, faster, or cheaper, you can even have two out of these but not three”
Well, actually that is not true
It is true if you never improve
But in an Agile implementation we are trying to improve
In this case, you can do even better
You can get Better, Faster, Cheaper and in addition get Happier
We then use these categories to judge how successful the Agile implementation is
To summarise, the Better, Faster, Cheaper, Happier categorisation is as shown here
What I do is the following: 1
Together with the client decide on what the current problem is 2
Define what measurements (metrics) could be used to track improvements 3
Assign each metric to one of the 4 categories 4
Make sure you have at least one, but preferably a few, metrics per category 5
Define how you can get a single number out of the metrics for each category 6
Create a dashboard that shows the values of each category over time Needless to say, step 5 is where the complication sets in, but it is manageable
What you end up with is something like this: The goal is to give an easily judged overview of where things stand at any given time
The management that you are going to be dealing with will not care about all of the underlying metrics, they just want to see whether or not things are working
In this way, we create transparency and we can show that the real investment made in an Agile transformation actually leads to improvements over time
And if it shows we are not getting improvements, then maybe we should change course, or even stop completely
That is a perfectly valid outcome, although if you start really measuring and acting on the results that will rarely, if ever, happenWhat is Xamarin
Xamarin is a platform for developing fully native applications for iOS, Android and Windows, with a shared code base
Xamarin apps are written in C# and can make use of most .NET libraries
Why Xamarin
Xamarin apps are native so the performance is great
Everything that can be done with native app development can be done with Xamarin
All native Android, iOS and Windows API’s are available
The UI is native
Xamarin Android apps use the same design guide lines as other Android apps
This is important because the user will have a similar user experience with your app as with other apps
Same goes for iOS and Windows apps
You only need to know or learn one language to develop for all three platforms
Some background information Some of you might be confused now because .NET only runs on Windows right
No that’s not completely true so I want to start with some background information about CLI and Mono
In 2000 Microsoft released the Common Language Infrastructure (CLI)
This is an open specification and is standardized by ISO and ECMA
The CLI describes how high-level language applications can run on different platforms without changing the code
There are a few implementations of the CLI like .NET Framework, .NET Core, Mono and more
The .NET Framework is the most used and most well-known implementation which only runs on Windows
Mono is a CLI implementation that runs on Windows, Linux, Mac and even Embedded systems
Mono makes it possible to write applications with WinForms, WCF, ADO.NET, Entity Framework and more in languages like C#, F#, VB.NET, Python and more
Mono is also available for iOS as Xamarin.iOS, previously named MonoTouch and Xamarin Android, previously named Mono for Android and MonoDroid
How does Xamarin work
When you scaffold a Xamarin app you get multiple projects
The number of projects you get depends on which platforms you want to support
The first project is a class library which contains all the code shared between all the supported platforms
You also have one project for each platform you want to support
Those projects contain all of the platform specific code
Xamarin contains binding for the iOS and Android SDKs
So Xamarin code looks pretty much like the code you would write when developing a native app
I’ve written a very simple Android example to show you what it looks like
It’s an app which fetches some information about countries from a webservice and show them in a list
It also has a switch
If the switch is turned on it shows the country codes and when turned off the app won’t show the country codes
The code should look very familiar to Android developers
code namespace XamarinCountries { [Activity(Label = "XamarinCountries", MainLauncher = true, Icon = "@drawable/icon")] public class MainActivity : Activity { private IEnumerable<Country> _countries; private Switch _countrySwitch; private ListView _countryList; protected override void OnCreate(Bundle bundle) { base.OnCreate(bundle); SetContentView(Resource.Layout.Main); _countryList = FindViewById<ListView>(Resource.Id.listCountries); _countrySwitch = FindViewById<Switch>(Resource.Id.swCountryCode); FetchCountries(); RedrawList(); _countrySwitch.Click += (sender, args) => { RedrawList(); }; } protected void RedrawList() { string[] items = new string[] { "No countries available" }; if (_countries != null && _countries.Count() > 0) { items = _countries.Select(c => c.Name + (_countrySwitch.Checked 
" - " + c.CountryCode : "")).ToArray(); } _countryList.Adapter = new ArrayAdapter<string>(this, Android.Resource.Layout.SimpleListItem1, items); } protected void FetchCountries() { CountryRepository repo = new CountryRepository(); this._countries = repo.GetCountries(); } } } code <?xml version="1.0" encoding="utf-8"?> <LinearLayout xmlns:android="http://schemas.android.com/apk/res/android" android:orientation="vertical" android:layout_width="match_parent" android:layout_height="match_parent" android:minWidth="25px" android:minHeight="25px"> <Switch android:layout_width="match_parent" android:layout_height="wrap_content" android:id="@+id/swCountryCode" android:text="Show country code" /> <ListView android:minWidth="25px" android:minHeight="25px" android:layout_width="match_parent" android:layout_height="match_parent" android:id="@+id/listCountries" /> </LinearLayout> But how about Windows
There is actually no such thing as Xamarin.Windows
Windows apps are developed the same way as you would normally develop native Windows apps
The only difference is that the app will use the shared code library which is also used by the Android and iOS apps
What do you need to get started with Xamarin
To develop with Xamarin you can use Windows or OS X
There are two IDEs available for Xamarin
The first one is Visual Studio which is only available for Windows and the second one is Xamarin Studio which is available for Mac and Windows
For developing Android and iOS apps you can use both Mac and Windows
But if you want to use Windows you still need a Mac for the iOS app because Xamarin relies on the iOS build tools which are only available for Mac
You’ll have to use the Mac as a build agent
You can connect the app by using a wizard in Visual Studio (see picture)
The Windows pc will send the code to the Mac and the Mac will create the iOS packages and will run the app on a simulator on the Mac, or an iOS device connected to the Mac
Developing Windows apps is only possible on Windows
If you still want to work on a Mac you can develop Android, iOS and shared code on the Mac and use source control to continue working on your Windows app on Windows
Click here for more info on this
Later, I will write a blogpost about Xamarin forms, a toolkit to create a shared UI for all three platforms and more information about the Xamarin platform
Sources: https://www.xamarin.com/ https://www.pluralsight.com/blog/tutorials/xamarin-webcast-qaSince Angular 2 became final, the final version of Ionic 2 comes in sight, which is heavily based on Angular 2
And yes, Ionic announced its first Release Candidate for version 2! This post will introduce you to version 2 of the Ionic framework, and help you setting up a simple App
What is Ionic “Ionic is an HTML5 mobile app development framework targeted at building hybrid mobile apps
Hybrid apps are essentially small websites running in a browser shell in an app that have access to the native platform layer
Unlike a responsive framework, Ionic comes with very native-styled mobile UI elements and layouts that you’d get with a native SDK on iOS or Android but didn’t really exist before on the web
Ionic also gives you some opinionated but powerful ways to build mobile applications that eclipse existing HTML5 development frameworks.” from: ionicframework.com
Why Ionic So there’re already several frameworks out there that seems to do the job
We have Angular as a proper front-end JavaScript framework and Cordova to build hybrid apps in HTML5
Shortly, Ionic combines these two and adds some native look and feel to it
So when you want more than just running your web app in a mobile browser, try Ionic
To see it in perspective, this is how it comes together: Preparation Before we can use Ionic, we need to install Ionic CLI, npm and Cordova
Follow the instructions on Ionic’s documentation page to install Ionic and its dependencies
As IDE I recommend using Visual Studio Code
It is free of charge, runs on OSX/Windows/Linux and supports JavaScript, TypeScript, HTML, CSS, and more out of the box
But you can use any other IDE or (text) editor if you prefer
Now you’ve installed the necessary software we can create the App
Open a command prompt or Terminal and run the command: javascript ionic start MyFirstApp blank --v2 This will pull down Ionic 2, install npm modules for the application, and get Cordova set up and ready to go
We pass ‘blank’ as argument to start with an empty project
It’s also possible to create an App that already contains a sidemenu, tab buttons or based on an existing project, see the CLI documentation
Now start the app with the following two commands: javascript cd MyFirstApp ionic serve When finished it will open a browser and load the App
You will see the default homepage saying: “The world is your oyster”
Change the homepage The folder MyFirstApp contains a minimal working Ionic 2 project
For now the most important part is the src/ folder
The src/ folder contains the raw, uncompiled code
When we run ionic serve, the TypeScript code is transpiled into JavaScript and the SASS files into one CSS file
The src/ folder looks like this: javascript src/ app/ assets/ pages/ home/ home.html home.scss home.ts theme/ index.html We will focus on the folder src/pages/home/
Start Visual Studio Code and open the folder MyFirstApp
Now open home.html and see: javascript <ion-header> <ion-navbar> <ion-title> Ionic Blank </ion-title> </ion-navbar> </ion-header> <ion-content padding> The world is your oyster
<p> If you get lost, the <a href="http://ionicframework.com/docs/v2">docs</a> will be your guide
</p> </ion-content> Replace everything inside <ion-content> tags with Hello world
When you save, you’ll see in the Terminal something has changed, and the app in the browser will be refreshed and now shows the text ‘Hello world
Display a list of items We will create a page that shows a list of commits from Github on the Ionic2 project
We use Github because it has a simple and public available REST API
Before fetching real data from Github, first create the list view
Open home.ts and add a field member to hold the list of items we’re going to show
So after export class HomePage { add: javascript commits: Array<any> = ['Commit 1', 'Commit 2']; Open home.html and replace the body of <ion-content> with: javascript <ion-list> <ion-item *ngFor="let c of commits"> {{c}} </ion-item> </ion-list> Now the App will look like this: Now let’s fetch some real data from Github
The REST endpoint we are going to use is: https://api.github.com/repos/driftyco/ionic/commits
To find out what the result looks like just open this URL in a browser
For more details consult the Github API doc
To fetch data we need to import the Http class of Angular
Open home.ts and add at the first line: javascript import { Http } from '@angular/http'; import 'rxjs/add/operator/map'; These are two imports, the first imports the Http and the second one I will come back to later on
Remove the hardcoded commits, e.g
replace line 11 with: commits: Array<any>; In the constructor (line 12) add an extra parameter: private http: Http so that the new constructor looks like: javascript constructor(public navCtrl: NavController, private http: Http) { This makes http available as member of the HomePage class
When the HomePage is loaded, Angular will call the ngOnInit() method, this is the right moment to implement the HTTP call to Github
So below the constructor, add the following method (e.g
after line 15): javascript ngOnInit() { this.http.get('https://api.github.com/repos/driftyco/ionic/commits') .map(response => response.json()) .subscribe(commits => { this.commits = commits; }); } Let’s take a look, line by line at what we just added
http.get() is performing a GET request to the given URL and its return value is an Observable object
Details about Observables are out of scope for this post
This would make a good topic for a next post though
For now assume that it’s an Object that emits a stream of events, and we can apply functions to that stream
In this example it will emit one event, containing the HTTP Response from Github
The body of the response contains a JSON string, so first we’ll convert it to a JSON object with the map() function
This is why we added the second import
When you omit this import you’ll get an compile error stating: “Property ‘map’ does not exist on type ‘Observable<Response>’.”
Finally we listen to the events with subscribe()
The JSON Response is a list of commits, and we choose to directly assign the JSON object to this.commits When you open the App in the browser you’ll see a list of [Object, Object]
That’s because in home.html we print each element in the list of commits with: {{c}}, and those elements are Objects
Let’s change the markup to show a few details of a commit
Replace {{c}} with: javascript <ion-avatar item-left> <img src="{{c.author.avatar_url}}"> </ion-avatar> <h2>{{c.commit.committer.name}}</h2> <h3>{{c.commit.committer.date}}</h3> <p>{{c.commit.message}}</p> Now you’ll see the list of commits and the avatar of their author like: Maybe you notice that the date is not nicely readable, because we directly displayed the value and that is ISO 8601 formatted
To make it better readable we add a Pipe
Pipes are a common way in Angular to write display values
Say that we only want to see the day, month, hours and minutes, we can add the DatePipe: | date:' dd MMM HH:mm'
So the line writing the date value will be: html <h3>{{c.commit.committer.date | date:' dd MMM HH:mm'}}</h3> We started with generating a blank Ionic app
And now it displays a list of commits, fetched from Github
The full project of the app we created can be found on Github
One last command you really need to know is: javascript ionic serve --lab This displays multiple platforms next to each other, so that you see Android, iOS and Windows at once
It comes in handy during development to run the App in a browser
But when you want to make use of native SDK like the camera or GPS, you need to ‘build’ the app for a specific platform
With one command you can ‘build’ the app which can be installed on a real device or in an emulator
How this exactly works is out of scope for this post
So this was a brief introduction into Ionic 2
When you start working with Ionic 2, you probably want to learn more about the concepts behind Angular 2 and Ionic 2
Also the Ionic website contains tutorials and documentationA couple of years ago I started to pursue my interest in machine learning
At that time, machine learning was more an academic area of computing
It was hard to put in practice
In this article, I want to show you that this has changed over the last year
Machine learning frameworks and libraries are improving and are becoming more practical
There seems to be an effort to put the newest machine learning algorithms in the hands of software developers to put them into practical use
Let us first have a look at why machine learning (ML) is so interesting for software developers
I think ML makes things possible that were previously impossible
This is something your shiny new functional programming language or framework will not do for you
Machine learning can solve problems that a procedural programming language can never solve for you
For example, let’s suppose I want to classify whether an animal is a cat or a dog
Can you give the business rules that separates a cat from a dog
Think about it for a minute or so
They both have four legs, they have hair, they have two eyes
Can you give a rule that is always true for a cat and always false for a dog
But still when I show you a cat or dog you immediately know which one it is
In the real world, it is hard to let a procedural program make this kind of decisions
As software engineers, we devise models of the real world which capture the information
We let humans make the conversion from the analog to digital world
When you make an appointment with your veterinarian you tell the vet what kind of animal you have and the vet will fill in the form, with the correct animal, in the appointment system
This how information is transformed from the analog world to the digital world
This is how traditional software is made
With machine learning, this has changed
A machine can tell from a photograph if your pet is a cat or dog
As developers, we have learned to avoid solving these kinds of problems
When a business expert cannot explain to us how he or she makes a decision (in other words the business rules) we cannot make software for it
But when applying machine learning we can
This opens up new opportunities
So how does this work
How can a machine learn do the right thing without explicitly telling it what to do
The answer, give it examples of previous cases with the right labels attached
In machine learning, this is known as supervised learning
In our case, give it lots of examples of photos of cats and dogs with the correct label attached
The machine will generalize from the examples and when an unlabeled example is entered give back the correct label
Intuition on neural networks Below I want to give some intuition on what neural networks do
It is by no means a complete explanation (for that I can recommend the Machine Learning course at Coursera or the website neuralnetworksanddeeplearning.com)
In order for a machine to learn from examples, it needs to be able to relate these examples
At http://playground.tensorflow.org they make a great visualization of this
What you see here is a dataset for a binary classification problem
The classes are represented by the colors yellow and blue
This dataset is two-dimensional
Each data point has two features, an x and a y coordinate
To go back to our example of classifying cats and dogs, the x feature could be the height of the animal and the y feature could be the weight of the animal (probably not very useful features to distinguish cats from dogs though)
Based on those features the neural network will learn the classes (yellow or blue) and later for an unknown data point predict the class
All the features of a single data point can also be called a vector
A vector is an array of all the features of a single data point
When you let the neural network run you see that the predictions of the neural network change, this is represented by the colors on the background
The neural network deduces the dominant class at each position in the feature space
This is done by looking at the classes of the surrounding data points
If you would give the neural network an unlabeled data point that would end up in the top left corner of the feature space it would predict the yellow class
Usually, your dataset will have more than two dimensions/features
But the same principles will apply, it’s just a lot harder to visualize 🙂 In this case, the neural network can easily generalize and separate the blue from the yellow data points
Hopefully, you can see that feeding it a data set where the blue and yellow classes are spread randomly across the feature space it would be hard for it to generalize
It’s important that the features you select for your dataset can be used to generalize on
To go back to our previous example of cats and dogs
If your feature is the count of the number of paws for the animal, no neural network can figure out for a new example if it’s a cat or a dog
One very simple technique you can try is given a datapoint of a known class see if datapoints of that same class are nearby
Furthermore, samples of other classes should be further away
This is not a definitive rule, there could be multiple clusters of the same class
Conclusion So what does a neural network do
It uses examples (encoded in vectors) with labels attached (in this case two colors) and generalizes on them so that it can make a prediction for an unlabelled data point
In the next article, I want to introduce an algorithm that can vectorize text documents
I will also make a classifier using these text vectors as input for a neural network
The classifier will be trained on the Reuters 21578 news article datasetThe guys at elastic have been working on the new 5.0 release of elastic and all the other products in their stack as well
From the first alpha release I have been playing around with new features
Wrote some blogposts about features I played around with
With release candidate 1 out, it is time to write a bit about the new features that I like, and (breaking) changes that I feel are important
Since it is a big release I need a big blog post, so don’t say I did not warn you
Important additions Let us first have a look at some of the new features
Ingest Ingest is new type of elastic node
An ingest node is meant as a processor for incoming documents
Is shares some functionality with logstash, but is lightweight, integrated with elastic and very easy to use
A few examples of what you can do with ingest are: Converting strings to other types: integer, float, string, boolean Date parsing Use a data field to store the document in the right time based index
Think of an index by day or month
Use Grok to parse text based messages like access logs and application logs
Typical use cases are: Sending logs from your servers through filebeat to elasticsearch and parse the logs using grok patterns
Sending events from your application immediately to elasticsearch but parse them before storing them
Parsing csv files by sending them line by line
The base for ingest is a pipeline
You can define multiple pipelines and when sending a document to elastic you can specify the pipeline to use
Each pipeline consists of a description and a number of processors that are executed in order
Pipelines are interacted with using the pipeline API
A good addition to this API is the Simulate option
With this functionality you can test your pipeline
You can even test a pipeline before you insert it into elastic
The processors can change messages, add fields, combine fields, remove fields, etc
They also have error handling covered
If one processor fails, another one can take over or the chain can continue if nothing happened
There is a list of available processors, next to that you can also create your own processors
reference Matrix stats aggregation If you are into mathamatics and regularly need to do advanced calculations over fields
You have to have a look at the matrix stats aggregation
With this aggregation you can determine things like: mean, variance, skewness, kurtosis, covariance and correlation
reference Search after Is new functionality to overcome a performance problem with using from/size when doing deep pagination
Nowadays there is even a soft limit of default 10000 records to use with from/size
If you move beyond that amount you get an exeption
With search_after this problem can be overcome
The idea is to use the results from the previous page to help the retrieval of the next page
The query needs to have a sort part, with as a final part the unique field to guarentee there is an order of documents
In the response for every document, there is next to the _source also a sort part
The contents of this sort part can be used in the next request
First have a look at the query and the response
Notice the sort part with a sort by createDate and _uid
java GET raw_events-*/_search { "query": { "term": { "name": { "value": "search_click_job" } } }, "sort": [ { "createDate": { "order": "desc" } }, { "_uid": { "order":"desc" } } ] } The next code block shows one hit of the response
java { "_index": "raw_events-2016.10.18", "_type": "logs", "_id": "AVfYOvw9lbxWedBmE49G", "_score": null, "_source": { "jobId": "1114", "@timestamp": "2016-10-18T14:38:46.366Z", "name": "search_click_job", "@version": "1", "userAgent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36", "locale": "nl_NL", "createDate": "2016-10-18T14:38:46.366Z", "visitorId": "87F95035-2674-EC2D-CAB5-B7CC4494DE50" }, "sort": [ 1476801526366, "logs#AVfYOvw9lbxWedBmE49G" ] } Notice the values for sort in the response
We need to use these values in the next request where we add the search_after element
java GET raw_events-*/_search { "query": {"term": {"name": {"value": "search_click_job"}}}, "sort": [{"createDate": {"order": "desc"}},{"_uid": {"order":"desc"}}], "search_after": [1476801526366 , "logs#AVfYOvw9lbxWedBmE49G"] } Now you have a more efficient mechanism to implement next page functionality
Beware though, it does not help you with get me page 20 functionality
reference Task Manager The task manager has become the central point to monitor all long running tasks
Tasks like creating snapshots and re-indexing can now be monitored using the _tasks endpoint
You can also filter the tasks based on groups
Next to that you can cancel tasks
This api is already available in 2.4 New in 5 is the integration in the cat api with _cat/tasks reference Percolator The percolator is sort of an inverse search
You store the query and when searcing use a document as the input to find matching queries
This technology has changed a lot in 5.0
It is moved from being a separate API with it’s own endpoint, to be part of the search API as a special percolate query
I wrote a longer blogpost about this topic, so if you want more information, read it here
Blogpost reference Painless There is a new scripting language in town
I must (almost) admit I never really use scripting with elastic
There have been a lot of issues with the security of scripts
Therefore elastic has made a lot of changes, they went from support all, to support sandboxed only (groovy)
Now they have implemented a new scripting language that should overcome a lot of the problems that the other scripting languages have
The performance of the scripts should be better, it should be easy to learn and as the syntax is similar to small groocy scripts migration should not be to hard
If you want more painless info, check the docs
reference Rollover endpoint This is an interesting new feature with which you can control the index that you write to does not become to big or to old
The idea is that if the index does become to big, a new index is created and the alias that first pointed to the first index now points to the newly created index
Documents are written using the alias, that way the document is inserted into the correct index
In the example code I use a number in the index, but you can also use dates
Check the docs for more info
First create the first index to store events and create an alias write_events to point to that index
Than add a few documents
java PUT events-000001 { "settings": { "number_of_shards": 1, "number_of_replicas": 0 }, "aliases": { "write_events": {} } } PUT write_events/error/1 {"message": "First event"} PUT write_events/error/2 {"message": "Second event"} PUT write_events/error/3 {"message": "Third event"} In then next code block I show the indexes that are available java GET _cat/indices/events-* green open events-000001 hB9RfDUJQtaUjysDtQyisA 1 0 3 0 9.6kb 9.6kb Doing the same call for aliases gives us
java GET _cat/aliases write_events events-000001 - - - Next we can ask for a rollover if the index contains more than 2 docs with the following request
java POST write_events/_rollover { "conditions": { "max_docs": 2 } } Asking for the indexes again gives you the following output java green open events-000001 hB9RfDUJQtaUjysDtQyisA 1 0 3 0 9.6kb 9.6kb yellow open events-000002 XsmOJN90RLmIyfcQF-fApg 5 1 0 0 650b 650b Beware that you need to create an index template with the mapping and settings you need
Without it, defaults are used
In my example for instance, the second index now has 5 shards instead of one which I created the first index with
Now asking for aliases gives us: java GET _cat/aliases write_events events-000002 - - - Notice the difference in the name of the index the alias points to
What I thought in the beginning was that elasticsearch would check for the rollover condition on each addition of a document
But now I realise how sub optimal that would be
So you have to embed this into your own process
Important changes Default memory usage The default memory usage now is min and max 2G BM25 The default similarity algorithm is now changed to BM25, so no longer the TD/IDF variant
This has a potential impact on the scoring of your documents
So beware
The old is still be available as the classic algorithm
Completion suggester The completion suggester is changed completely and should be re-evaluated when you depend upon it
Norms Deal with boosting and scoring fields
If you use a field solely for filtering and aggregations you should disable norms
Refresh on GET If a document has changed and this change has not been sent to Lucene yet (by a refresh), a refresh will be executed before executing the GET
You can disable this using realtime=false index defaults You can no longer set index level defaults in the elasticsearch.yml file or using the command line
You should use index templates instead
Examples of these kind of properties are number_of_shards and number_of_replicas
Java 8 Elasticsearch 5 requires java 8 Refresh options Refresh now has three different options: None, Immediate and Wait for
Obtaining all values in terms aggregation I saw customers using the size:0 in aggregations to obtain all items
This is no longer possible
You have to provide a non null value for the size
The final part of the blog is about the most important breaking changes Breaking changes When upgrading your cluster, this is most likely the most important part to read
There is extensive documentation on breaking changes that you should read
This part is just about the things that are important to me
Indexes created in 1.x cannot be migrated to 5.x
So you have to explicitly re-index them
You have two options: migrate to 2.x, use re-index api to create new index or, create second cluster and re-index from 1.x cluster to 5.x cluster
If you want to migrate an index in 2.x, you can use the migration plugin that comes with a mechnanism to migratie an index
Check the blog post: Upgrade your elasticsearch 1.x cluster to 5.x
Query features that have been removed search_type Search type count and scan have been removed
If you need the count (in case of pure aggregations for instance), use the size is 0 option
If you want to do a scan, use the scroll functionality with a sort based on _doc
maximum amount of shards to query When executing a query over to many shards an error is thrown
The default for this behavior is a 1000 shards, but you can change this using the parameter action.search.shard_count.limit
With the following command yoiu can change it, after that check the response when you query to many shards
java PUT _cluster/settings { "transient": { "action.search.shard_count.limit":10 } } { "type": "illegal_argument_exception", "reason": "Trying to query 27 shards, which is over the limit of 10
This limit exists because querying many shards at the same time can make the job of the coordinating node very CPU and/or memory intensive
It is usually a better idea to have a smaller number of larger shards
Update [action.search.shard_count.limit] to a greater value if you really want to query that many shards at the same time." } removed exists api Not something I use a lot myself, the exists API
With this query you could check if a document exists or not
Now you should use the size 0 and terminate_after is 1
Using the terminate option, each shard will return as soon as the maximum amount of documents are found, in this case 1
remove deprecated queries A lot of 2.x deprecated queries are now removed and can no longer be used: filtered, and, or (use bool queries now) limit (use the terminate_after parameter) top level filter parameter The top level filter parameter is now removed
Only the “post_filter” can now be used
inner hits Beware that the format for source filtering of innerhits is changed, now you need to specify the full path
Mapping changes You should no longer use the type string, now use the type text for analysed strings and keyword for not analysed strings
When providing a field containing text (being a string) without a prior mapping, the field will become a multi field with the main being text and a subfield of type keyword
So ditch the default implementations for raw now use the keyword alternative
And provide a mapping if you do not need this functionality
Numeric fields are now stored using a new mechanism called BKD Tree
One side effect of this is that the document frequency is not stored and therefore scoring of numbers does not use the document frequency
If you do need this behavior index the number as a keyword as well
The index property should now have the value treu/false instead of not_analyzed/no
Beware that there now is a limit in the number of fields in an index (1000)
Of course there is a setting to change this if you need to
Also the number of nested fields is limited as well as the depth of the nesting
Settings changes There are now only three node type settings available: node.master, node.data, node.ingest
So node.client has been removed
Elasticsearch can no longer be configured by setting system properties
Instead, use -Ename.of.setting=value.of.setting
Scripts Indexed scripts zijn vervangen door stores scripts, inclusief alle settings
For scripts most options now only allow true/false, so sandboxed for instance has been removed
Java Client Now the java client is in its own package: org.elasticsearch.client:transport https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.0/java-api.html DocumentAlreadyExistsException has been removed and can therefore no longer be used
Removed ability to pass in boost value using field(String field) method in form e.g
field^2
Use the field(String, float) method instead
A lot of changes in builders with properties that were deprecated and now have been removed
There is also a new REST based java client available
I wrote two blog posts about that one already: part 1 and part 2
Reference documentation can be found here
Scripting Now the default scripting language is Painless, also the configuration for specifying a file instead of inline has changed
Indexed scripts and templates have been replaced by stored scripts which stores the scripts and templates in the cluster state instead of a dedicate .scripts index
Error handling In some scenarios errors will be thrown instead of returning no results
Some examples: querying an unindexed field strict url query_string parameter parsing
Example of providing analyser in stead of analyzer
In case of fatal errors like OutOfMemory that leave the JVM in questionable state, the jvm is now shut down
The end Wow you have come far
This is it for now
I think the guys at elastic did a tremendous job
I can’t wait to get this version into production and start using all the cool new features
If you want to upgrade your cluster and need a second opinion or some help doing it, feel free to contact me to discuss available optionsIn the upcoming version 5 of elasticsearch the implementation for the percolator has changed a lot
They moved the percolator from being a separate endpoint and API to being a member of the search API
In the new version you can execute a percolator query
Big advantage is that you can now use everything you want in the query that you could already in all other queries
In this blogpost I am going to show how to use the new percolator by building a very basic news notification service What is the percolator
For those of you who have never heard about the percolator, let me do a short introduction
The percolator is sort of an inverse search
Usually you store documents and execute a query to match documents
With the percolator we store the queries and take a document as input
Based on that document we look for the queries that the document would match to
The document can be an existing indexed document, or you can provide the document with the query
What are we going to do
In our case I want to create a notification service for interesting news items
So I give the users of my website an option to create a query based on a search term that is being searched for in the title field
I also give them the option to filter on only specific categories
Setting it up First we create an index that we use to store news items
And of course we also need to news items
The next code block shows the commands in curl format so you can try it out yourself as well
Personally I prefer the console in Kibana, with code completion and other interesting stuff, but curl will do fine for now
javascript <pre>curl -XPUT "http://localhost:9200/news" -d' { "mappings": { "item": { "properties": { "title": {"type": "text"}, "body": {"type": "text"}, "category": {"type": "keyword"}, "tags": {"type": "keyword"} } } } }' curl -XPUT "http://localhost:9200/news/item/1" -d' { "title": "Early snow this year", "body": "After a year with hardly any snow, this is going to be a serious winter", "category": "weather" }' curl -XPUT "http://localhost:9200/news/item/2" -d' { "title": "Snow on the ground, sun in the sky", "body": "I am waiting for the day where kids can skate on the water and the dog can play in the snow while we are sitting in the sun.", "category": "weather" }' Now we have an index with two documents
Notice that we used the new elasticsearch 5 field types: text and keyword
These have replaced the string type
Time to configure the percolator index
The mapping for this index contains two parts
The first part, in our case doctype, specifies the fields that we can use in the query
In our case we provide access to the title and category fields
The second part is configuring the field and type containing the stored query
In our case we call this type the notification and the field is query with the type percolator
javascript curl -XPUT "http://localhost:9200/news-notify" -d' { "mappings": { "doctype": { "properties": { "title": { "type": "text" }, "category": { "type": "keyword" } } }, "notification": { "properties": { "query": { "type": "percolator" } } } } }' Now we can add the documents containing the queries to store
Notice that we also metadata fields for the user that we should notify if the query matches a document and the date when this percolator query was inserted
javascript curl -XPUT "http://localhost:9200/news-notify/notification/1" -d' { "query": { "bool": { "must": [ { "match": { "title": "snow" } } ], "filter": [ { "term": { "category": { "value": "weather" } } } ] } }, "meta": { "username": "sander", "create_date": "2016-10-13T14:23:00" } }' curl -XPUT "http://localhost:9200/news-notify/notification/2" -d' { "query": { "bool": { "must": [ { "match": { "title": "sun" } } ], "filter": [ { "term": { "category": { "value": "weather" } } } ] } }, "meta": { "username": "jettro", "create_date": "2016-10-13T14:21:45" } }' Execution time Now imagine that we have just inserted a new document and we want to check if people are interested in the document
The following percolator query would do just that
The field parameter points to the field of type percolator
The document_type points to the type containing the specification of the fields used in the query
The index, type and id parameters point to the actual document under test
The document we match to the stored queries
javascript curl -XGET "http://localhost:9200/news-notify/_search" -d' { "query": { "percolate": { "field": "query", "document_type": "doctype", "index": "news", "type": "item", "id": 1 } }, "_source": { "includes": "meta.*" } }' That is it, how many queries would match for document with id:1
Only Sander would be interested
What about id:2
Both of us, Sander because he likes the snow, and jettro because he likes the sun
javascript { "took": 2, "timed_out": false, "_shards": { "total": 5, "successful": 5, "failed": 0 }, "hits": { "total": 2, "max_score": 0.25811607, "hits": [ { "_index": "news-notify", "_type": "notification", "_id": "2", "_score": 0.25811607, "_source": { "meta": { "create_date": "2016-10-13T14:21:45", "username": "jettro" } } }, { "_index": "news-notify", "_type": "notification", "_id": "1", "_score": 0.25811607, "_source": { "meta": { "create_date": "2016-10-13T14:23:00", "username": "sander" } } } ] } } If you want to be more explicit in why the queries matched the documents, you can use highlighting
The next block shows how to enable highlighting on the title field, as well as the results with highlighting enabled
javascript curl -XGET "http://localhost:9200/news-notify/_search" -d' { "query": { "percolate": { "field": "query", "document_type": "doctype", "index": "news", "type": "item", "id": 2 } }, "_source": { "includes": "meta.*" }, "highlight": { "fields": { "title": {} } } }' javascript { "took": 3, "timed_out": false, "_shards": { "total": 5, "successful": 5, "failed": 0 }, "hits": { "total": 2, "max_score": 0.25811607, "hits": [ { "_index": "news-notify", "_type": "notification", "_id": "2", "_score": 0.25811607, "_source": { "meta": { "create_date": "2016-10-13T14:21:45", "username": "jettro" } }, "highlight": { "title": [ "Snow on the ground, sun in the sky" ] } }, { "_index": "news-notify", "_type": "notification", "_id": "1", "_score": 0.25811607, "_source": { "meta": { "create_date": "2016-10-13T14:23:00", "username": "sander" } }, "highlight": { "title": [ "Snow on the ground, sun in the sky" ] } } ] } } Did you notice that highlighting works on the document you provide to match against, and not somehow to the query that is stored
In a normal situation that would be the other way around
However in this case that would feel useless
I think the new percolator is a lot more flexible, love the way you can now add meta data or just plain data to your percolator query object
Using this data you can create a notification, send it to the persons with the username and tell the users why we think they find the article interestingComplete Cordova App with Angular JS tutorial In part 1 we setup Angular, build the views and route to navigate through pages with a menu
In part 2 we continue with controllers and services to show data from a backend
In part 3 we created the environment to build apps with Cordova In this part we will actually build the apps
Create a new Cordova project To build an app from the angular code we created in step 1 and 2, a new Cordova project has to be created
First create a new folder to start with the Cordova project
On Windows open a command line or, on OS X, open a terminal and go to the new folder and execute the following command to create a new Cordova project: basic cordova create 
nl.hollebm.angularjscordovaapp 'Angular JS Cordova App' ‘.’ represents the current folder, but could be any folder of your choice ‘nl.hollebm.angularjscordovaapp’ is the reversed domain app identifier to identify different projects build by the same company
Your own reversed domain should be used here
This way the identifier is unique on the web
‘Angular JS Cordova App’ is the name of the app Inside the current Cordova folder a www folder exists
Go to this folder and replace all files inside this folder with the Angular files created in step 1 and 2
Build for Android Before building the app for Android, the platform first needs to be added to the project by entering the following command inside the new Cordova folder on the command line / terminal: basic cordova platform add android Adding platforms has to be done only once for each project
Now we can build the app for Android with: basic cordova build android This creates an .apk file inside /platforms/android/build/outputs/apk/android-debug.apk or inside /platforms/android/ant-build/Angular JS Cordova App-debug.apk, depending on your Cordova installation
Make sure the device you are going to install the app on has developer options enabled
Find the build number under settings of your device
Tap several times on the build number, until it mentions ‘developer options are enabled’
Go to developer options and enable USB-debugging
Make sure an Android device is connected to your computer via USB and check if the device is detected by your system with: basic adb devices The detected device should be printed with its id and the word ‘device’ after it
If no device is listed, try remove the device and connect it to a different USB port
Now install the apk on Windows with: basic adb install -r path\to\APK-file\apk-file-name-debug.apk and on OS X with: basic adb install -r path/to/APK-file/apk-file-name-debug.apk Build for iOS Building iOS apps is not possible on Windows, only on OS X
Before building the app for iOS, the platform first needs to be added to the project by executing the following command inside the cordova folder: basic cordova platform add ios Now we can build the app for iOS
basic cordova build ios This command creates an Xcode project file inside /platforms/ios/ which can be opened with Xcode
Clicking the ‘play’-button in Xcode will install and open the app in a simulator
To deploy on a real device a Apple iOS Developer membership is required, which you can get here
And that’s it! We have an Angular JS app, with a menu with which we can navigate (route) to 3 different pages
We can get data with AJAX from a server backend and show this in a page
We now have an environment to build apps with Cordova And we can actually build apps for Android and iOS These are the basics to create hybrid apps
What’s left to do and not covered in this tutorial: Create some real content and functionality for your app Setup some styling to improve the looks of your app Install Cordova plugins to enable native functionality inside your angular app, such as gps, camera, notifications, etc
Add the app to the store Android or iOS « Part 1: How to setup Angular « Part 2: How to use Angular controllers and services « Part 3: How to setup an environment for CordovaComplete Cordova App with Angular JS tutorial In part 1 we setup Angular, build the views and route to navigate through pages with a menu
In part 2 we continue with controllers and services to show data from a backend
This part covers set up an environment to build the app
Building an app for iOS, Android or Windows can be done with Cordova
To use Cordova it needs to be installed with NodeJS
Because Cordova uses Git, we need to install this too
To build an app for Android the Android SDK is needed
Cordova and the Android SDK need Java and Ant
Ant can be installed with Homebrew
For iOS only Xcode is needed
An iOS app can only be build on OS X system, so if you have a Windows computer, you can not build an app for iOS
Let’s start installing… Install NodeJS To install Cordova, we need NodeJS
Download NodeJS install package from http://nodejs.org and run the installer
After the NodeJS installation is complete, open a terminal on OS X or a command line on Windows and execute the following command: basic node --version If the installation was successful this command shows the version of NodeJS that is installed on your system
Install Git Git is a version control system that is used by Cordova
Download an installation package from https://git-scm.com/downloads and run the installer
Install Cordova With the Node Package Manager (NPM) we can install Cordova
on OS X To install Cordova globally with -g, sudo is required
basic sudo npm install -g cordova on Windows To install Cordova globally use the -g
basic npm install cordova -g If the installation was successful the next command should give the installed version of Cordova: basic cordova --version Install Java The Android SDK needs Java to be installed
The minimal version of Java to be installed is 1.6
Check your version: basic javac -version If no version or a lower version is installed, download and install the latest Java version on OS X Download and install from https://support.apple.com/downloads/java on Windows Download and install from https://www.oracle.com/technetwork/java/javase/downloads/ Find the installer file for your system
If you run a 32-bit Windows, download the Java SE JDK x86 installer
If you run a 64-bit Windows version, download the 64-bit installer and execute it
Next update your PATH variable for JDK on Windows
Open the Control Panel, click System and Security, click System, click Change settings, which will open the System Properties window
Select the Advanced tab, then click the Environment Variables button
In the User variables, choose PATH and click edit, or create a new one
At the end of the field Variable value add the next line (including the semicolon as first character): basic ;C:\Program Files\Java\jdk1.8.0_11\bin Make sure the path is correct for your machine
If all is set, click OK
Then add a new variable, if not already exists, by clicking ‘new’
Enter ‘JAVA_HOME’ as the name and as value: basic ;C:\Program Files\Java\jdk1.8.0_11 Then click OK
Open a new command line and check if java is installed: basic javac -version Install Ant on OS X Install Homebrew to install Ant Check the version of Hombrew with: basic brew --version If Homebrew is installed, update to the latest version: basic brew update If Homebrew is not yet installed, download and install from http://brew.sh/
Install Ant with Homebrew basic brew install ant Install Ant on Windows Download Ant from http://ant.apache.org/bindownload.cgi
Unpack the zip file where you want Ant to be installed
For example basic C:\Users\HolleBM\ant The files of the zip should go directly to this folder
Ant also needs to be added to the PATH variable
Open the Control Panel, click System and Security, click System, click Change settings, which will open the System Properties window
Select the Advanced tab, then click the Environment Variables button
In the User variables, choose PATH and click edit
At the end of the field Variable value add the next line (including the semicolon as first character): basic ;C:\Users\HolleBM\ant\bin Make sure the path is correct for your machine
If all is set, click OK
Then add a new variable, of not already exists, by clicking ‘new’
Enter ‘ANT_HOME’ as the name and as value: basic C:\Users\HolleBM\ant Then click OK, and click OK again to close the Environment Variables window
Check if installation was successful basic ant -version on OS X Download the Android Developer Tools (Eclipse ADT) from developer.android.com/sdk and unpack to a location of your choice
For example in basic /Users/HolleBM/android/ Add the path of the location to your .bash_profile in your home folder
Put the next command in a terminal to go to your home folder basic cd Open this folder with basic open 
Search for the file called .bash_profile and open it in your favorite editor
If it does not exist, create a new file and add the following line basic export PATH=/usr/local/bin:/Users/HolleBM/android/sdk:/Users/HolleBM/android/sdk/platform-tools:$PATH If the .bash_profile file already exists, open it and add the path where the ADT is unpacked to the PATH variable at the end of the file
Leave the existing lines as is
This is needed for Cordova to find the Android SDK basic export PATH=/Users/HolleBM/android/sdk/platform-tools:/Users/HolleBM/android/sdk/tools:$PATH Now check if installation was successful, run the following in your terminal basic adb version Open the SDK manager with basic android To install the required Android versions for Cordova, leave all selected presets as is and, depending on the Cordova version you use and the Android versions your app should support, select the required Android versions to install
on Windows Goto developer.android.com/sdk and search for SDK tools package and download Android SDK for Windows (The complete studio is not needed, downloading only the installer_r24.4.1-windows.exe file will do)
During installation copy the path where the SDK is installed, to add to your PATH variable
Open the Control Panel, click System and Security, click System, click Change settings, which will open the System Properties window
Select the Advanced tab, then click the Environment Variables button
In the User variables, choose PATH and click edit
At the end of the field Variable value add the next line (including the semicolon as first character): basic ;C:\Users\HolleBM\AppData\Local\Android\android-sdk\tools;C:\Users\HolleBM\AppData\Local\Android\android-sdk\platform-tools Make sure the paths are set correct (the code example is fictional)
Now check if installation was successful, run the following in a new command line window basic adb version Open the SDK manager with basic android To install the required Android versions for Cordova, leave all selected presets as is and, depending on the Cordova version you use and the Android versions your app should support, select the required Android versions to install
Install the SDK to build an app for iOS Building iOS apps on Windows is not supported by Windows
On OS X, download Xcode from the iStore, install it
Result, so far… We have an Angular JS app, with a menu with which we can navigate (route) to 3 different pages
We can get data with AJAX from a server backend and show this in a page
And we now have an environment to build apps with Cordova What’s left to do: Build an Android and an iOS app with Cordova « Part 2: How to use Angular controllers and services Part 4: How to build an app with Cordova »Complete Cordova App with Angular JS tutorial In part 1 we set up Angular JS, build the views and route to navigate through pages with a menu
This part covers controllers and services to get data from a backend and show it on a page
Angular Controllers To show data from a backend in the view, we need to bind this data to the view, that’s what a controller does
Start with creating a file called ‘contact-controller.js’ and place this file in a new folder called ‘controllers’, which is to be placed in the ‘js/app’ folder, (created in part 1)
Define a controller by calling Angular’s controller function on the angular.module: html $scope.contactData = { companyName: 'HolleBM Webdevelopment', street: 'Hazeleger', streetNumber: '159', postcode: '5431 HR', city: 'Cuijk', phone: '06 - 1404 5334', email: 'info@hollebm.nl' }; }); Pass the name of our controller (‘ContactController’) as the first parameter
In this case we create a controller for the contact.html view
And a callback function is defined with a parameter $scope
The properties of this object becomes available in the view
We start with a static object ‘contactData’ on the scope, which later on will be received from a backend
Include ‘contact-controller.js’ in ‘index.html’ (created in part 1)
html <!DOCTYPE html> <html ng-app="App"> <head></head> <body> <div ng-view></div> <!-- libraries --> <script src="js/libs/angular.min.js"></script> <script src="js/libs/angular-route.min.js"></script> <!-- angular init --> <script src="js/app.js"></script> <!-- directives --> <script src="js/app/directives/main-menu-directive.js"></script> <!-- config --> <script src="js/app/config/route.js"></script> <!-- controllers --> <script src="js/app/controllers/contact-controller.js"></script> </body> </html> Link the controller to the view In route.js (created in part 1) we tell Angular it should use ‘ContactController’ when navigated to #/contact javascript angular.module('App').config(function ($routeProvider) { $routeProvider .when('/home', { templateUrl: 'partials/pages/home.html' }) .when('/another-page', { templateUrl: 'partials/pages/another-page.html' }) .when('/contact', { templateUrl: 'partials/pages/contact.html', controller: 'ContactController' }) .otherwise({ redirectTo: '/home' }); }); Use Angular’s $scope object in the view We have set up a controller and told Angular it’s scope should be used when routed to #/contact
We can now use the scope data in contact.html html <div main-menu></div> <h1>Contact</h1> <strong>{{ contactData.companyName }}</strong><br> {{ contactData.street }} {{ contactData.streetNumber }}<br> {{ contactData.postcode }} {{ contactData.city }}<br> {{ contactData.phone }}<br> {{ contactData.email }} Angular services We can show data in the view, defined in a controller
But we do not want this data to be static
Instead we want to get this data from a server backend
We can do this by creating a service, that gets this data with AJAX
Inside the js/app/ folder create a new folder called ‘services’ where we put all our services
Add a new file called ‘contact-service.js’ to the services/ folder
Like we did with creating a controller, we create a service by calling Angular’s factory function on the angular.module: javascript angular.module('App').factory('ContactService', function ($q, $http) { return { get: function () { var deferred = $q.defer(); $http.get('mock-data/contact-mock-data.json').then(function (result) { deferred.resolve(result.data); }, function (error) { deferred.reject(error) }); return deferred.promise; } }; }); Pass the name of the service, ‘ContactService’, as first parameter
Followed by a callback function with 2 Angular services injected, $q and $http
$q is needed for handling asynchronous results with Promises and $http is used to make an ajax call
The factory callback function returns an object with function named ‘get’
This object can be extended with whatever function is needed to get and return data from or post or put data to a server, local storage or even locally stored values
The get function declares a Promise by $q.defer()
And returns deferred.promise
It makes an asynchronous request by $http.get() and once the server responds, the result is passed through the resolve() or reject() function
Further explanation on Promises and AJAX calls are beyond this tutorial
More information on $q and $httpcan be found in the Angular JS documentation
For now it is enough to understand that this service returns an object with a function ‘get’, that gets data from an external file we will create later and call ‘contact-mocked-data.json’
After creating the ‘contact-service.js’ file we have to include it in index.html html <!DOCTYPE html> <html ng-app="App"> <head></head> <body> <div ng-view></div> <!-- libraries --> <script src="js/libs/angular.min.js"></script> <script src="js/libs/angular-route.min.js"></script> <!-- angular init --> <script src="js/app.js"></script> <!-- directives --> <script src="js/app/directives/main-menu-directive.js"></script> <!-- config --> <script src="js/app/config/route.js"></script> <!-- services --> <script src="js/app/services/contact-service.js"></script> <!-- controllers --> <script src="js/app/controllers/contact-controller.js"></script> </body> </html> Create mock data Create a folder called mock-data in the root folder, the same as where the file ‘index.html’ and the folders ‘js/’ and ‘partials/’ are
Add a file called ‘contact-mock-data.json’ to the new mock-data folder
Add the data defined in the ‘ContactController’ to this new ‘contact-mock-data.json’ file
Make sure the format is valid JSON by adding double quotes to the keys and replace single quotes by double quotes where needed
html { "companyName": "HolleBM Webdevelopment", "street": "Hazeleger", "streetNumber": 159, "postcode": "5431 HR", "city": "Cuijk", "phone": "06 - 1404 5334", "email": "info@hollebm.nl" } This content can now be requested by the service function that we are going to invoke inside the ‘ContactController’
Use the service inside the controller We have a service that contains a function that can be called to get some data
We have a controller that shows data in a view
It is now time to use the service in the controller and replace the static scope data by the result from the service
javascript angular.module('App').controller('ContactController', ['$scope', 'ContactService', function ($scope, contactService) { contactService.get().then(function (data) { $scope.contactData = data; }, function (error) { console.log(error); }); }]); The controller callback is now replaced by an array, [], with as last item the callback function
The first 2 parameters represent the injections of the $scope object and ContactService
This way the callback is passed two instances of these injections
The static data we defined before as $scope.contactData is now replaced by calling the get() function of an instance of the ContactService
When the request is being resolved, the result is put on $scope as the contactData
Navigating to #/contact will now show the contact data requested by the service
Make some changes in the mock-data/contact-mock-data.json file and refresh #/contact to see the changes
Result, so far… We have an Angular JS app, with a menu with which we can navigate (route) to 3 different pages
And we can now get data with AJAX from a server backend and show this in a page
What’s left to do: Use Cordova to create an app for Android and iOS « Part 1: How to setup Angular Part 3: How to setup an environment for Cordova »In the previous blog post I started writing about using the new Rest based client for java in elasticsearch 5
In this blogpost I am going to continue that quest by experimenting with some layers of abstraction using spring features and other considerations
Spring Factory Bean for RestClient First I introduce the Client factory, responsible for creating the singleton client of type RestClient
This factory creates the client as a singleton
It also creates the Sniffer to check for available nodes, like we discussed in the first blog post (see references)
The code is the same as before, therefore I am not going to show it again
Check the class: nl.gridshore.elastic.RestClientFactoryBean, git repo is referenced later on
The RestClientFactoryBean extends the spring AbstractFactoryBean
The result of using the spring Factory pattern is that this factory object creates the RestClient instance which can be injected into other beans
The goal for the application we are going to create is an employee search tool
Therefore I have created and Employee object that I am going to use to index but also as the query result
But I do not want the elastic client execution code to be mixed with the employee business code
Therefore I have created an abstraction layer of interacting with the elastic client
The code to execute index requests for indexing employees and querying for employees is using this abstraction layer
The EmployeeService knows the QueryTemplateFactory
It uses the factory to get an instance of a QueryTemplate
The QueryTemplateFactory knows the RestClientFactoryBean
The RestClientFactoryBean is responsible for maintaining the one instance of RestClient
The QueryTemplateFactory creates a new instance of the QueryTemplate for each call and injects the RestClient
The QueryTemplate could now be used to inject a string based query and handle the execution and response handling
Of course that is not really what we want, adding strings to create a query
There is a better way of doing this using a template engine
But first I want to talk about handling the response
Handling the response Handling a response can be challenging
For a basic query it is not to hard
But when interacting with nested structures and later on adding aggregations it is a lot harder
For now we focus on a basic response that looks like this: java { "took": 19, "timed_out": false, "_shards": { "total": 5, "successful": 5, "failed": 0 }, "hits": { "total": 1, "max_score": 0.7373906, "hits": [ { "_index": "luminis", "_type": "ams", "_id": "AVX57b0YY0m5tWxWetET", "_score": 0.7373906, "_source": { "name": "Jettro Coenradie", "email": "jettro@gridshore.nl", "specialties": [ "java", "elasticsearch", "angularjs" ], "phone_number": "+31612345678" } } ] } } The QueryTemplate must to the boilerplate and from the Employee perspective we want to provide enough information to convert the _source parts into Employee objects
I use a combination of Jackson and generics to accomplish this
First let us have a look at the code to interact with the QueryTemplate: java public List<Employee> queryForEmployees(String name) { Map<String, Object> params = new HashMap<>(); params.put("name", name); params.put("operator", "and"); QueryTemplate<Employee> queryTemplate = queryTemplateFactory.createQueryTemplate(); queryTemplate.setIndexString(INDEX); queryTemplate.setQueryFromTemplate("find_employee.twig", params); queryTemplate.setQueryTypeReference(new EmployeeTypeReference()); return queryTemplate.execute(); } As you can see from the code, the QueryTemplate receives the name of the index to query, the name of the twig template (see next section) and the parameters used by the twig template
The final thing we need to give is the TypeReference
This is necessary for Jackson to handle the generics
This type reference looks like this: java public class EmployeeTypeReference extends TypeReference<QueryResponse<Employee>> { } Finally we call the execute method of the QueryTemplate and notice that it returns a list of Employee objects
To see how this works we have to take a look at the response handling by the QueryTemplate
java public List<T> execute() { List<T> result = new ArrayList<>(); this.queryService.executeQuery(indexString, query(), entity -> { try { QueryResponse<T> queryResponse = jacksonObjectMapper.readValue(entity.getContent(), this.typeReference); queryResponse.getHits().getHits().forEach(tHit -> { result.add(tHit.getSource()); }); } catch (IOException e) { logger.warn("Cannot execute query", e); } }); return result; } Using jackson we convert the son based response from elasticsearch client into a QueryResponse object
Notice that we have a generic type ’T’
Jackson only know how to do this by passing the right TypeReference
The java object tree resembles the json structure: java public class QueryResponse<T> { private Long took; @JsonProperty(value = "timed_out") private Boolean timedOut; @JsonProperty(value = "_shards") private Shards shards; private Hits<T> hits; } public class Hits<T> { private List<Hit<T>> hits; private Long total; @JsonProperty("max_score") private Double maxScore; } public class Hit<T> { @JsonProperty(value = "_index") private String index; @JsonProperty(value = "_type") private String type; @JsonProperty(value = "_id") private String id; @JsonProperty(value = "_score") private Double score; @JsonProperty(value = "_source") private T source; } Notice what happens with the generic type ’T’, so the source is converted into the provided type
Now look back at the code of the execute method of the QueryTemplate
Here we obtain the hits from the QueryResponse and loop over these hits to obtain the Employee objects
Using the twig template language Why did I chose to use a template language to create the query
It is not really safe to just copy a few strings including user input together into one long string and provided that to the QueryTemplate executer
I wanted to stay close to json to make copy paste from the elastic console as easy a possible
However I also wanted some nice features to make creating queries easier
I found jTwig to be powerful enough and very easy to use in java
I do not want to write an extensive blogpost about jTwig
if you want to know more about it please check the references
The following code block shows how we use jTwig: java public void setQueryFromTemplate(String templateName, Map<String, Object> modelParams) { JtwigTemplate template = JtwigTemplate.classpathTemplate("templates/" + templateName); JtwigModel model = JtwigModel.newModel(); modelParams.forEach(model::with); this.query = template.render(model); } First you have to create the model and load the template
In my case I load the template from a file on the class path in the templates folder
Next I add the provided parameters to the model and finally I render the template using the model
The next code block shows the template
java { "query":{ {% if (length(name)==0) %} "match_all": {} {% else %} "match":{ "name": { "query": "{{ name }}", "operator": "{{ default(operator, 'or') }}" } } {% endif %} } } This model supports two parameters: name and operator
First I check if the user provided something to search for
If the name is empty we just return the match_all query
If the name has length, a match query is created on the field name
Also notice that we can provide a default value for the operator parameter in this case
So if no operator is provided we make it or
Concluding That is it, now we have an easy way to write down a query in a jTwig template and parse the results using jackson
If would be very easy to query for another object instead of Employee if we wanted to
The indexing side is similar to the query side
Feel free to check this out yourself
All code is available in the github repo
References http://jtwig.org – Template language used for the queries
https://amsterdam.luminis.eu/2016/07/07/the-new-elasticsearch-java-rest-client/ – part 1 of this blog series https://github.com/jettro/elasticclientdemo – The sample projectComplete Cordova App with Angular JS tutorial
Set up Angular, create a folder structure, create routing, parse templates and create a directive
When building an Angular JS app with Cordova my experience is that I need several forums and other sites to get all the information I need
I haven’t found a complete tutorial to build a simple standard Angular JS application with Cordova
The goal of this tutorial is to give the basics without the need of external sources
This tutorial consists of 4 parts
The files we create in this part and part 2 can be downloaded here
Angular JS Create a file called ‘index.html’ and a folder named ‘js’
Inside the js folder create a new folder named ‘libs’
Download angular.min.js (the latest Angular JS 1 version) from Angular JS website and place the file in the ‘libs’ folder
Include angular at the bottom of the body of ‘index.html’
html <!DOCTYPE html> <html> <head></head> <body> <script src="js/libs/angular.min.js"></script> </body> </html> When opening the inspector in for example Chrome, you should see in the network tab that the angular.min.js file has been included
Define ng-app Add an attribute ‘ng-app’ to any DOM element in ‘index.html’
Angular uses everything inside that element to do the Angular magic
I place the attribute on the html tag, so the angular scope is the complete html page
Let’s start by adding the ng-app attribute with value ‘App’ to the ‘html’ node
html <!DOCTYPE html> <html ng-app="App"> ..
</html> Initialize angular Now that we’ve assigned an element for Angular to work on, we can initialize angular with the ng-app value ‘App’
create a file called ‘app.js’ and place it inside the ‘js folder’ add a first call with an empty array to angular.module function
This initializes angular javascript angular.module('App', []); 3
include the app.js file at the bottom of the body in ‘index.html’ html <!DOCTYPE html> <html ng-app="App"> <head></head> <body> <!-- libraries --> <script src="js/libs/angular.min.js"></script> <!-- angular init --> <script src="js/app.js"></script> </body> </html> The view We can now start with routing for different pages
Add an element with an ‘ng-view’ attribute to index.html
Templates are parsed into that element
The ng-view attribute does not need a value
html <html ng-app="App"> <head></head> <body> <div ng-view></div> <!-- libraries --> <script src="js/libs/angular.min.js"></script> <!-- angular init --> <script src="js/app.js"></script> </body> </html> Route In order to let angular parse templates into the view element, we are going to create route configuration
download angular-route.js file from the Angular JS website if you downloaded angular.min.js from 1 https://ajax.googleapis.com/ajax/libs/angularjs/1.5.0/angular.min.js you can find angular-route.min.js in the same version sub folder: 1 https://ajax.googleapis.com/ajax/libs/angularjs/1.5.0/angular-route.min.js download and place the angular-route.min.js file in js/libs folder and include it in index.html html <!DOCTYPE html> <html ng-app="App"> <head></head> <body> <div ng-view></div> <!-- libraries --> <script src="js/libs/angular.min.js"></script> <script src="js/libs/angular-route.min.js"></script> <!-- angular init --> <script src="js/app.js"></script> </body> </html> 3
create a folder ‘app’ inside the js folder 4
create a folder called ‘config’ inside the new ‘app’ folder 5
create an empty file inside the ‘config’ folder called ‘route.js’ 6
include route.js in ‘index.html’ at the bottom of the body html <!DOCTYPE html> <html> <head></head> <body> <div ng-view></div> <!-- libraries --> <script src="js/libs/angular.min.js"></script> <script src="js/libs/angular-route.min.js"></script> <!-- angular init --> <script src="js/app.js"></script> <!-- config --> <script src="js/app/config/route.js"></script> </body> </html> In route.js we configure routing with the $routeProvider
Before we can use the route provider, we have to tell Angular we are going to use it
So extend the app.js file by adding ‘ngRoute’ to the empty array; javascript angular.module('App', ['ngRoute']); Now we can write the route configuration in route.js; javascript angular.module('App').config(function ($routeProvider) { $routeProvider .when('/home', { templateUrl: 'partials/pages/home.html' }) .when('/another-page', { templateUrl: 'partials/pages/another-page.html' }) .when('/contact', { templateUrl: 'partials/pages/contact.html' }) .otherwise({ redirectTo: '/' }); }); This example is going to create an app with 3 pages
A home page, another page and a contact page
In route.js we have defined that partials/pages/home.html should be parsed into the element with the ng-view attribute when a user navigates to index.html/#/home
When the user navigates to index.html/#/contact, the partials/pages/contact.html template will be parsed into the ng-view element
And if the user navigates to an url that does not exists, the user will be redirected to #/home
Of course these partials must first be created
Creating the page partial templates Create a folder called ‘partials’ in the same folder as the ‘index.html’ file
Inside the new ‘partials’ folder create another folder called ‘pages’ Create 3 html files with the names ‘home.html’, ‘another-page.html’ and ‘contact.html’ Place these html files inside the ‘partials/pages’ folder Add a title to each of the partials with the name of the page
For the home.html this would be html <h1>Home</h1> For the another.html this will be html <h1>Another page</h1> And for the contact.html this will be html <h1>Contact</h1> In a real app these pages contain of course the content of that page
Try navigating by going to index.html/#/home and after that to index.html/#/contact
You could omit index.html and just go to #/home or #/another-page
The menu Now we have the pages working, we can build a menu to navigate with
Create a file called ‘main-menu.html’ inside the partials folder (not inside the ‘pages’ folder)
html <nav> <ul> <li><a href="#/home">Home</a></li> <li><a href="#/another-page">Another page</a></li> <li><a href="#/contact">Contact</a></li> </ul> </nav> We are going to include this partial in every page we have by using Angular’s directive functionality
Angular directive For the menu to show up on each page, we add a div with a ‘main-menu’ attribute before the title on every page; html <div main-menu></div> <h1>Home</h1> Make sure the attribute is lowercase and seperated by a dash (-)
Create a folder called ‘directives’ inside the ‘js/app/’ folder
Add a file called ‘main-menu-directive.js’ to the ‘directives’ folder
javascript angular.module('App').directive('mainMenu', function() { return { restrict: 'A', replace: true, templateUrl: 'partials/main-menu.html' }; }); A directive is created by calling Angular’s directive function with the name of the directive attribute written in camelCase
In the callback function we return an object with the restrict property: ‘A’
The ‘A’ stands for attribute, which we are using (‘ ‘)
We also return a property ‘replace’ with value true
This means the templateUrl will replace the element with the attribute
If set to false, it will be parsed inside the directive element
And at last we define the html template that will be used by the ‘templateUrl’ property
Include the main-menu-directive.js in ‘index.html’ html <!DOCTYPE html> <html ng-app="App"> <head></head> <body> <div ng-view></div> <!-- libraries --> <script src="js/libs/angular.min.js"></script> <script src="js/libs/angular-route.min.js"></script> <!-- angular init --> <script src="js/app.js"></script> <!-- directives --> <script src="js/app/directives/main-menu-directive.js"></script> <!-- config --> <script src="js/app/config/route.js"></script> </body> </html> The order of including the files is very important
For example to invoke angular.module(‘App’).directive() angular must exist and it should be initialized
Result, so far… We have an Angular JS app, with a menu with which we can navigate (route) to 3 different pages
What’s left to do: Get some data from a backend and show that on the pages we created Use Cordova to create an app for Android and iOS Part 2: How to use Angular controllers and services »This is the introduction to a new series of blog posts about data analysis with the SMACK stack
Follow along in the coming weeks to learn more! Over the last 20 years, the architectures of (big) data platforms have changed and companies processing large amounts of data have moved from big Enterprise Data warehouse solutions to on-premise / cloud-based solutions, with Apache Hadoop (Hadoop Distributed File System, MapReduce, and YARN) as a fundamental building block
Architectures based on Hadoop tend to be focussed on (long running) batched or offline jobs, where data is captured to storage and then processed periodically
For usage in an online environment, this batch based processing was becoming too slow and business expectations were changing
Over the last 5-10 years, the demand for performing (near) real-time analysis has been pushing the industry into finding new solutions and architectural patterns to achieve these new goals
This has led to several ‘new’ architectural patterns like the Lambda architecture and the Kappa architecture
Both architectural patterns have a focus on processing data at speed (stream processing), where the Kappa architecture is purely focussed on a streaming (speed) layer and completely removes batch-oriented processing
When designing a data platform there are many aspects that need to be taken into consideration: the type of analysis – batch, (near) real-time, or both the processing methodology – predictive, analytical, ad-hoc queries or reporting data frequency and size — how much data is expected and at what frequency does it arrive at the platform the type of data – transactional, historical, etc the format of incoming data — structured, unstructured or semi-structured the data consumers – who will be using the results This list is by no means exhaustive, but it’s a starting point
Organisations processing high volumes of data used to always pick a (single) vendor backed product stack, but these days there are so many great, open source, reliable and proven solutions out there that you can easily take a best of breed approach and build your own stack
There is a wide variety of components to select, so always do it based on your specific requirements
One of the more popular, general purpose and best-of-breed big data stacks I’ve seen lately is the SMACK stack
The SMACK stack The SMACK stack consists of the following technologies: Spark – Apache Spark™ is a fast and general engine for large-scale data processing
Spark allows you to combine SQL, streaming, and complex analytics
It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs
Spark has support for both real-time (Spark Streaming with µ batches) as well as batch (MapReduce) processing
Mesos – Apache Mesos™ abstracts CPU, memory, storage, and other compute resources away from machines (physical or virtual), enabling fault-tolerant and elastic distributed systems to easily be built and run effectively
Mesos runs applications within its cluster and makes sure they are highly available and in the case of a machine failure will relocate applications to different nodes in the cluster
Akka – Akka is a toolkit and runtime for building highly concurrent, distributed, and resilient message-driven applications on the JVM
Akka uses the Actor Model to raise the abstraction level and provide a better platform to build scalable, resilient and responsive applications
Everything in Akka is designed to work in a distributed environment: all interactions of actors use pure message passing and everything is asynchronous
Cassandra – Apache Cassandra™ is a proven, high performant, durable and fault tolerant NoSQL database
Cassandra can easily manage large amounts of data and offers robust support for clusters spanning multiple datacenters and geographical locations
Kafka – Apache Kafka™ is a distributed, partitioned, replicated commit log service
It provides the functionality of a messaging system, but with a unique design to allow a single cluster to serve as the central data backbone for a large organization
The tools are very easy to integrate with each other and serve their own purpose within a modern platform for Big Data applications
The ease of integration between has probably helped a lot in making it a popular solution, but it’s not the only reason
I think the most important reasons are because: it’s a concise toolbox that can deal with a wide variety of data processing scenarios it’s composed of proven, battle tested and widely used software components
The individual components are open source and backed by a large open-source community the stack is easily scalable and replication of data happens while still preserving low latencies the stack can run on a single cluster managed platform that can handle heterogeneous loads and any kind of applications Over the next couple of weeks, we’ll be doing a deep-dive into each individual technology, so we can elaborate why these technologies combined are extremely powerful and give you a wide variety of options when designing your (big) data architecture
Feel free to continue reading in the second part of this series, which covers Apache Mesos, the foundation of the SMACK stackTools for virtualization and provisioning like Vagrant and Ansible have been around for a while now, but I haven’t met many people that use them for their everyday development work
So I am going to try to make some PR for Vagrant and Ansible
I first started working in virtual machines because I wanted to be independent of the hardware my employer provided me with
At that time, we moved from a desktop computer to a frequently crashing Windows laptop, to a perfectly good System76 laptop with Ubuntu pre-installed
By the end of that transition, I did all my development work in a vm
I had an Ubuntu guest running inside an Ubuntu host and I was happy; because I knew that when I would have to switch hardware again, I could just export my vm and import it somewhere else
Back then I only used VirtualBox
Nowadays I use Vagrant and Ansible to completely automate the process of setting up a new development environment
Whenever I start working on a new project, I let Vagrant and Ansible do their magic
Vagrant takes care of configuring VirtualBox and it will also pull a base image with some OS pre-installed
Ansible does provisioning, which, in a nutshell, means installing packages and copying configuration files
This approach has several advantages
Your host OS will not become a monolith with all kinds of packages installed that you no longer need
Each one of your projects has its own personal sandbox
It’s like Python’s virtualenv or Ruby’s RVM on an OS level
If you manage to break apt-get or grub, it will only affect 1 project
And you can easily recreate that 1 vm
If your hardware breaks or you need to switch hardware for some other reason, you won’t have to cry
If you backed up your VirtualBox files, you can continue working on another machine without having to reinstall anything
And if you didn’t make backups, you can just rerun Vagrant and Ansible
When you close your vm, you can choose to save its state
Memory will be freed so you will be able to use all your system’s resources for something else
And when you start your vm again, it’s like you never left and you are right back in the middle of that Jupyter notebook file
You can easily share your Ansible roles with your team members
If there is a new colleague coming to work, you won’t have to sit next to him/her for an entire day to help him set up his environment
Just sit next to him while his machine is provisioning and explain to him in general terms what Ansible is installing and why
In case you need to transfer your work to a co-worker, you can give him/her your vm image in addition to any other files/information
When you are done working on a project, make sure all your code is in git, like a proper developer
Then you can just delete your vm without leaving any debris
And of course there are drawbacks as well Obviously, you would need to know a little bit about Vagrant and Ansible
Both your hardware and host OS need to support hardware virtualization
There is some overhead involved in running an OS on top of another OS
But I don’t think I ever noticed enough to complain about it
In any case, storing your VirtualBox files on an SSD will certainly help
You need a little bit of extra disk space for all those guest OS’s that will start gathering dust
You should just delete old vm’s that you don’t use anymore, or at least remove them from your SSD
If you use proprietary software, you might run into licensing issues
In the case of IntelliJ, it seems you cannot run 2 instances of the IDE simultaneously
Occasionally you need to have 2 vm’s open at the same time
Make sure you have enough memory available to support both vm’s
If not, the 1st vm you started will be terminated while starting up or un-suspending the 2nd
Depending on the memory usage of your vm’s and your host, running 2 vm’s simultaneously on an 8 GB host can get tricky
I have learned to appreciate extra RAM as well as lightweight Linux distributions
I hope this piece of propaganda will motivate people to try something new
On my Github account, you will find some scripts that I use to set up a development environment for data science projects that I sometimes do
Take note that it’s a work in progress and it probably won’t work out of the box, because I use my own private base box
If you are completely new to Vagrant or Ansible, it’s best you start with some tutorialIn my first month at Luminis I got the chance to dive into a data set stored in Elasticsearch
My assignment was easily phrased: detect the outliers in this data set
Anomaly detection is an interesting technique we would like to apply to different data sets
It can be useful in several business cases, like an investigation if something is wrong with these anomalies
This blog post is about detecting the outlying bucket(s) in an aggregation
The assumption is that the data is normally distributed
This can be tested with normality tests
But there is no mathematical definition of what constitutes an outlier
The domain of this data set is that the data is about messages to and from different suppliers
The point of interest is if the number of messages linked to a supplier differs in a week from all the weeks
The same can be done with different time intervals or over other aggregations, as long as they are normally distributed
Also a number field can be used instead of the number of messages in an aggregation (bucket document count)
Or if the success status of a message is stored, that can be used as a boolean or enumeration
The first step is to choose an interesting aggregation on the data that gives normally distributed sized buckets
In the example below I chose a Date Histogram Aggregation with a weekly interval
This means that the data is divided into buckets of a week
Over the weeks the messages to and from the suppliers were evenly sent per week
Actually this results in that the size of the buckets are normally distributed
Then the following mathematical theory can be applied to determine the anomalies
The second step is to determine the lower and upper bound of the interval in which the most of the normally distributed data lies
This is done by calculating the mean and the standard deviation of the number field
It this case these are calculated over the document count of the weekly buckets
In empirical sciences it is conventional to consider points that don’t lie within three standard deviations of the mean an outlier, because 99,73 % of the points lie within that interval (3-sigma rule)
The lower bound of the interval is calculated by subtracting three times the standard deviation of the mean and the upper bound by adding it to the mean
In Elasticsearch this can be done with the Extended Stats Bucket Aggregation, which is a Pipeline Aggregation that can be connected to the previous aggregation
We set parameter sigma to 3.0
Note that Elasticsearch calculates the population standard deviation by taking the root of the population variance
This is mathematically correct here, because the entire population of messages is stored
(If there would be more data than we have and we want to determine the outliers in our sample, then merely applying Bessel’s correction wouldn’t be enough, an unbiased estimation of the standard deviation needs to be used.) Both steps can be done in the same GET request in Sense
java GET indexWithMessages/_search { "size": 0, "aggs": { "byWeeklyAggregation": { "date_histogram": { "field": "datumOntvangen", "interval": "week" } }, "extendedStatsOfDocCount": { "extended_stats_bucket": { "buckets_path": "byWeeklyAggregation>_count", "sigma": 3.0 } } } } The third step is to do the same query with the same aggregation, but this time only select the buckets with a document count outside the calculated interval
This can be done by connecting a different Pipeline Aggregation, namely the Bucket Selector Aggregation
The *lower* and *upper bound* need to be taken from the *std_deviation_bounds* of the *extendedStatsOfDocCount* Aggregation of the response
In my case these are *11107.573992258556* and *70207.24418955963*, respectively
Unfortunately the buckets path syntax doesn’t allow to go up in the aggregation tree
Otherwise the requests could be combined
It is possible to get the lower bound and the upper bound out of the Extended Stats Aggregation in a buckets path, but the syntax is not intuitive
See my question on discuss and the issue raised from it
java 1 GET indexWithMessages/_search { "size": 0, "aggs": { "byWeeklyAggregation": { "date_histogram": { "field": "datumOntvangen", "interval": "week" }, "aggs": { "outlierSelector": { "bucket_selector": { "buckets_path": { "doc_count": "_count" }, "script": "doc_count < 11107.573992258556 || 70207.24418955963 < doc_count" } } } } } } Now we have the weeks (buckets) that are outliers
Further investigation with domain knowledge can be done with this information
In this case, it could for example be a vacation or a supplier could have done less or more in a certain weekWith the latest release of elasticsearch 5.0.0 alpha 4, a new client for java is introduced
The idea behind this new client is less dependencies on elasticsearch
At the moment you have to include the complete elasticsearch distributable with even a lot of Lucene libraries
Also there were some requirements when using the Transport client
The application has to make use of the same JVM version as the running elasticsearch instance and the version of elasticsearch in the application needs to be the exact same as the running elasticsearch cluster
Therefore they have started creating the new http based client
It is going to be created in multiple layers
The low end layer only contains the http communication, a sniffer to find other nodes, and maybe some classes for basic operations
The other layers will contain a query DSL and whatever may become important
At the moment only the low level layer is available
Also it is the first available version, so be warned, changes may come
In this blogpost we introduce the new java http based client
We create a basic application that interacts with an elasticsearch cluster
We start with the connection and sniffing part
Then we send some data and create a search request to obtain the data again
Setting up your java project The sample project is a spring-boot project
I choose maven for the dependencies, just because it is so easy
To work with the new elasticsearch client you need 1 dependency
If you also want to sniff for other hosts, you need a second dependency
The following code block shows the required dependencies java <dependency> <groupId>org.elasticsearch.client</groupId> <artifactId>sniffer</artifactId> <version>${elastic.version}</version> </dependency> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> Creating the connection To create a connection you can use just one line
The goal is to create an instance of RestClient
java RestClient.builder(new HttpHost("localhost", 9200)).setFailureListener(loggingFailureListener).build(); Of course you can provide more than one host, but our goal is to use the sniffer to find the other hosts
Sniffing for nodes The RestClient has an option to find other hosts in the cluster using the sniffer
This way you can provide one host and the sniffer will find all other nodes in your cluster
The following lines initialise the sniffer
java @PostConstruct public void afterCreation() { this.client = RestClient .builder(new HttpHost("localhost", 9200)) .setFailureListener(loggingFailureListener) .build(); this.sniffer = Sniffer.builder(this.client, HostsSniffer.builder(this.client).setScheme(HostsSniffer.Scheme.HTTP) .build() ).build(); } The Sniffer uses the HostsSniffer to find the other nodes in the cluster
It also maintains a blacklist of nodes that are not available anymore
In alpha 4 you have to specify the Scheme, in the next version this will become optional
The next block shows the log lines that more nodes are found using the sniffer
java RestClient : request [GET http://127.0.0.1:9202/_nodes/http?timeout=1000ms] returned [HTTP/1.1 200 OK] HostsSniffer : adding node [EX2OSXW2QrOR0qnhgJyTIQ] HostsSniffer : adding node [ZhmTiqJ-QF-R6RsMYvKKwg] HostsSniffer : adding node [gz9vOjMaTVmyz0maxEn96w] Sniffer : sniffed hosts: [http://127.0.0.1:9202, http://127.0.0.1:9200, http://127.0.0.1:9201] Sniffer : scheduling next sniff in 300000 ms Obtain the cluster health The first thing we are going to try is a very basic request, obtain the health of the cluster
We use Jackson to parse the son result into a java bean
In our case the result we need is the ClusterHealth
java public class ClusterHealth { @JsonProperty(value = "cluster_name") private String clusterName; @JsonProperty(value = "status") private String status; @JsonProperty(value = "number_of_nodes") private int numberOfNodes; } At the moment the RestClient has one method to perform a request
The following code block shows how we do a get request with the url _cluster/health
You can provide request parameters
In this case just an empty map
You can also provide a request body, in this case we do not provide one, therefore we set it to null
Finally you can also add headers, but in our case we do not need them
java Response response = client.performRequest( "GET", "/_cluster/health", new Hashtable<>(), null); HttpEntity entity = response.getEntity(); Then using the Jackson mapper, we can transform the HttpEntity into the ClusterHealth object java ClusterHealth clusterHealth = jacksonObjectMapper.readValue(entity.getContent(), ClusterHealth.class); Now let us move on to create a document
We have an index called luminis
The type of the documents is ams and the id is generated by elastic
At the moment the document contains just one field: employee
I take it you know what the java bean would look like
Now let us create the request to actually create the document in elasticsearch
java HttpEntity requestBody = new StringEntity(jacksonObjectMapper.writeValueAsString(employee)); Response response = client.performRequest( "POST", "/luminis/ams", new Hashtable<>(), requestBody); Finally query for employees Now we are going to execute a match query
The response is the exact response from a normal search request
Therefore we create an object hierarchy that resembles the son structure
The classes we use are like this
java public class ResponseHits { private Hits hits; } public class Hits { private List<Hit> hits; } public class Hit { @JsonProperty(value = "_index") private String index; @JsonProperty(value = "_type") private String type; @JsonProperty(value = "_id") private String id; @JsonProperty(value = "_score") private Double score; @JsonProperty(value = "_source") private Employee source; } Then we can obtain all employees with the provided name using this code
java String query = "{\"query\":{\"match\":{\"employee\":\"" + employee + "\"}}}"; Response response = client.performRequest( "GET", "/luminis/_search", new Hashtable<>(), new StringEntity(query)); HttpEntity entity = response.getEntity(); ResponseHits responseHits = jacksonObjectMapper.readValue(entity.getContent(), ResponseHits.class); response.close(); return responseHits.getHits().getHits().stream() .map(Hit::getSource) .collect(Collectors.toList()); Ok, writing queries like this is of course not what we would like to do, so we need the next layer with a query DSL
But now you can see how to create a query and how to parse the result
Concluding The start of the new client is there
You are now able to start querying elasticsearch using the new client
Since this is all still alpha there can be changes and I cannot wait to get my hands on the first higher level layer with the query DSL
References Original issue with a lot of additional information https://github.com/elastic/elasticsearch/pull/18735 The sample project https://github.com/jettro/elasticclientdemo Part two of this series https://amsterdam.luminis.eu/2016/07/25/the-new-elasticsearch-java-rest-client-part-2/In this post, after a brief commercial for my somewhat recently defended dissertation[1], I’ll draw a couple of parallels between the work I did in academia and how we approach data science at Luminis
My dissertation is about search, or, information retrieval: how to find relevant bits in the ever growing multitude of information around us
Machine learning pops up a lot in search these days, and also in my work, whether it is classifying queries, clustering search results, or learning to rank documents in response to a query
All chapters handle specialized search engines, e.g., people, scientific literature, or blog search engines
Using background knowledge, we improve general algorithms
As an example, to improve Twitter search, we used hashtags to automatically train a machine learning algorithm (Chapter 7, based on [2])
A first obvious parallel between what I did before and what we do at Luminis Amsterdam is search
Elasticsearch is one of our biggest knowledge areas, as the proportion of blog posts on this site about it indicate
A second is that the machine learning algorithms used in my dissertation can be applied to a wide range of problems, also outside search
For example, at Luminis we use classification techniques for problems like churn prediction and prospect scoring
The algorithms I personally have most experience with, e.g., decision trees, support vector machines, Bayesian classifiers, share the property that they are interpretable
At Luminis, we feel it is important for our customers to be able to understand, maintain, and manipulate the predictive algorithms we build for them
A third is data
Right in the beginning of my PhD, in one of my favorite projects, we performed a query log analysis of a people search engine [3]
What made this exciting for me was the fact that we were working with real data, from real people
At Luminis, we work with real data as well, e.g., data from schools, hospitals, and businesses
A fourth is tooling
In my PhD, as my experiments grew more complex and my datasets larger and larger, I appreciated more and more the software engineering challenges associated with data science
Working at Luminis means upgrading my software toolbox in almost every aspect
Python libraries like pandas, scikit-learn, Javascript frameworks like Angular, The ELK stack (Elasticsearch, Logstash, Kibana), Spark, Java frameworks like Spring and OSGI are some of the software that I’ve started using a lot more
A fifth is dissemination
Of course, science is all about dissemination of knowledge (after one has been the first to obtain a publishable bit of it)
But at Luminis, too, we believe in sharing our knowledge, and even our code, with large open source projects like Amdatu [4]
For me personally, it means among other things that I was given the chance to prepare a talk about how one might approach a data science project for a retail business starting from just an Excel sheet and without any business input; the video [5] and code [6] are online
A sixth is experience
At ILPS, led by Maarten de Rijke, where I did my PhD, there was a vast amount of experience with challenges and opportunities of the full range of major web search engines to smaller specialised search engines like, for example, Netherlands Institute of Sound and Vision
At Luminis, we can draw on a vast amount of experience with the challenges and opportunities of our customers—businesses, semi-public and public organisations
Putting these ingredients together, this is one way I like to think about how we approach data science: we enable organisations to build a data driven culture that can be sustained by its people, and based on which decision makers can make responsible and informed strategies and decisions
Interpretable algorithms, insightful reports and experiments, interactive dashboards with visualisations that directly relate to what is going on under the hood in predictive algorithms, useful applications; all backed by solid software engineering, resulting in lean and maintainable code bases
[1] Berendsen, R
W
(2015)
Finding people, papers, and posts: Vertical search algorithms and evaluation
PhD Thesis, UvA
URL: http://dare.uva.nl/record/1/489897 [2] Berendsen, R., Tsagkias, M., Weerkamp, W., & De Rijke, M
(2013, July)
Pseudo test collections for training and tuning microblog rankers
In Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval (pp
53-62)
ACM
Pre-print: http://wouter.weerkamp.com/downloads/sigir2013-pseudotestcollections.pdf [3] Weerkamp, W., Berendsen, R., Kovachev, B., Meij, E., Balog, K., & De Rijke, M
(2011, July)
People searching for people: Analysis of a people search engine log
In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval (pp
45-54)
ACM
Pre-print: http://www.wouter.weerkamp.com/downloads/sigir2011-peoplesearchlog.pdf [4] https://www.luminis.eu/what-we-share/open-source/ [5] https://youtu.be/8wNl89zXlKw [6] https://github.com/luminis-ams/devcon-2016-rbWith around 6.2 million active customers, Bol.com is the number one online retailer in The Netherlands
One of the most important components of an E-commerce platform is having a good search engine
Luminis Amsterdam is helping Bol.com implement Elasticsearch for search capabilities
Synonyms in Elasticsearch A common feature of search engines is the ability to search using synonyms
In the UK people use the word bag, whereas people in the US use the word purse for the same thing
What if our documents are UK-oriented and don’t contain the word purse
We would still like to help our US customers, by adding a synonym that either replaces the word purse with bag or adds bag to the query as an extra search term
Whether you want to use synonym expansion or synonym contraction depends on your use-case and won’t be part of this post
Configuring synonyms in Elasticsearch can be done in two ways
Either by adding your synonyms directly to a SynonymTokenFilter like so: java PUT luminis-synonyms { "settings": { "index": { "analysis": { "analyzer": { "synonym": { "tokenizer": "whitespace", "filter": [ "synonym" ] } }, "filter": { "synonym": { "type": "synonym", "synonyms": [ "bag, purse, handbag" ] } } } } } } Or by placing a file on your server and referencing that file with the synonym_path property: java PUT luminis-synonyms { "settings": { "index": { "analysis": { "analyzer": { "synonym": { "tokenizer": "whitespace", "filter": [ "synonym" ] } }, "filter": { "synonym": { "type": "synonym", "synonyms_path": "analysis/synonym.txt" } } } } } } Pretty easy to setup and it works within minutes
Now there is one caveat here: What if we want to update the synonyms after the analyzer has been created
Bol.com wants to have full control of their synonyms and the ability to update them at any given time
The official documentation says the following about updating synonyms: If you specify synonyms inline with the synonyms parameter, your only option is to close the index and update the analyzer configuration with the update index settings API, then reopen the index
Updating synonyms is easier if you specify them in a file with the synonyms_path parameter
You can just update the file (on every node in the cluster) and then force the analyzers to be re-created by either of these actions: Closing and reopening the index (see open/close index), or Restarting each node in the cluster, one by one Of course, updating the synonym list will not change any documents that have already been indexed
It will apply only to searches and to new or updated documents
To apply the changes to existing documents, you will need to reindex your data
What if we want to add more synonyms on our production environment
It’s not possible for us to use index-time synonyms since that requires reindexing the data, but if we would just use query-time synonyms we would have to restart our production nodes or close/open the index
Not very feasible for a production environment
Elasticsearch has support for creating your own plugins and effectively extending the functionality of the engine
In order to meet Bol.com’s requirement to update synonyms on the fly, we created a plugin that does just that
Since we’re using query-time synonyms, our plugin extends the functionality of the SynonymTokenFilter by having a updatable synonymMap
We added custom REST handlers that will process new synonyms after they’ve been uploaded to Elasticsearch
Now people responsible for maintaining synonyms at bol.com can add/remove synonyms on the fly, whereas with their current Endeca setup it takes one day for new synonyms are processed
How the plugin works As mentioned above I created a new _synonym endpoint in Elasticsearch which allows the following functionality: GET: Retrieve the current list of synonyms POST/PUT: Upload a new synonym file containing the new updated synonym file DELETE: Delete all known synonyms Since we’re working in a distributed environment, we have to let other nodes in the cluster know that a new file has been uploaded
As soon as the node receiving the file has processed the new synonyms, it will store them inside an index in Elasticsearch
After indexing the new synonyms the plugin will broadcast to the other nodes in the cluster that new synonyms are available, which in turn leads to the other nodes applying the new file and confirming back to the original node if everything went ok
Once all nodes processed the new synonyms, we report back to the user that applying the new synonyms was successful
If an error happens during processing, we report back to the user that the operation was unsuccessful
At the time of writing, there is no automatic failover/rollback mechanism (yet), which means that we have to manually check what happened and re-apply the file
Since synonyms are being kept in memory, we need a way for new nodes joining or restarting nodes to load in the synonyms during startup
We created a service that will check for an existing synonyms index, and load these into memory
This way we make sure that all nodes in the cluster have the same version of the synonyms file in memory
This is just one of the cool new things we’re doing at Bol.comIn this blog, I will be presenting two strategies for implementing faceted search with Elasticsearch
Few days back I had a discussion with my colleague Byron about implementing faceted search when Elasticsearch is being used to serve the search results
And this blog is a culmination of our discussion
In Elasticsearch, the aggregation framework provides support for facets and executing metrics and scripts over those facet results
Following is a simple example wherein each Elastic documents contains a field color and we execute a Term Aggregation on the document set javascript { "aggs" : { "colors" : { "terms" : { "field" : "color" } } } } And we get the following javascript "buckets": [ { "key": "red", "doc_count": 2 }, { "key": "green", "doc_count": 2 }, { "key": "blue", "doc_count": 1 }, { "key": "yellow", "doc_count": 1 } So we get each of the unique color as a bucket key and doc_count is the number of documents have corresponding color field value
One of the options is to keep going on with sub-aggregations which is also called Path hierarchy sub-aggregation but that is something which is very expensive and also not feasible beyond a certain hierarchy level, as discussed here
First approach The first approach for faceted search is when we have a unique field corresponding to a hierarchy level present in each document
For example, if we have documents pertaining to products for online webshop and 3 levels of hierarchy then a product document would look something like – javascript { ..
"categoryOneLevel": [ "8299" ], "categoryTwoLevel": [ "8299-3131" ], "categoryThreeLevel": [ "8299-3131-2703", "8299-3131-2900" ] } On the UI we can visualize it with something like – Computer (8299) (level 1) > laptop(3131) (level 2) > (Linux (2703) and Mac(2900)) (level 3) > further on
The reason for storing the other levels in each deeper located category is because a product can exist in multiple categories
If we take this approach then sample queries would be something like – When user is on landing page (no facetting yet) javascript GET document/_search { "aggs": { "categories": { "terms": { "field": "categoryOneLevel", } } } } Now when the user clicks, the next query fired would be javascript GET document/_search?size=0 { "query": { "bool": { "filter": { "term": { "categoryOneLevel": "8299" } } } }, "aggs": { "categories": { "terms": { "field": "categoryTwoLevel", "include": "8299-.*" } } } Here the filter query selects all the documents of categoryOne and then does the aggregations on all categoryTwo values which are directly linked with categoryOne as we choose the include : 8299-* filter Thus you can extend this approach further that if the user clicks on categoryTwo then results from categoryThree are returned
In this way we can have faceted search results from ElasticSearch
Second Approach Now, let’s look at the second approach which somewhat comes out of the box
Using the built-in Path tokenizer
Here a single field holds the hierarchy values and while indexing we specify the “path tokenizer” mapping setting for that field
For example, a single field would hold the comma separated value and path tokenizer would produce the token values as – Computer,Laptop,Mac And produces tokens: Computer Computer,Laptop Computer,Laptop,Mac In this approach, I am taking the actual user-readable values of categories instead of Ids, it’s only for example sake, as taking Ids gives you the flexibility to update the category names like Computer, Laptop etc associated with that Id
Let’s look at some elastic queries – javascript PUT blog_index/ { "settings": { "analysis": { "analyzer": { "path-analyzer": { "type": "custom", "tokenizer": "path-tokenizer" } }, "tokenizer": { "path-tokenizer": { "type": "path_hierarchy", "delimiter": "," } } } }, "mappings": { "my_type": { "dynamic": "strict", "properties": { "hierarchy_path": { "type": "string", "analyzer": "path-analyzer", "search_analyzer": "keyword" } } } } } Our index is ready with field “hierarchy_path” having the path tokenzier set as the analyzer thus now the terms of this field will be tokenized based on the path_tokenizer
Now lets, add a document to the index javascript POST blog_index/my_type/1 { "hierarchy_path": ["Computer,Laptop,Mac","Home,Kitchen,Cookware"] } We have added a document with field hierarchy_path having example of two set of categories and each set having comma separated values
If we have execute a terms aggregation on field “hierarchy_path” we get javascript GET blog_index/my_type/_search?search_type=count { "aggs": { "category": { "terms": { "field": "hierarchy_path", "size": 0 } } } } We get the following buckets javascript "buckets": [ { "key": "Computer", "doc_count": 1 }, { "key": "Computer,Laptop", "doc_count": 1 }, { "key": "Computer,Laptop,Mac", "doc_count": 1 }, { "key": "Home", "doc_count": 1 }, { "key": "Home,Kitchen", "doc_count": 1 }, { "key": "Home,Kitchen,Cookware", "doc_count": 1 } ] From the above results, we can see that the path_tokenizer has split the comma separated values of the field “hierarchy_path”
So, now based on the user activity we can fire queries to select the documents pertaining to the category which user is looking for
The query for selecting the top level category would be javascript GET blog_index/my_type/_search?search_type=count { "aggs": { "category": { "terms": { "field": "hierarchy_path", "size": 0, "exclude": ".*\\,.*" } } } } and we get javascript "buckets": [ { "key": "Computer", "doc_count": 1 }, { "key": "Home", "doc_count": 1 } ] We have used the regular expression exclude”: “.*\\,.* which excludes all the sub-levels thus we get only the top hierarchy
If user wants only second level then the query fired would be javascript GET blog_index/my_type/_search?search_type=count { "query": { "bool" : { "filter": { "prefix" : { "hierarchy_path" : "Computer" } } } }, "aggs": { "category": { "terms": { "field": "hierarchy_path", "size": 0, "include" : "Computer\\,.*", "exclude": ".*\\,.*\\,.*" } } } } Wherein we specify the regex for include which would mean all documents which are part of Computer hierarchy but we exclude the third level of hierarchy thus result only contains second level of hierarchy
javascript "buckets": [ { "key": "Computer,Laptop", "doc_count": 1 } ] When the user activity requires third level of hierarchy then the query fired would be javascript GET blog_index/my_type/_search?search_type=count { "query": { "bool" : { "filter": { "prefix" : { "hierarchy_path" : "Computer" } } } }, "aggs": { "category": { "terms": { "field": "hierarchy_path", "size": 0, "include" : "Computer\\,.*\\,.*" } } } } Based on the include regex “Computer\\,.*\\,.*” we wil get only the documents have the third level of hierarchy as well javascript "buckets": [ { "key": "Computer,Laptop,Mac", "doc_count": 1 } ] In this way based on the user activity of our application we can fetch corresponding results from Elastic and while indexing our documents we need to make sure the product documents have relevant value in the “hierarchy_path” field based on the hierarchy level which that product would be present inIn our projects we often have several Java objects that needs to be mapped to other objects, depending on it’s purpose
One of the most common cases is a domain item that needs a representation on the frontend
Another case is that we want to store data in elasticsearch, but don’t want to use the domain item there as well
Mapping one object to another is a boring task and could easily lead to mistakes
MapStruct can generate bean mappings at compile-time based on annotations
The generated mapping code uses plain method invocations and thus is fast, type-safe and easy to understand
How does it work
Defining a mapping is really simple
First you need to create an interface with the @Mapper annotation
Then add a method that expects the source as parameter and the target as return type
java @Mapper public interface RelationMapper { RelationListDTO relationToListDto(Relation relation); } If your object contains another object you will need to define a mapping for that object as well
So if for example our Relation object contains an Address object and the RelationListDto contains a AddressListDto, the mapper has no idea how to map the properties of Address to our AddressListDto
java public class Relation { private String name; private String email; private Address address; // getters and setters } public class RelationListDTO { private String name; private String email; private AddressListDto address; // getters and setters } Therefor you need also need to create a mapper for Address and use that mapper in the RelationMapper class
java @Mapper(uses = {AddressMapper.class}) public interface RelationMapper {...} Instead of defining a separate mapper for Address you can also specify how to map specific properties
Let us change the RelationListDto a little bit so that we only have a few properties instead of an Address object
java public class RelationListDTO { private String name; private String email; private String street; private String city; // getters and setters } Now we can specify how to map these properties with the @Mapping annotation: java @Mapper public interface RelationMapper { @Mapping(source = "address.street", target = "street") @Mapping(source = "address.city", target = "city") RelationListDTO relationToListDto(Relation relation); } Dependency injection If you use a dependency injection framework you can access the mapper objects via dependency injection as well
Currently they are supporting CDI and Spring Framework
All you have to do is specify which framework you are using in the @Mapper annotation: java @Mapper(componentModel = "spring") public interface RelationMapper {} Warnings When you have a mismatch between properties in your objects, MapStruct will generate a warning that will tell you that you have unmapped properties
This can be a completely valid scenario as you maybe not want to map all properties
If this is the case you can add a mapper annotation with the property name and the ignore attribute
This way MapStruct wil skip the property from being mapped
java @Mapping(target = "somePropertyName", ignore = true) And more.
These examples are just the simple use cases with MapStruct, but there is so much more
Things like implicit type conversions or referencing another mapper in a mapper
You can also customise your mapper with the @BeforeMapping and @AfterMapping which allows you to manipulate the objects before and after the mapper starts
So it’s not really magic, but I like MapStruct
Just give it a try and maybe it will also help you in your projectSigning XML in .Net has been supported since I started working with .Net
Signing XML with SHA-256 signatures is a different story, this wasn’t supported out of the box and you had to write some obscure code
Luckily this will change with the release of .Net 4.6.2
In this post I will show how to sign XML with SHA-256 signatures before the release of .Net 4.6.2 and after
The first step in both scenario’s is to obtain a digital certificate which can be used for SHA-256 signing on Windows
A good to follow tutorial can be found here
After obtaining this certificate you can write code which uses the certificate to digitally sign XML
Before .Net 4.6.2 The first step is to register a SignatureDescription class that defines SHA-256 as the digest algorithm
.NET already contains a class called “RSAPKCS1SHA1SignatureDescription” that supports SHA1
We have to create a similar class for SHA-256: We need to register this class with the framework before we can use it: You can see it being registered on line 12, in the static initializer of our extension class
This class also contains a method which can be used to digitally sign an XML document with SHA-256: You can see the reference being made to SHA-256 on line 32 and on line 24
With this method you can sign XML documents using SHA-256, pre .Net 4.6.2
After .Net 4.6.2 After .Net 4.6.2 is released, we don’t need any of our custom classes, as both the SHA-256 description class and the namespace constants (they are present on the SignedXml class) are provided by the framework
This means that the modified method for .Net 4.6.2 will look like this: Conclusion With the release of .Net 4.2.6, the platform get’s a long awaited update to the Crypto api’s
You can change the references in the above method to references to SHA-384 or SHA-512 and those will also work
For a complete overview of what else is new in the Crypto api’s and other stuff, you can take a look hereFor quite some time I’ve been meaning to rewrite an old Elasticsearch plugin to a new Kibana plugin
It’s quite different than you were used to
The Kibana plugins are quite new and were released in version 4.2.0
There are quite a few warnings on the Kibana Github issues regarding not having a public API yet or not making plugins at all
Essentially this means you need to keep up with the Kibana releases if you’d like to proceed anyway
Starting First you would need to set-up a Kibana development environment
In order to do this you can follow these steps: https://github.com/elastic/kibana/blob/master/CONTRIBUTING.md#development-environment-setup
I would recommend not to pick the master branch
Kibana Plugin Yeoman Generator Next up is generating the boilerplate for your Kibana plugin using the Kibana Plugin Yeoman Generator
When following the instructions of the readme please notice there is an instruction to put the Kibana development environment at the same folder level as your plugin named kibana
What did you get
After we generated the project there is in the root of the project a file called index.js listed below, which ties up the whole project together
In this file we see the configuration of a Kibana plugin
The uiExports object configures your frontend of the plugin
There are more variants other than ‘app’ that can be found here
The properties an app can have are listed in the source code here
The ‘main’ property lists the path to your angular code, which is in fact the public folder of your project
code import exampleRoute from './server/routes/example'; export default function (kibana) { return new kibana.Plugin({ require: ['elasticsearch'], // Your frontend app uiExports: { app: { title: 'Example', description: 'An awesome Kibana plugin', main: 'plugins/example/app' } }, config(Joi) { return Joi.object({ enabled: Joi.boolean().default(true), }).default(); }, init(server, options) { // Add server routes and initalize the plugin here exampleRoute(server); } }); }; The latter part of the sample above contains configuration for a hapijs server
With this you’re able to write your custom backend API
More of this you can find in the server/routes folder
The frontend In the public/app.js file we can find two important parts of code
One of which is the part with uiRoutes
This object allows you to embed your routing within Kibana
The syntax of the routing is according angular’s ngRoute module
When you don’t need routing remove it
code uiRoutes.enable(); uiRoutes .when('/', { template, resolve: { currentTime($http) { return $http.get('../api/example/example').then(function (resp) { return resp.data.time; }); } } }); The latter part has the uiModules object which is in charge of generating dynamic modules on the fly and coupling them
You can see the uiModules.get() function as replacement for angular.module()
code uiModules .get('app/example', []) .controller('exampleHelloWorld', function ($scope, $route, $interval) { $scope.title = 'Example'; $scope.description = 'An awesome Kibana plugin'; var currentTime = moment($route.current.locals.currentTime); $scope.currentTime = currentTime.format('HH:mm:ss'); var unsubscribe = $interval(function () { $scope.currentTime = currentTime.add(1, 'second').format('HH:mm:ss'); }, 1000); $scope.$watch('$destroy', unsubscribe); }); When writing the templates and styles you should keep in mind that Kibana uses Twitter Bootstrap
Another note is that in the previous file mentioned there is also a chrome object, which you can ignore it will be deprecated in Kibana 5.0.0
It was used to control the navbar
Result When all went well this should be the result
Useful links Source Sense Source Timelion Kibana styleguide The plugin I’m converting More information about pluginsWith the upcoming release op Elasticsearch 5.0, Elastic will introduce a nice new feature: Ingest When Elastic first presented the Ingest API, their introduction to this subject was: “I just want to tail a file”
What they meant with that, is that right now with the current Elastic Stack you need quite a bit of setup in order to get logs into Elasticsearch
A possible setup could be the following: First, you have some logs sitting in your application server
Maybe some application logs, maybe some access logs
Let’s take access logs as an example for this blog post
In order to get those logs into Elasticsearch, we can ship those to a queue (for example Redis), using Beats
We then have Logstash pulling logs out of the queue so that we can process these raw logs and turn them into JSON documents
One of the great things about Logstash is that we can enrich our logs as well as making them ‘Elasticsearch-ready’
After processing our logs we can tell Logstash to output our processed logs to Elasticsearch and voila, we can now either search for our logs in Elasticsearch or visualise them using Kibana
Now if we take a closer look at the above setup, we can see that there are quite some components needed in order to ‘just tail a file’
With the new Ingest feature, Elasticsearch has taken the ‘filter’ part of Logstash so that we can do our processing of raw logs and enrichment within Elasticsearch
At the moment of writing this post, the Ingest feature has been merged into the master branch of Elasticsearch which means that we can already play around with it
In this blog post, I will show you some of the new nifty features of Elasticsearch Ingest
To make use of the Ingest features, you need to define ‘pipelines’
Elastic defines pipelines as: “A pipeline is a definition of a series of processors that are to be executed in the same order as they are declared
A pipeline consists of two main fields: a description and a list of processors”
The description speaks for itself, but processors are actually where all the magic happens
Elasticsearch comes with a list of processors, which Logstash users will recognise as ‘filters’ in Logstash
Let’s create a new pipeline: json PUT _ingest/pipeline/log-parsing { "description": "Our first log-parsing pipeline", "processors": [ { "grok": { "field": "message", "pattern": "%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes}" } }, { "remove": { "field": "message" } } ] } Now on the top, we defined our pipeline to be named ‘log-parsing’
Now during indexing, we can refer to this pipeline so that Elasticsearch will try and process the document according to our defined processors
Ingest comes with a nice ‘_simulate’ endpoint so that we can see what would happen to our documents if we’re to ingest a document through our newly made pipeline
This makes testing your pipeline a lot easier: json POST _ingest/pipeline/_simulate { "pipeline": { "description": "Our first log-parsing pipeline", "processors": [ { "grok": { "field": "message", "pattern": "%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes}" } }, { "remove": { "field": "message" } } ] }, "docs": [ { "_source": { "message": "212.78.169.6 GET /index.html 26224" } } ] } Within the doc’s part of the JSON request, we can define our documents that we want to test against the pipeline
Our document contains one field with a fictional log line
If we run this query, the simulate api will show us the data transformation: json { "docs": [ { "doc": { "_id": "_id", "_index": "_index", "_type": "_type", "_source": { "request": "/index.html", "method": "GET", "bytes": "26224", "client": "212.78.169.6" }, "_ingest": { "timestamp": "2016-04-01T13:37:11.716+0000" } } } ] } As you can see our grok pattern matched and Ingest was able to process the log line and split up our message field into separate fields as defined in our grok pattern
Since we’re not interested in the message field anymore, it has been thrown out by the ‘remove’ processor
Next to having our fields available in ‘_source’, Ingest also adds an extra Ingest Metadata block to our documents
This metadata is also accessible in a processor
As mentioned before, Elasticsearch comes with a list of processors which you use
These are the currently added processors: html Append Convert Date Fail ForEach Gsub Join Lowercase Remove Rename Set Split Trim Uppercase In our first example, we added a pipeline to Elasticsearch
Now instead of using the ‘_simulate’ API, let’s index some documents using our pipeline! json POST logs/log/1?pipeline=log-parsing { "message": "212.78.169.6 GET /index.html 26224" } Now if there were no errors during processing, we can retrieve our document and check it’s contents
json GET logs/log/1 Which results in: json { "_index": "logs", "_type": "log", "_id": "1", "_version": 1, "found": true, "_source": { "request": "/index.html", "method": "GET", "bytes": "26224", "client": "212.78.169.6" } } It worked! Next to creating pipeline’s through JSON, Elasticsearch has also extended Kibana with the possibility to chain processors and create your own pipeline’s through Kibana’s interface! At the moment of writing this post, not all processors have been added to Kibana yet, so we’ll save that one for when all processors have been added to Kibana
These are just the basics of the new Ingest feature
In my next blog post, we will combine Ingest with the new Reindex API, use Kibana to setup Ingest and we will also explore more advanced Ingest features
If you’re excited about Ingest, you can read more on Elasticsearch’s GitHub page here
Stay tuned! If you are interested in Ingest you can also come to our conference where Luca Cavanna will do a deep dive into Ingest: Ingest node reindexing and enriching documents within elasticsearch Contacts us if you are interested in attendingRecently, I went to a three-day workshop on OSGi and Amdatu and I enjoyed the shift in thinking that is required when working with modular applications
Modularization is one of the core principles of the Microservices architecture
Working with OSGi/Amdatu presents, in some aspects, the same difficulties as working with Microservices, that’s why I will compare building a modular OSGi/Amdatu application with building a monolithic Spring Boot application
This comparison aims to give a feeling of how it is to work with both technologies, it avoids in-depth details or discussions about use cases
A common denominator for these technologies are RESTful web-services, so that will constitute the ‘comparison case’
I wrote this article for developers that don’t have experience with Microservices or with OSGi/Amdatu
What is Spring boot
According to its creators, ‘Spring boot takes an opinionated view of building production-ready Spring applications
It favours convention over configuration and is designed to get you up and running fast.’ It’s been around for quite a while and it has become quite popular
In simple terms, with Spring Boot, you can create a Spring web application without the hassle of wiring in the application the various Spring components by yourself
Spring Initialzr goes a step further and allows you to generate a working scaffold of an application by using a wizard in which you can select the components you need
There are plenty examples online of how to use Spring Boot, I won’t delve now into further details
What is OSGi
What is Amdatu
I remember that a couple of years ago I was making my first contact with OSGi
It wasn’t that widely used back then and that didn’t change that much nowadays (even if it evolved a lot since then)
The OSGi technology is ‘a set of specifications that define a dynamic component system for Java
These specifications enable a development model where applications are (dynamically) composed of many different (reusable) components.’ One of the popular OSGi implementations is Apache Felix
In simple terms, OGSi is for Apache Felix what SQL is for MySQL
‘Amdatu is a set of open source components and tools to build modular applications in Java.’ Its purpose is to make modular development of enterprise/web/cloud applications easy
It achieves that by providing a comprehensive documentation, an OSGi application bootstrap wizard and various mostly used components
For simplicity, in this article, I’ll refer to OSGi & Amdatu as OSGi
Case study As I mentioned before, I want to show how developer productivity is affected by the choice of technology (modular versus monolith)
I feel it’s important to restate that each architecture excels in particular use cases, my goal is to show, as objectively as possible, how it affects development
In the case study, I started off from two simple RESTful Hello World web applications with in-memory persistence (one with OSGi, the modular application, and one with Spring Boot, the monolithic application) and I added persistence with mongoDB and refactored some interfaces
A major limitation of this comparison is that it does not look at bigger applications
That is where things can get more interesting/messy and the differences between architectures might become more obvious
To counter this, I’ve made sure that the observations below are general enough and will apply to any application size
Dependencies of modules With the modular application (OSGi), as I keep writing more and more modules, I notice that the task of adding the correct dependencies repeats itself
A module might need some dependencies that another module does not and it is a best practice to only include what you use
Pro: the classpath for that specific module will be clean and lean and the module itself will have quite a small footprint
Additionally, since adding new dependencies is a manual task, the developer becomes more aware of concerns of the module (I see myself stopping and thinking whether a dependency on apache.common.lang belongs to a JPA module) Con: the creation of new modules requires a bit more work because all the dependencies need to be explicitly defined
You will have to know exactly in which dependency a class is, there’s no support from the IDE here
With the monolithic application (Spring Boot), all added dependencies are available across the application
Pro: Development goes fast when you don’t have to worry about dependencies (most modern IDEs allow you to add dependencies automatically when you first use one of the classes they define)
Con: The classpath gets cluttered fast
If you stop using a dependency, it will stay on the application classpath until you manually remove it
You might end in jar hell
Thinking with modules With the modular application (OSGi), the need to think about modules appears from the beginning, in the application design phase
Nothing stops you from writing a modular application that has big modules, or even just one module (transforming the application into a monolith)
Pro: you are encouraged to think about modules from the start
Writing new modules is easy and resembles the mindset needed to write Microservices
Writing a small application with OSGi seems, to me, to be the easiest way of trying out the idea of modularity (setting up the OSGi environment is a lot easier that setting up the infrastructure needed to run Microservices)
Con: At the beginning, the modular thinking feels awkward, because it’s new
A bit later, problems with the infrastructure that enables inter-modules communication, might make it seem that the modular approach is not worth it
With the monolithic application (Spring Boot), developing is familiar and feels easy
Pro: Monolithic applications have been around since the beginning and everyone is used to working with them
Con: Modularity is not enforced by the system but thought out good architecture and good coding practices
Tight deadlines or lack of discipline in the development team can easily affect the application modularity
Casual development Day-to-day development activities usually revolve around refactoring, changing implementations and adding new features
Let’s see how easy or hard it is to to each of them
Changing the implementation of an existing interface: Currently, both applications have in-memory persistence
Let’s switch to mongoDB persistence (see tag mongo-persistence; OSGi, spring-boot)
The switch to a different implementation of an interface is straightforward: for the modular application, a new module needs to be created and the bindings need to be redone so they use the new implementation; for the monolith, the old implementation is simply replaced with the new one
Refactoring: On the existing persistence service interface, let’s add a new method and let’s change an existing method (see tag refactoring; OSGi, spring-boot)
For both applications adding a new method is easy: update the interface and then use the IDE to fix the implementations
Same goes for changing the signature of a method because it’s fully automated by the IDE
Adding a new feature: Let’s add a feature that reports the number of records stored in the database every minute(see tag added-new-feature; OSGi, spring-boot)
This feature is not a critical feature (the application can function well without it)
For both applications, adding the feature was as easy as ‘Changing the implementation of an existing interface’ (due to the low coupling of the feature with existing application)
But I need to make an interesting observation with the occasion of adding this feature
This is another moment at which the modular application excels: this feature can be enabled/disabled easily, without any downtime of the application
The monolith requires a complete restart
I like this free functionality that OSGi provides out of the box
And I can imagine that the architecture of the modular application could be created in such a way that it would leverage this feature even more
General remarks In this last section, I will make a couple of remarks that don’t fit in the other sections
Extra layer: OSGi adds an extra infrastructure layer to the application
This is a low-level layer that normally you don’t have to care about it, but it will make the application more complex (for example, modules must be bound manually, the application will have extra OSGi specific configuration)
This is also true for Microservices
They also come with an extra infrastructure layer (that is a lot more complex than in the case of OSGi and, currently, you have to design and build yourself)
Out-of-the-box lower downtime: In modular architectures, like OSGi and Microservices, applications are broken down into smaller pieces
These pieces contain less code and are independent of each other
This means that you don’t have to take the whole application down when you want to update one of its parts
And this is a feature that comes together with the architecture, you get it for free
Conclusion In this article, I gave a broad image of how the choice of architecture, modular or monolithic, affects development efforts
I’ll repeat here the most valuable ideas that came up: OSGi can be used, at least, as a learning tool, to get a feel of how it is to write modular applications (thinking about modules, lower downtimes)
This experience can be handy when using Microservices Modular architectures have an extra layer of complexity compared to monolithic architectures
Some of the development efforts will go into maintaining this layer Hope you enjoyed the article
I’d like to hear your thoughts on it (use the comment form below)This week we are attending elasticon 2016
If you want to follow us along you can check our twitter feeds
If you like to read the summary read this and the previous posts
Our twitter handles are: @rjlinden @ByronVoorbach @jettroCoenradie https://amsterdam.luminis.eu/2016/02/18/elasticon-2016-day-1/https://amsterdam.luminis.eu/2016/02/19/blog-elasticon-day-2/ All About Elasticsearch Algorithms and Data Structures (Zachary Tong and Colin Goodheart-Smithe) This was the most complex talk we attended this year
To be honest, it was a bit to complex
So if you are interested in this topic, wait for the video to appear and look at it yourself
The only few remarks I have are about filteres and their caching mechanism
Filters work on immutable segments, therefore we can keep them for a long time
There are three different mechanisms that all have their own advantages mostly based on the amount of documents in the segment
Luckily the choice for the mechanism to use is chosen automatically
So depending on the amount of values in a segment different ways to store the filter results are used: Less than 4096 documents, a Sorted List is used, More than 4096 documents, a Bit map is used, A lot more documents, a Compressed bit map How to Build Your Own Kibana Plug-ins (Tanya Bragin and Spencer Alger) This was a very interesting presentation with some live hacking of creating a custom plugin showing how to create a new visual component in Kibana
The plugin takes a new D3 Guage component and makes it available in Kibana
There was a lot of code in the presentation that is going to be put on github
We have a lot of interesting ideas for Kibana plugins, so stay tuned for interesting new blog post in the near future
Bringing Healthcare Analytics to the Point-of-Care @ Mayo Clinic (Peter Li) Interesting story about using elasticsearch to store medical events in elasticsearch and do analysis over a lot of patients to predict what could happen to current patients
They used technologies like the percolator as well as parent child relation ships to enrich the events before actually being stored
Quantitative Cluster Sizing (Christian Dahlqvist and Ryan Schneider) This session did not give a lot of insights in how to size your cluster, but it did explain why
Also it showed how you can do experiments with your own data and how to evaluate the results to make it possible to determine the number of shards, size of hardware for your situation
The conference ended with a chat between Shay CTO from elastic and Lew CTO cloud from Cisco
They talked about open source and the future of it
Finally Steven took the stage to thank for attending the conference and to ask if we want to come back next yearThis week we are attending elasticon 2016
If you want to follow us along you can check our twitter feeds
If you like to read the summary read this and the previous and the next blog posts
Our twitter handles are: @rjlinden @ByronVoorbach @jettroCoenradie https://amsterdam.luminis.eu/2016/02/18/elasticon-2016-day-1/ Get the Lay of the (Lucene) Land (Adrien Grand) Lucene started out as a library for full-text search
Today Lucene is used for so much more: Analytics, Geo Search, Suggestions, Data store, structured search
To support these functionalities the Inverted Index was not enough any more, the Column store was introduced in 4.0
This enabled faster aggregations and sorting for instance
The column store was introduced in Lucene to make it easy to combine the inverted index queries with a column store store queries in one API
In Lucene a faster implementation of the k-d tree is provided
This enables a much faster structured search, which is important for numbers, geo queries and dates
To enable fast structured search, numbers are index on multiple levels
First the data is filtered using the 1st level, than the heavier searching is done one less terms
More levels means more detail combining the levels, search make it much faster
The idea behind K-D trees is to split the space into cells that contain the same amount of nodes and keep on splitting until you have one item per cell
For Lucene this is too fine grained
Lucene will stop the splitting until you reach around 1000 spots
Elasticsearch 5.x will support BigDecimal and IPv6 which is currently not supported based on the 64byte limit using the new k-d structure
An important change in the upcoming Elastic version is the change of the default scoring algorithm
BM25 is going to be the standard over TF-IDF in elasticsearch 5.x, it has better support for common terms
Lucene 5.4 has improved support for Two-phase iteration, it can do a fast approximation followed by a slower match Additional information about k-d tree can be found here: https://www.elastic.co/blog/lucene-points-6.0 Eventbrite’s Search-based Approach to Recommendations (John Berryman) Eventbrite is building an Elasticsearch-powered, content- and behavior-based recommendation system to match users with events they are sure to enjoy
Sometimes organizers complained that their events are not easy to find
To solve this problem Eventbrite combines the search and recommendations creates a personalized search for different users, allowing them to provide better results
This way a user from United Kingdom would get different results with the same search terms as a user from the Canada.Eventbrite applies data at index time to make it easier to query for recommendations
Security, Alerting, Monitoring & More With the Elastic Stack (Lots of speakers) This is all about x-pack, the commercial products of elastic
Monitoring Marvel 1.x was great when you know exactly what you are looking for
The next Marvel becomes Monitoring for elasticsearch 2.0
It will support the new Kibana 5 look and feel
Monitoring comes with support for multiple clusters
It becomes easier to compare the health of different Nodes with each other
There is integration with Watcher to create a feature, most likely called issues, that will automatically recognise problematic situations and make a notification available for you to act upon
Finally it will provide cross stack monitoring, so it includes monitoring of KIbana, logstash and beats together with elasticsearch monitoring
Security with the Elastic stack New API for adding users to shield, currently you have to do this using a CLI tool
Kibana Security comes with lots of features to secure your Kibana installation
It has some built in users, secured dashboards, user management
Alerting 2.0 Introduces an api to activate and deactivate a watch
Good addition
Watcher will embed curator as an action, available to do some cleaning up based on certain states
A cool addition is the watcher UI, which will become available in the next version
This UI will help with managing your watches
Reporting It will become possible to generate pdf using Watcher triggers and send the report using watcher actions
A management panel will be available with the possibility to look at the history of generated reports
Graph Capabilities in the Elastic Stack (Mark Harwood and Steve Kearns) Data is not flat, relationships live in our data
You can relate by having a document that relates two other entities
You can also have a relationship when two documents have the same value for a field
This is ideal to be presented and used as a Graph of relationships, but also for recommendations
Creating the Graph and finding the relevant related nodes is usually done based on the count of relationships
With the availability of elasticsearch, graph exploration can be done with relevance: Follow links not by count, but by relevance
Don’t skip super connected entities, account for them
But understand this does not work in all situations
Therefore explore count based or by relevance and provide the option to choose
An example is finding creditcard problems of the people in the room
Imagine everyone here has problems with their creditcard
Questions that you can ask to identify the source are: Who bought something from Amazon
Almost everybody
Now who bought a ticket for Elasticon
Again almost everybody
Not a good distinction
But if you change the context from people in the room to people on the street, there will be a big difference
Still almost everyone has bought something at Amazon, but not to many people have bought an Elasticon ticket
Therefore the relevance for people buying Elasticon tickets of the people in this room is a lot bigger
Elasticsearch queries can help with this context
A Kibana plugin to support visualising the graph and providing input to play with the graph is available in version 5
Using the interface you can store the results of a graph API call as a percolator query
That way use related items to be notified of new albums or something
Within the Graph GUI you can expand your visible nodes easily from each node
And you can group found nodes into one node to interact with that node by finding new relations between that node and others
All of this by pushing a few buttons
Ingest Node: Enriching Documents within Elasticsearch (Tal Levy and Martijn van Groningen) The Ingest Pipeline is kind of the filter part from logstash
It does not contain input and output
The input is intercepting all incoming documents from elastic and the output is always the indexer for elastic
All nodes can become ingest nodes, in production you most likely will create specific nodes for the ingestion
Ingest comes with a number of processors
Think about Grok, Geo, mutations and converters
You can assign documents that fail the original pipeline to go to another pipeline to be handled differently, use the on_failure to do this
Debugging and testing pipelines is easy to do with a simulation, that way you can try out your pipeline without actually indexing the documents
With ingest it is now possible to intercept every document add using plugins add fields for metadata
The interception is done without knowledge about how the document is stored
The mapping is not available, the shard it will be send to is unknown
Advantage is that you do not have to do the pre-processing for every shard
The interception and processing is taking place on the node level, not on shard level
That way when working with replica shards all the processing is only done once
The ingest node also make the re-index API more powerful
Using the new reindex api together with the ingest functionality, you can alter the original documents using the ingest pipeline before sending them to the new index
The gui wizard in Kibana helps you to create the pipeline
It will also generate the index mapping template when you have configured your complete pipeline and it has a nice feature to help you programming your grok commands by showing the result immediately
Some useful links about the new ingest node.https://github.com/elastic/elasticsearch/issues/14049https://github.com/elastic/elasticsearch/blob/master/docs/reference/ingest/ingest-node.asciidoc Time for some drinks, tomorrow the final day of elasticon 2016This week we are attending elasticon 2016
If you want to follow us along you can check our twitter feeds
If you like to read the summary read this and the following two blog posts
Our twitter handles are: @rjlinden @ByronVoorbach @jettroCoenradie After some very nice days in San Francisco the conference finally started today
After the registration it was time to catch up with some of our previous colleagues
We had a chat with Noi, Uri, Steven, Elissa, Sejal, Martijn and of course Luca from Elastic
So much fun to travel the world to speak to people living close to us
After some hours of mingling and talking to some of the sponsors of the conference began with the keynote
Always fun to see Steven on stage, makes me feel proud we used to work together
Ok, now to the real content
What was announced during the keynote?Some numbers: 1800+ attendees, 50.000.000 downloads Elastic products versions will be stream lined
Starting with the next major release bonanza all products will become 5.0
The first alfa release is expected within a few weeks
Next to that the logo’s of all the products have been streamlined as well
So fresh new logo’s for elastic, logstash, kibana, beats and some new ones
They now want to make it easier to install extensions to different kind of products
Think about a Kibana extension together with a custom beats product
You can now install all these together using the packs product
Elastic is going to use this approach for their own commercial plugins
This is now introduced as x-pack
This is bundled set of features for security, alerting, monitoring and more to come
X-pack introduces tight shield integration into Kibana, now they also make it possible to create a login form in Kibana to secure everything you show in Kibana
Elastic told us last year that they acquired Found, a hosted elasticsearch platform
They invested heavy in it, but found was hard to find
Therefore they renamed found into Elastic Cloud
To top it of, they are making found available to buy as a product
This way you are now able to host your own private elastic cloud on your own hardware or cloud provider
This is branded as Elastic Cloud Enterprise
The beta is becoming available shortly
What’s Evolving in Elasticsearch (Clinton Gormley and Simon Willnauer) Multiple improvements have been made to the usage of Heap space
Among them a different store than the inverted index
The columnar store is used for things like aggregations and sorting
In elastic this is called the doc values
This is one of the many improvements in the heap value usage which is very important for the performance of elastic
One important functionality of elastic is support for Geo locations
Most of the functionalities are backed by Lucene ones
Recently Geo points v2 is introduces
It has a 10 times performance increase, 50% less index size
Based on this new geo support elastic can create cool new features
One of them being the geo centroid aggregations, these determine the center of geo points instead of a grid of data points
For us one of the most interesting features in the re-index api
This will enable you to re-index your index into a new one with new mappings or change the number of shards
Next to the re-index api some other new api’s were introduced
Among them the _update_by_query and task management api
Scripting has been an issue from 1.2 onwards
Do we support scripting, what scripting language is safe
To make it possible to do scripting elastic needed another approach
No general purpose scripting language, but specific for elastic
It is called “Painless”, it does only support what elastic needs
New type introduced for data structures, string is replaced with text and keyword A new feature is Search after, this adds support for deep pagination
It has always been a problem for elastic to ask for page 200 or something
Now with Search after you can enable this functionality
From 5.x onwards a new Java client will be introduced
It is a wrapper around the Java Http Client, the same as for all the other supported language drivers
This driver decouples the server and client
The driver makes it possible to upgrade the server but leave the client behind
The client also needs a lot less dependencies
What’s Cookin’ in Kibana (Chris Cowan and Rashid Khan) This talks first talks about some features in the Kibana 4 interface
Also it gives a preview of the new 5.x interface
With 4.x it now possible to customize the axis labels and colours on charts for the different bars
Kibana re-introduces the dark side theme, which is received with much enthusiasm
You can use field formatters to show certain images instead of a value, this can be used to show a coloured icon based on the actual data
You can also write your own plugin for a formatters
It is now easy to export everything from Kibana, even to export a visualization to generate more of the similar one and then reimport it
In Kibana 5.x the new interface is introduced
The interface removes a lot of clutter
They made it possible to give you around 20% more space to show your data
A lot has been done on supporting plugins
Everything you see in Kibana is already a plugin and you can easily create your own plugin by using the yeoman generator for node.js
If you want to extend Kibana in more depth, hacks are the way to go
They make it possible to do almost everything you need to do
Make a hammer to hit every nail
Examples of really cool Kibana hacks can be found on github: simianhacker/Kibana-hacks Rashidkpc/Kibana-hacks What’s Brewing in Beats (Monica Sarbu and Tudor Golubenco) Beats are about importing data into elastic
There are a lot of beats out of the box and they all use the library, libbeat
Examples are File beat, Top beat, Packet beat
With packet beat you can decode network traffic for known sources
Some of these sources are: HTTP, DNS, MySQL and PostgreSQL
Besides the elastic supported plugins there are also more than 10 community supported beats
Some examples are Nginx and Docker
Starting version 5.x a new beat is introduced
Metric beat, collecting metrics from other systems into elastic using the libbeat library
Metric beat can be used out of the box with a number of systems, but it can also be a library to include in your own beat
Other new features for 5.x are: Filebeat will contain redis/kafka output options out of the box
Each beat will include generic filtering, for instance to drop certain events or even fields
That way you could filter all 200 OK messages in an http beat
Hunting the Hackers: How Cisco Talos is Leveling Up Security (Kate Nolan and Samir Sapra) Cisco’s Talos provides protection before and after cybersecurity threats
During this talk Kate and Samir gave a rough overview what their system is doing and how they use Elasticsearch to leverage data to detect bad guys
They run a 10 node cluster with approximately 3TB of data and about 100k events per day, to do dynamic malware analysis
Simple automated Elasticsearch queries help with finding (new) exploit kits, by searching for common patterns within their event data
Samir dove deeper into how the system helped them takedown a hacker group called SSHPsychos/Group 93 and explained how they were able to shut these hackers down
TAP(ping) Out Security Threats at FireEye (Chris Rimondi) With 3.6 petabyte of raw storage, 700 billion events on 400+ nodes and 300k events per second; FireEye’s Threat Analytics Platform (TAP) uses Elasticsearch for analytics and sub-second search in the hunt for cybersecurity events
This talk was mainly focussed on how FireEye stores their data, manage their clusters and how to avoid users breaking their system by running too complex queries
One example given was a regex query searching for some credit card data
This query got 1080 cpu cores spinning at 100% for 83 minutes, turning the system inoperable
In order to avoid this in the future, they’re actively working on several levels (hardware, Lucene, Elasticsearch) to improve the speed with which Elasticsearch can handle queries FireEye’s users fire at their systemIn most latest browsers it is now possible to deliver different images for different devices by only one image element, using the srcset attribute
This is important for preventing small devices loading unnecessarily large images, but still having sharp big images on large (retina) devices
Using srcset Multiple image sources can be suggested to a browser by size or by the device pixel ratio of the browser
Using the x unit (device pixel ratio of the browser) html <img srcset="http://placehold.it/500x150 1x, http://placehold.it/750x150 1.5x, http://placehold.it/1000x150 2x" src="http://placehold.it/500x150" alt="" width="500"> On retina screens with a device pixel ratio of 2, the image will show the source with width of 1000px
Using the w unit (size of the image source) html <img srcset="http://placehold.it/500x150 500w, http://placehold.it/1000x150 1000w" src="http://placehold.it/500x150" alt="" width="500"> Using w tells the browser the width of the image source
The browser can decide which width matches best for the device
The choice depends not only on the browsers viewport, but can depend on bandwidth and device hardware too
How to tell the browser what source it should choose, depending on size AND device ratio
Telling the browser how large an image source is by defining the size in w, is not enough for a browser to choose the best source, when the image is styled with CSS to, for example, 200px
In this case we use the size attribute, together with the srcset
html <img sizes="200px" srcset="http://placehold.it/200x150 200w, http://placehold.it/400x300 400w" src="http://placehold.it/200x150" alt=""> Because now we tell the browser what the size will be, after applying CSS styling, and we specify the width of each source, the browser can choose the best image
The image will always be displayed with a width of 200px, but a retina device will choose the source with 400w and a non-retina device will choose the 200w source
What is exactly the problem
The problem is that images are preloaded by the browser
This is at the moment the DOM is not even build yet
So the browser has no knowledge about the CSS styling which is calculated after the DOM has been build
So we have to give the browser some clue what the size of the image will be, after fully rendered and styled with CSS, for the browser to be able to pick the most suitable source from a list of sources with different widths
The sizes attribute is not the same as the width attribute
The difference lies in the responsive possibilities of the sizes attribute
Responsive fluid design For a responsive and/or fluid design a fixed size of, for example, 200px, does not suffice
A fluid design has sizes relative to the parent element
Which is not known when images are being preloaded, by the browser, before the DOM is ready and the view is rendered with CSS styling
However there is a unit that is known, before CSS is calculated
It is the view width
With the view width we can define a fluid (relative) size of the image
html <img sizes="100vw" srcset="http://placehold.it/200x150 200w, http://placehold.it/400x300 400w, http://placehold.it/600x450 600w, http://placehold.it/800x600 800w, http://placehold.it/1000x750 1000w, http://placehold.it/1200x900 1200w" src="http://placehold.it/300x150" alt=""> Media queries It is even possible to use media queries in the sizes attribute
This is needed when, for example, we have an image that has a width of 33% of the viewport, but on small devices a full width of 100%
html <img sizes="(min-width: 768px) 33vw, 100vw" srcset="http://placehold.it/200x150 200w, http://placehold.it/400x300 400w" src="http://placehold.it/300x150" alt=""> Here the sizes attribute is interpreted as the image being 33% of the viewport when the viewport is larger than 768px, otherwise it is 100%
What is going on
When we resize the browser window when viewing this page in Chrome we see in the next example that the breakpoint is a little surprising
html <img sizes="100vw" srcset="http://placehold.it/200x150 200w, http://placehold.it/400x300 400w, http://placehold.it/600x450 600w, http://placehold.it/800x600 800w, http://placehold.it/1000x750 1000w, http://placehold.it/1200x900 1200w" src="http://placehold.it/300x150" alt=""> When starting with a browser width of 150px on a retina device, Chrome chooses the 400w source as expected
But it switches to the 800w source at a browser width of 283px, which I find rather unexpected! I would have expected the breakpoint earlier
Another inexplicable thing happens at 388px viewport width, switching to the 1200w source
I would have expected it to happen later
I would expect the browser to switch source at viewport widths 200 and 400px
Because, to be able to render a sharp retina image at 201px viewport width, the browser needs an image larger than 400px
The same applies to the 1200w image, which is only needed for a viewport with a width from 401px
So what is going on hereIn my first month at Luminis I got the chance to dive into a data set stored in Elasticsearch
My assignment was easily phrased: detect the outliers in this data set
Anomaly detection is an interesting technique we would like to apply to different data sets
It can be useful in several business cases, like an investigation if something is wrong with these anomalies
This blog post is about detecting the outlying bucket(s) in an aggregation
The assumption is that the data is normally distributed
This can be tested with normality tests
But there is no mathematical definition of what constitutes an outlier
The domain of this data set is that the data is about messages to and from different suppliers
The point of interest is if the number of messages linked to a supplier differs in a week from all the weeks
The same can be done with different time intervals or over other aggregations, as long as they are normally distributed
Also a number field can be used instead of the number of messages in an aggregation (bucket document count)
Or if the success status of a message is stored, that can be used as a boolean or enumeration
The first step is to choose an interesting aggregation on the data that gives normally distributed sized buckets
In the example below I chose a Date Histogram Aggregation with a weekly interval
This means that the data is divided into buckets of a week
Over the weeks the messages to and from the suppliers were evenly sent per week
Actually this results in that the size of the buckets are normally distributed
Then the following mathematical theory can be applied to determine the anomalies
The second step is to determine the lower and upper bound of the interval in which the most of the normally distributeddata lies
This is done by calculating the mean and the standard deviation of the number field
It this case these are calculated over the document count of the weekly buckets
In empirical sciences it is conventional to consider points that don’t lie within three standard deviations of the mean an outlier, because 99,73 % of the points lie within that interval (3-sigma rule)
The lower bound of the interval is calculated by subtracting three times the standard deviation of the mean and the upper bound by adding it to the mean
In Elasticsearch this can be done with the Extended Stats Bucket Aggregation, which is a Pipeline Aggregation that can be connected to the previous aggregation
We set parameter sigma to 3.0
Note that Elasticsearch calculates the population standard deviation by taking the root of the population variance
This is mathematically correct here, because the entire population of messages is stored
(If there would be more data than we have and we want to determine the outliers in our sample, then merely applying Bessel’s correction wouldn’t be enough, an unbiased estimation of the standard deviation needs to be used.) Both steps can be done in the same GET request in Sense
java GET indexWithMessages/_search { "size": 0, "aggs": { "byWeeklyAggregation": { "date_histogram": { "field": "datumOntvangen", "interval": "week" } }, "extendedStatsOfDocCount": { "extended_stats_bucket": { "buckets_path": "byWeeklyAggregation>_count", "sigma": 3.0 } } } } The third step is to do the same query with the same aggregation, but this time only select the buckets with a document count outside the calculated interval
This can be done by connecting a different Pipeline Aggregation, namely the Bucket Selector Aggregation
The *lower* and *upper bound* need to be taken from the *std_deviation_bounds* of the *extendedStatsOfDocCount* Aggregation of the response
In my case these are *11107.573992258556* and *70207.24418955963*, respectively
Unfortunately the buckets path syntax doesn’t allow to go up in the aggregation tree
Otherwise the requests could be combined
It is possible to get the lower bound and the upper bound out of the Extended Stats Aggregation in a buckets path, but the syntax is not intuitive
See my question on discuss and the issue raised from it
java GET indexWithMessages/_search { "size": 0, "aggs": { "byWeeklyAggregation": { "date_histogram": { "field": "datumOntvangen", "interval": "week" }, "aggs": { "outlierSelector": { "bucket_selector": { "buckets_path": { "doc_count": "_count" }, "script": "doc_count < 11107.573992258556 || 70207.24418955963 < doc_count" } } } } } } Now we have the weeks (buckets) that are outliers
Further investigation with domain knowledge can be done with this information
In this case, it could for example be a vacation or a supplier could have done less or more in a certain weekWith popular poster children such as Netflix and Amazon, using microservices-based architectures seems to be the killer approach to twenty-first-century architecture
But are they only for Hollywood coders pioneering the bleeding edge of our profession
Or are they ready to be used for your projects and customers
This session goes over the benefits, but more so the pitfalls, of using a microservices-based architecture
What impact does it have on your organization, your applications, and dealing with scale and failures, and how do you prevent your landscape from becoming an unmaintainable nightmare
This presentation goes beyond the hype and explains why organizations are doing this and what struggles they need to deal withWhen dealing with data that you want to import into elasticsearch, one of the challenges is to store the data in a uniform way
This will make it easier for you to query your data
To store data in a uniform way you often need to transform the data
In this serie of blog posts we will discuss libraries that make you’re life easier when transformations are needed
This blog is about the Google’s phone number library
This library allows you to parse, format, store and validate international phone numbers
Google provides a Java, C++ and JavaScript variant which allows you to use these functionalities on either the front-end or back-end of an application
Parsing single phone number The first thing that you need to do is parse the phone number
This results in a PhoneNumber object, which can be used to retrieve or remove information like country code
We need to provide a default region to the parse method
This will only be used if the number being parsed is not written in international format
The next code block shows an example of the parsing
java String phoneNumberAsString = "088 5864 670"; PhoneNumberUtil phoneNumberUtil = PhoneNumberUtil.getInstance(); PhoneNumber phoneNumber = phoneNumberUtil.parse(phoneNumberAsString, "NL"); The phone number in the example is formatted correctly, but the library is smart enough to recognise other formats as well
So it doesn’t matter if you use white spaces, the country code (with or without plus sign) or any other combination
Parsing text Another option that this library provides is extracting phone numbers from text
Also for this method you need to provide a default region
java Iterable phoneNumberMatches = phoneNumberUtil.findNumbers( "Call Luminis Amsterdam office at 088 5864 670 or at the" + " international version +31 (0)88 58 64 670", "NL"); The returned iterable phoneNumberMatches contains in this example two PhoneNumberMatchobjects
Each PhoneNumberMatch object has a PhoneNumber object which can be used for further actions
Validating With the created PhoneNumber object we can validate if this phone number is a correct phone number
java boolean isValidPhoneNumber = phoneNumberUtil.isValidNumber(phoneNumber); When the provided phone number does not contain the country code and is not recognised as a phone number from the specified default region, the isValidNumber() method will return false
Formatting Now that we have identified that we have a valid phone number, we can format it in a way we want to store it
java phoneNumberUtil.format(phoneNumber, PhoneNumberUtil.PhoneNumberFormat.INTERNATIONAL); The outcome of the formatter will be +31 88 586 4670
Final thoughts The google phone number library contains a few more functionalities like matching numbers or provide geographical information related to a phone number
For more information check out there github page
That’s it for the first part of libraries that can help you transform your data
Hopefully this can be handy in your project
See you next time!Some time a go I started experimenting with c3js, a nice library around the very popular D3 graphics library
It is all about creating nice looking charts with lots of options
When integrating this in an AngularJS application it is logical to create some reusable directives
The next step is to make these directives easily available to integrate in other projects
Nowadays the easiest way to do this in front-end projects is to use Bower
During development GruntJS helps with some tasks you just need to do
In this blog post I am going to introduce you to the library, show you how I used it in my elasticsearch plugin and show some of the GruntJS functionality that help me during development
The c3-angular project The project is hosted on Github under the MIT license
Use it like you want to, comments are welcome and if you have requests or bugs please use the issue system in github
https://github.com/jettro/c3-angular-sample The directives are javascript only
The library is just a wrapper, I do not include any of the required libraries in the project
So this is something you have to do yourself
You have to include c3js, AngularJS, D3js and do not forget the css from c3js
A very basic wrapper for an AngularJS application using the directives looks like this
javascript <!doctype html> <html ng-app="graphApp"> <head> <meta charset="utf-8"> <link href="assets/css/c3.min.css" rel="stylesheet" type="text/css"> </head> <body ng-controller="GraphCtrl" > <c3chart bindto-id="chart" chart-data="datapoints" chart-columns="datacolumns" chart-x="datax"> <chart-axis> <chart-axis-x axis-id="x" axis-type="timeseries"> <chart-axis-x-tick tick-format="%Y-%m-%d"/> </chart-axis-x> </chart-axis> </c3chart> <!-- Load the javascript libraries --> <script src="assets/js/d3.min.js" charset="utf-8"></script> <script src="assets/js/c3.min.js"></script> <script src="assets/js/angular.min.js"></script> <script src="assets/js/c3-angular.min.js"></script> <script src="js/graph-app-directive-2.js"></script> </body> </html> This is not the very basic application where you also provide the data in the directive
This is possible, but usually you obtain the data using an AngularJS controller
In the first element of the directive we configure the scope objects that are attached to $scope in the controller
These are: chart-data, chart-columns and chart-x
The other elements in the directive deal with the configuration of the chart
Notice that we load all the required javascript libraries at the bottom and the css in the head
The next code block shows the javascript code for the controller and the service
javascript var graphApp = angular.module('graphApp', ['gridshore.c3js.chart','graphApp.services']); graphApp.controller('GraphCtrl', function ($scope, $interval,dataService) { $scope.datapoints=[]; $scope.datacolumns=[{"id":"top-1","type":"line","name":"Top one"}, {"id":"top-2","type":"spline","name":"Top two"}]; $scope.datax={"id":"x"}; $interval(function(){ dataService.loadData(function(data){ $scope.datapoints.push(data); }); },1000,10); }); var services = angular.module('graphApp.services', []); services.factory('dataService', function() { function DataService() { var maxNumber = 200; // API methods this.loadData = function(callback) { callback({"x":new Date(),"top-1":randomNumber(),"top-2":randomNumber()}); }; function randomNumber() { return Math.floor((Math.random() * maxNumber) + 1); } } return new DataService(); }); Check the project if you need more information about the different options that you have
Below you’ll see an example of the chart You can checkout this sample and some others in the examples folder of the mentioned Github project
Using Grunt and Bower This project is made available using bower
You can checkout the available versions with the following command
# bower info c3-angular Than you can use it in your bower project using the following command
Beware that you have to include the AngularJS, C3js and D3 libraries yourself in your project
# bower install c3-angular --save During development I have used Gruntjs do to some tasks for me
In the past I use python to start a very basic webserver when I needed one
With grunt this becomes a very basic task as well
Just add the deserver plugin to your project
In my case I can now run the grunt command to start a webserver with the examples folder as the root on the default 8888 port
Below first the command to run and than the configuration of Grunt grunt devserver javascript devserver: { options: { base: 'examples' }, server: {} } Besides compacting and minifying the code I also use Grunt to watch the sources and copy changes to the examples project
That way when changing a javascript file I immediately can watch the browser and test the change
The following code block shows you the copy plugin configuration of grunt
javascript copy: { examples: { files: [ {expand: true, flatten: true, src: ['bower_components/angularjs/angular.min.js'], dest: 'examples/assets/js/', filter: 'isFile'}, {expand: true, flatten: true, src: ['bower_components/angularjs/angular.min.js.map'], dest: 'examples/assets/js/', filter: 'isFile'}, {expand: true, flatten: true, src: ['bower_components/d3/d3.min.js'], dest: 'examples/assets/js/', filter: 'isFile'}, {expand: true, flatten: true, src: ['bower_components/c3/c3.min.js'], dest: 'examples/assets/js/', filter: 'isFile'}, {expand: true, flatten: true, src: ['bower_components/c3/c3.min.css'], dest: 'examples/assets/css/', filter: 'isFile'}, {expand: true, flatten: true, src: ['c3-angular.min.js'], dest: 'examples/assets/js/', filter: 'isFile'}, {expand: true, flatten: true, src: ['c3-angular.min.js.map'], dest: 'examples/assets/js/', filter: 'isFile'} ] }, bysource: { files: [ {expand: true, flatten: true, src: ['c3-angular.js'], dest: 'examples/assets/js/', filter: 'isFile'} ] } } Notice how I copy the libraries from the bower_components to the right positions for the samples
Using this technique I make it possible for you to immediately try out the samples without having to do bower stuff first
The Elasticsearch-gui implementation The next step is to use the directives in another project
In my elasticsearch-gui project I wanted to have charts
Sounds like a good opportunity to try out the directives
You can find the project on github
https://github.com/jettro/elasticsearch-gui In the plugin I give the user the option to choose between a pie chart, and a few different bar charts
This choice is based on the type of elasticsearch aggregation you choose
You have to chose an appropriate field
The sample index I use contains all my music of iTunes
In the pie chart you have an overview based on genre and the bar chart is a histogram with periods of 10 years when the songs were released
This application is a bigger AngularJS application
We make use of routing and therefore we can use partial html documents
I am not going to copy paste the complete document in here
This is kinda big
Also the data retrieval is to big to discuss in this blogpost
Check the source code and if you have question contact me
Check the following 2 sources if you are interested
https://github.com/jettro/elasticsearch-gui/blob/master/partials/graph.html https://github.com/jettro/elasticsearch-gui/blob/master/javascript/controllers/GraphCtrl.js In the next two code blocks I do show you the directive configuration of both the pie chart and the histogram chart
javascript <c3chart bindto-id="chartterm" chart-data="results" chart-columns="columns"> <chart-legend show-legend="true" legend-position="right"/> <chart-size chart-height="400" chart-width="600"/> </c3chart> This is a basic chart with not so much configuration, the next chart is more complicated javascript <c3chart bindto-id="charthisto" chart-data="results" chart-columns="columns" chart-x="xaxis"> <chart-legend show-legend="false"/> <chart-size chart-height="400" chart-width="600"/> <chart-axis> <chart-axis-x axis-position="outer-left" axis-label="Number by {{aggregate.interval}}" axis-type="category"> <chart-axis-x-tick tick-fit="true" tick-rotate="50"/> </chart-axis-x> <chart-axis-y axis-id="y" axis-position="outer-right" axis-label="Document count" padding-bottom="0" range-min="0"/> </chart-axis> </c3chart> Summarising You should now have an idea about what you can do with the c3-angular directives
You have seen how to use it in your project
I am planning on adding more features the coming months
If there is something specific that you would like to see, let me know
references c3js c3-angular elasticsearch-guiIntroduction Yes, this is a somewhat long article
With no images, code samples, fancy screencasts or other visuals
It is just a story about me and my passion for OSGi
Perhaps there will be sequels, because there is still a lot to come
However, these intentions always seem to vanish over time so I am not making any promises
In any case, I felt the urge to share my thus far little experience with real world OSGi
For a real customer with a real product on a small budget
Where software is developed by Average Joe or Jane and not by some cracker jack software guru flying around the world doing talks and publishing books
These people are of course working hard to make our life a better place while we just gloat over travelling abroad and back cover blurbs
So I thought: Lets help these fellows by sharing something about my personal experience with OSGi
Context In short: The customer has a product consisting of a central server with a number of embedded devices connected over the internet
The server is a plain Java application with a Jetty web interface to operate, configure and monitor the system
In the near future, we want to extend the capabilities of the system and will of course result in more network and other domain related complexities
Project When I started working at this project, a rudimentary rest interface for the server was already present and one of my jobs was to extend it to a full working version
Of course extensive testing was required, so I started by developing a simple tool to test a live rest service
It was sufficient to just let it fire a bunch of predefined http calls and log the results
I could extend it by automated validation and other should-haves later on
Eager to show off I started hacking away and the first requests were sent within the hour
Hardcoded urls launched from a static main method was the result but who cares
It was just a simple tool right
After tinkering with Commons CLI for a bit, I started to think about other future needs
Next to a command line interface, repeatable tests or extensive logging could soon be needed
Such requirements were still uncertain and not that essential for the current state of the project
However, I did not want to rule them out by making it too hard to code in at a later stage
After all, these kinds of ‘simple’ programs tend to become an unmaintainable mess overnight
If you are not thinking OSGi already you are not paying attention
What is more is that OSGi would be extremely useful for a new software version on the embedded devices
So the more hands-on experience, the merrier
Tools What you don’t want is vi to maintain manifest files
When it comes to OSGi, Bndtools is the only viable way to go, trust me
Unfortunately only available as Eclipse plugin yet, but the future is bright
Eclipse still has a grudge ever since I abandoned it for IntelliJ and I feel like I am still paying the price
But Bndtools is a loyal companion and creating bundles with correct dependencies, setting up run configurations, debugging and headless build support is a breeze
The first thing I did for my test tool was searching for a bundle version of Apache’s httpclient
Marcel pointed to JPM and suggested to include the required bundles in the local workspace repository and just commit them in Git
For the Bndtools novice: You can set up local and remote repositories for external bundles
This allows you to easily setup package requirements for your own bundles
And JPM is very extensive when it comes to OSGi bundles for much well-known libraries
Service dependencies Sometimes the only reason I want to use OSGi is because I just love using Java interfaces
These files with clear methods definitions always make me smile
Especially when they are defined as: “request(method, url)” or “saveResult()” when you are making a rest test tool
Of course this feeling is quickly gone when you have to implement them, but with OSGi I can delay these ‘trivialities’ until the software has to actually do something
At that point, all is spoiled and you start calling constructors and in OSGi’s case, the Service Registry
To avoid a dependency hazard up front, I started to use Felix’ dependency manager
With a fluent API I was managing services like a skilled puppeteer
That is until you try to set up the run configuration and Bndtools is giving you a bit of a cryptic error when you hit the resolve button
Apparently I was missing some packages required by the dependency manager
Are these like meta dependencies
Anyhow, the video tutorials from Amdatu provided valuable help
It worked, but it was not ideal for a software engineer wanting to show off and just needed a quick fix
Whats wrong with the endless blogs with code samples and screenshots people used to write
😉 Apparently my colleague thought this as well
I started this story with the need for a command line interface, but until now it was still missing
Luckily, the Gogo shell of Felix provided just what I need
Next to basic bundle lifetime management, Gogo allows a simple method to implement your own commands
It is just a matter of setting some properties and adding some annotations
However, like most of these ‘easy-as-pie’-solutions, you have to invest a bit or more to learn them
Afterthought OSGi’s service layer is easy to understand, so for me it is mostly about figuring out the compendium and other service definitions
These libraries all differ in (documentation) quality
But then, how is that any different from plain Java libraries
I now have a simple test tool which I can easily extend with all kinds of result logging
I could add another UI or some automated way to launch the rest calls
All thanks to small and loosely coupled services
It kind of feels like ice skating on a lake: The joy of freedom and adrenaline you only understand when you feel the ice rushing under your skates
Don’t be afraid to fall!Welcome to my first blog post for Luminis! This time I am not going to blog about .Net related stuff, as I started working on a project which uses all the above technologies together
It took a little time to get my head around it all and I bumped into quite a few things when combining these cool frameworks, so I wanted to share my findings with you all
First off: Typescript and Angular together Typescript is a cool new language that makes it a lot easier to write structured and object oriented JavaScript
If you come from a language like C# or Java you will feel right at home
It has classes, interfaces, strongly typedness and a lot more cool stuff to write your applications
Another cool feature is that it compiles to just JavaScript! So if you have been doing a lot of object oriented JavaScript, using the revealing module pattern or class patterns to write classes and modules you will be pleased to hear that typescript takes that pain away
I am not going to give a complete typescript tutorial or description, you can find those here http://www.typescriptlang.org/
Now take Angular, the cool MVC
MVVM JavaScript framework
Angular has modules, databinding, controllers, services, all the cool stuff we use in object oriented languages to separate our concerns
If you come from C# and XAML, Angular can definitely be compared to PRISM, only for JavaScript
And that is exactly my problem, you have a cool structured framework like Angular, with an unstructured, non-oo language like JavaScript
Let’s combine the two! Let’s create a very simple Angular web application
Almost no UI, just to understand what is going on
The tooling I am going to use is Visual Studio 2013, it get’s Angular and Typescript, but you could also use Web storm for example
Typescript is completely open source and already has a lot of support
In the figure below you see my attempt to set up a simple hello world controller, you can see Visual Studio, which is great tooling, does not has a lot of intellisense to offer me, even though I reference angular with an intellisense comment
This is due to the dynamic nature of JavaScript
In a minute I will show you the complete app, but this tooling trouble, is one of the problems Typescript aims to solve so stay with me 
Now on to the complete example
html <html ng-app="appModule" lang="en"> <head> <meta charset="utf-8" /> <title>Typescript HTML App</title> <link rel="stylesheet" href="app.css" type="text/css" /> <script src="Scripts/angular.js"></script> <script src="Home/js/appController.js"></script> </head> <body ng-controller="appController as ourController"> <h1>Typescript Angular App</h1> <div id="content">{{ourController.sayHello()}}</div> </body> </html> Notice a couple things about this html
First of the multiple script references
This can become a hassle
The controller has to come after Angular
The order of the scripts matter, when I create a lot of files and dependencies this becomes hard to manage
Will solve this later using RequireJs
Further more on line 8 I am using the as syntax to define a controller
This allows us to bind to properties defined on our class instead of properties defined on our $scope
Let’s have a look at the JavaScript: javascript /// <reference path="....Scriptsangular.js" /> var HelloWorldController = (function () { function HelloWorldController() { } HelloWorldController.prototype.sayHello = function () { return "Hello world!"; }; return HelloWorldController; })(); var module = angular.module('appModule', []); module.controller('appController',HelloWorldController); Notice the whole anonymous function part
This is the JavaScript pattern to define a class
In JavaScript a class is basically just another function
It get’s created in an anonymous function to prevent the global scope from polluting and in this anonymous function it also get’s populated with variables and methods
This is a lot of work for just a class! Let’s convert this sample to Typescript! javascript class appController { constructor() { this.name = "Chris"; } name: string sayHello(): string { return "Hello " + this.name; } } Look at this! If you come from C# or Java or C++ even you will feel right at home! Very nice
And this just compiles to plain JavaScript, so in the browser you will just reference the same .js file as before
Now let’s make use of angular
The controller still needs to get registered in Angular
As Typescript is a superset of JavaScript we can do the Angular registration below the class definition, or in a separate typescript file
The only problem is that we will get compiler errors as the typescript compiler does not know about Angular or, other normal JavaScript libraries
To get Typescript to play nice with Angular and other JavaScript libraries you can make use of so called definition files
These are basically descriptions of a JavaScript library so that the Typescript compiler knows which functions there are, what they return and parameters they have
You can create these, there is a big tutorial on these on typescriptlang
Luckily most of them are already there and you can download them from https://github.com/borisyankov/DefinitelyTyped
When you use Visual Studio 2013 you can right click on a JavaScript file and download the files from there
Let’s update our code with an Angular definition file
I created a screenshot of my Visual Studio just to let you see how cool this is 
In the top of the file I reference the Angular definition file
Now Typescript knows of the angular variable and gives me complete intellisense about types, function etc
You can see this function returns an ng.IModule instance, on which we also get complete intellisense
Here is the complete code
Keep in mind that this is just compile time
Run time the scripts still need to be included in the right order to make sure the angular global exists before our controller registration
javascript /// <reference path="../../scripts/typings/angularjs/angular.d.ts" /> class appController { constructor() { this.name = "Chris"; } name: string sayHello(): string { return "Hello " + this.name; } } var appMpdule: ng.IModule = angular.module("appModule", []); appMpdule.controller("appController", appController); What is also cool, is that if define parameters in our constructor, they will get inject by angular
We could let inject Angular all kinds of registered services just like with normal controller functions! Let’s create a cool alert service which our controllers can use
Here is the code for our alert service
javascript class defaultAlertService implements alertService { showAlert(text: string): void { alert(text); } } interface alertService { showAlert(text: string): void; } var servicesModule: ng.IModule = angular.module("services", []); servicesModule.factory("alertService", () => new defaultAlertService()); Our service implements an interface, so we can easily switch it for a service which uses bootstrap for example
Also on line 14 you can see the angular registration
Yes Typescript has lambda expressions! The difference with a normal anonymous function is that the lambda keeps the this pointer of the parent function instead of a new scope
Now the service needs to get injected in our controller
Here is our controller with it’s new showAlert function
It get’s called when someone clicks a button
The html will follow later
javascript /// <reference path="../../scripts/typings/angularjs/angular.d.ts" /> class appController { constructor(private alertService:alertService) { this.name = "Chris"; } name: string sayHello(): string { return "Hello " + this.name; } showAlert(text: string) { this.alertService.showAlert(text); } } var appMpdule: ng.IModule = angular.module("appModule", ["services"]); appMpdule.controller("appController", appController); On line 5 you see a cool Typescript construction
For a constructor parameter that has a modifier Typescript will automatically emit a class variable
If the name of our parameter is the same as the name of the registered service Angular will just inject it
If not, when you use minimizing you have to use another inline annotation for it to work
You can also see the dependency on the services module when loading our appmodule
Keep in mind that this is not a file dependency
That is still up to us
Look in the html next, there we will need to include the scripts in the right order
Angular –> Service –> Controller
It is simple right now, but this can quickly grow and will lead it’s own life
html <html ng-app="appModule" lang="en"> <head> <meta charset="utf-8" /> <title>Typescript HTML App</title> <link rel="stylesheet" href="app.css" type="text/css" /> <script src="Scripts/angular.js"></script> <script src="Services/alertService.js"> </script> <script src="Home/js/appController.js"> </script> </head> <body ng-controller="appController as ourController"> <h1>Typescript Angular App</h1> <div id="content">{{ourController.sayHello()}}</div> <button ng-click="ourController.showAlert('Hello world!!')">Press me</button> </body> </html> You can see, starting on line 6, the script tags
They have to be in this order or the app will break
On top of that, the browser will load all these scripts, even if there are services included that the user does not need because he does not touch the functionality tat requires these services
Enter RequireJS Typescript and RequireJS Let’s take a look at RequireJS
This is a library that adds modularization to JavaScript applications
Uhm wait, modularization
Doesn’t Angular also have modules
Yes it does! But Angular has logical modules
Angular does not modularize the file loading for example, or the handles the file dependencies as you can see in our index.html
Require can work neatly together with Angular modules, keep in mind that both libraries solve different problems
You can find everything about RequireJS here: http://www.requirejs.org/docs/api.html
let’s have a look
I am going to add Require to my application
The first thing that will change is my index.html
Two things that stand out are there is only one script tag left in our html
That is the reference to Require
As Require will manage all our script loading, this is the only thing we need in our main html page
The second thing is the data-main attribute on the script tag
This tells Require where it should start with executing
Very comparable to a main function in C for example
next we have a look at the contents of our main file
javascript require.config({ paths: { 'angular': '/Scripts/Angular/angular' }, shim: { 'angular': { exports: 'angular' } } }); // Start the main app logic
require(['angular', '../Home/js/appController'], function (angular) { var elem = document.getElementsByName("html")[0]; angular.bootstrap(elem, ['appModule']) }); First in our main js file, I call require.config
We can use this function to give require some extra config stuff
Like short names for libraries
My call tells Require that whenever I want to use angular, it can be found by following the path I pass to it
On line 8 you see another cool thing
It’s a so called shim
Require loads JavaScript files and sees them as modules
But those JavaScript files ideally should know that they contain Require modules
When defining an Require module a JavaScript file should tell Require what it is exporting and on what other modules it depends
We shall have a look at that later
The problem with angular is that this is an existing JavaScript library that does not contain Require modules
Angular just populates the global scope with an angular variable
By using a shim, we tell Require that when angular is done loading, it should clean up the global angular variable and instead pass it to the function on line 17
This function get’s executed when angular is done loading, and it’s dependency, our appController is done loading
You can see that this call to require, executes and when all the modules in the parameters are done loading
If you are paying attention you are probably wondering why we don’t have to shim our appController, as I said earlier that JavaScript files should tell require what modules they are exporting
This is because I made a little modification in our appController
Here is the code javascript /// <reference path="../../scripts/typings/angularjs/angularreq.d.ts" /> /// <amd-dependency path="../../Services/alertService"/> import ServicesModule = require("../../Services/alertService"); export class appController { constructor(private alertService:ServicesModule.alertService) { this.name = "Chris"; } name: string sayHello(): string { return "Hello " + this.name; } showAlert(text: string) { this.alertService.showAlert(text); } } import angular = require('angular'); var appMpdule: ng.IModule = angular.module("appModule", ["services"]); appMpdule.controller("appController", appController); Couple of cool things here
On line two you can see a typescript statement that is unknown to us
This is an undocumented feature of typescript
This statement allows us to specify the Require dependencies of our current module
officially, the import statement should also take care of this, but this doesn’t work when I am not using the module in a statement in my code, I only use types from the module to type parameters, these won’t be there in the resulting JavaScript so Typescript decides it does not need my module
the fix is to also specify the <amd thingy.
On line 4 you can see the import statement
This makes sure I can use types from the module and the module itself in my code
On line 6 you can see an export keyword
This instructs the Typescript compiler to define a Require module that exports our controller
On line 16 you see that we need to import angular to make use of Angular
Just as with our alertService we want to make use of the angular module, so we add an import statement
This leads to compiler errors however
Because unlike our alertService, the angular library is no typescript, and the angular.d file does not tells the compiler that angular in our project get’s loaded as a Require module
So we need to write a new definition file that tells typescript that angular is loaded as a Require module, and from that module exports the angular variable
Basically we need to make Typescript aware of our Require configuration in our main.js file
Here is the .d file to do that
javascript /// <reference path="angular.d.ts" /> declare module "angular" { export = angular; } You can see the file getting referenced on line 1 or our controller and also in our alertService
Because this file get’s referenced Typescript knows of the angular module that exports the angular variable
Just as the shim configuration for require in the beginning of this post
The alertService is defined below javascript /// <reference path="../scripts/typings/angularjs/angularreq.d.ts" /> export class defaultAlertService implements alertService { showAlert(text: string): void { alert(text); } } export interface alertService { showAlert(text: string): void; } import angular = require('angular'); var servicesModule: ng.IModule = angular.module("services", []); servicesModule.factory("alertService", () => new defaultAlertService()); Nothing strange here
It is just like our controller
We use export to export the different types from this module
But there is still something strange happening
When we load our application, I get no errors at all, but it also isn’t working correctly
The problem here comes from the fact that Require loads and bootstraps angular before the DOM is ready
What we want, is a way to tell require load the modules as soon as the dom is ready
Fortunately there is a way to this
This is actually a RequireJS plugin called domReady
It is really cool
Just download the domReady.js file
And make the domReady a module dependency of your main module
Here is the modified main.js
javascript require.config({ paths: { 'angular': '/TypeScriptAngular//Scripts/Angular/angular', 'domReady': '/TypeScriptAngular//Scripts/domReady' } , shim: { 'angular': { exports: 'angular' } } }); // Start the main app logic
require(['domReady', 'angular', '../Home/js/appController'], function (domReady, angular) { domReady(function (doc) { angular.bootstrap(doc, ['appModule']); }); }); You can see that when you add domReady as a dependency, it will give you the current document as a parameter to your module function
So now we are done! Everything works, is object oriented by using typescript, is MVC’d by using angular and asynchronously loads our modules and dependencies when we need them
Typescript makes working with require a lot easier, but you do have to know how Typescript accomplishes this, and how to make use of the .d files
Here is the code by the way! See you next time! TypeScriptAngular.zipIn the previous installment of this series of articles on the Felix Dependency Manager, we showed how the DependencyManager can help to handle required service dependencies
A required service dependency means that your component cannot act without it and the DependencyManager ensures that your component is only started when the required service is available
Likewise it stops your component when the required service is not available anymore
And of course, the DependencyManager injects the required service into your component, so you can use the beloved inversion-of-control principle to keep your component’s implementation free of handling dependencies
Optional service But what if your component just wants to use a service when it’s available, but can perfectly live without it
In other words: if the dependency on the service is optional
A good example of an optional service dependency is a log service: usually you don’t want your software to stop functioning, just because there is no logger
We’ll continue our running example and extend the temperature converter service with logging: apache private volatile LogService logger; public int getTemperature() { logger.log(LogService.LOG_DEBUG, "getTemperature is called"); ..
} As before, we need to specify the dependency in the init() method of the Activator; we just create a second ServiceDependency and set required to false: apache @Override public void init(BundleContext bundleContext, DependencyManager dependencyManager) { Component component = ..
component.add(createServiceDependency() .setService(LogService.class) .setRequired(false)); dependencyManager.add(component); } Note that you can add as many dependencies as you want
However, make sure that you add the dependencies to the component before you add the component to the DependencyManager (the last line in the code fragment above)
Otherwise, the DependencyManager might already have started your component (triggered by the dependencyManager.add(component) call) while you’re still adding additional dependencies, which might lead to confusing behavior (your component being started and stopped even before the init() call has returned)
Let’s see how this revised example behaves with respect to the availability of a LogService
In the sample we used the standard OSGi LogService, of which you can find many implementations on the internet
For the demo, the OSGi Screen logger is very convenient
Just download it and install it in Felix; as soon as you start the bundle a logger window will appear
For the demo,DependencyManager we’ll need something that calls our service; in the sample repository on bitbucket you’ll find a display bundle that presents a tiny, but functional, UI for displaying the temperature
If you installed and started it all, you’ll see something like this: If you hit the refresh button on the temperature display, you’ll notice an extra log message appearing in the logger window
To see what’s happening when the log service becomes unavailable, stop the logger bundle; the log window will disappear
Now hit the refresh button on the temperature display again, and you’ll see that the temperature service is still functioning normally; it’s not affected by the LogService‘s absence
Null, but not really That the service is still running when the (optional)log service is not available anymore, won’t surprise you, but you might wonder why the logger.log(…) call in the getTemperature() method does not throw a NullPointerException
It will even become more funny when you add a line like this apache System.out.println("The value of the 'logger' field is now: " + logger); and come to the conclusion that it prints apache The value of the 'logger' field is now: null WTF!
This surprise is powered by the Null Object pattern
For your convenience, the DependencyManager will inject a null object for each (optional) service that is not available, so you don’t have to pollute your code with if-not-null checks
Of course the injected null object implements the same interface as an actual service implementation would do
While very convenient in a lot of cases, the null object injection might lead to unexpected behavior when the null-implementation does not conform to the interface specificiation
For example, suppose we have a service interface that returns a list: apache public interface Foo { /** * @return all bars in the system; returns an empty list when there are not bars */ List getBars(); } The javadoc indicates that the method should never return null, so clients of this interface will never have to do any null checks on the result obtained from getBars()
However, the null object implementation that is injected by the DependencyManager is not that smart (it can’t read javadoc): it will simply return null
Of course, this is absolutely not what you want, because it would force you to introduce these ugly null checks again (which readers of the source code wouldn’t understand, because wasn’t the interface required to never return null???)
To overcome this, you can provide your own null object implementation, and ensure that getBars() will always return a list
We’ll demonstrate the use of a custom null object in our running example, in which the null object will do something usefull instead of being a no-op implementation
When there is no logger, we’d like to write the log messages to system out directly, like this: apache public class SysoutPrinter implements LogService { @Override public void log(int level, String message) { System.out.println(message); } ..
Note that the DependencyManager uses the term “default implementation” for a custom null object (which is actually quite right in our example, as the SysoutPrinter is doing more than you’d expect a null object to do)
apache // in Activator.init: component.add(createServiceDependency() .setService(LogService.class) .setRequired(false) .setDefaultImplementation(new SysoutPrinter())); // Set custom "null object"-implementation If you add these lines and update the bundle, you’ll see that the log message either appears in the logger window (when the log service is started) or in the console (when it’s not)
Nice isn’t it
Still, there is a second option: you can switch off the use of a null object (or default implementation) completely
But before we’ll go into that, we need to talk about callback methods
Callback methods The DependencyManager has two ways for dependency injection
The first is injecting the service into a member field, which you’ve seen being used in our examples so far
The DependencyManager searches the implementation class for a member field of the right type and injects the dependency into that field
In this case, the “right type” means the same type as the interface the dependency was declared with
Usually, this algorithm works perfectly well, but you don’t have to be a rocket-scientist to think of an example where it doesn’t work out
For those cases, you can set the name of the field to inject too; we’ll come to that in a minute
The second method for dependency injection is with callback methods
Simply pass the names of the callbacks to the ServiceDependeny with the setCallbacks method: apache // in Activator.init: component.add(createServiceDependency() .setService(LogService.class) .setRequired(false) .setCallbacks("setService", "unsetService") Note that you setting and unsetting (or adding and removing as we’d like to call it), requires two separate methods, which might differ from other dependency injection mechanisms that you’ve seen (that simply call a setter with a null argument to unset)
We’ll explain the reason in the next installment about multiplicity
You implement the callback methods in your implementation class
For the number and type of parameters, you can choose various variants, but for now the following will do: apache // "add service"-callback private void setService(LogService newLogger) { logger = newLogger; } // "remove service"-callback private void unsetService() { logger = null; } You can use any access modifier you like; we encourage you to use private because it avoids other classes accidently calling these methods (and the DependencyManager has no problem calling private methods)
If you use callbacks like the ones above, you have to do null-checks for optional services the moment you use them
A common mistake, is to check like this: apache if (logger != null) { logger.log(LogService.DEBUG, "yes, logger is functioning!"); } which will fail miserably when the service happens to be unset just between the condition and the log-statement (remember that clients call services on their own (clients') thread)
There are several ways to solve this: either synchronize all access to the member field, apache private synchronized void setService(LogService newLogger) { ..
} private synchronized void unsetService() { … } // in any method that uses the logger: synchronized { if (logger != null) { logger.log(LogService.DEBUG, "yes, logger is functioning!"); } } or create a temporary copy of the member before the null check, and make the member volatile: apache private volatile LogService logger; // in any method that uses the logger: LogService currentLogger = logger; if (currentLogger != null) { currentLogger.log(LogService.DEBUG, "definitely save to use the logger here."); } By now you’ll understand why the member field has to be volatile when you use member injection: the DependencyManager has no way of knowing how you implemented synchronized access to the member (e.g
which Object you’d use for synchronization) and even though object assignment is atomic in Java, the Java memory model requires use of the volatile keyword for data synchronization (refer to java memory model if you want to know more)
Of course, you can do anything you like in the callback methods and that’s usually why people use them: because more things have to be done than just injecting a dependency (notifying other classes, sending events, updating administration, etc.) – we’ll have a nice example at the end of this article
However, it’s important to realize that if you use callbacks, member injection is not done anymore
And of course, as the service is not injected, the null-object or default implementation aren’t either
Auto-config Now let’s return to the question how to disable the use of the null object pattern
You can explicitely do this, by switching off the so-called auto-configure option: apache component.add(createServiceDependency() .setService(LogService.class) .setAutoConfig(false) …); which disables both member and null-object injection
As you would have guessed by now: the default value for the autoConfig property is true (auto configure is on)
If you use callback methods (like we did above), auto-configure is switched off automatically
If you expect ambiguities w.r.t
the injection field, you can explicitely set it too; and, surprisingly, with the same method: apache component.add(createServiceDependency() .setService(LogService.class) .setAutoConfig("injectedLoggerService")); This will inject the LogService service a member field named injectedLoggerService (if available)
Naturally, setting the name of the to-be-injected field, switches on the auto-config option (if you’d switched it off before)
As explained above, specifying callbacks disables auto-config automatically, However, there is an escape which enables you to use both at the same time: set the auto-config on explictly before settting the callbacks, e.g.: apache component.add(createServiceDependency() .setService(LogService.class) .setAutoConfig(true) .setCallbacks("addLogger", "removeLogger")); Callback example We conclude with an example that both uses auto-config and callbacks
It’s based on the temperature display bundle that accomponied the temperature samples
If you press the button, it displays the temperature; unless there is no temperature service in which case it clears the output box
From a usability point of view, it would be much better if the UI indicates that the service is not available, e.g
by disabling the button (and clearing the output box of course)
This functionality is a perfect fit with the callback methods
As we still need the service itself being injected too, we use the combination of auto-config and callback methods as explained above
As before, you can find the complete source code on bitbucket
We add callbacks for setting as well as for unsetting the TemperatureService: apache private void serviceAdded() { SwingUtilities.invokeLater(new Runnable() { @Override public void run() { enableUI(true); updateUI(); } }); } private void serviceRemoved() { SwingUtilities.invokeLater(new Runnable() { @Override public void run() { enableUI(false); } }); } In serviceAdded method we enable the UI and we trigger the updateUI method, which will call the injected temperature service to obtain the current temperature and displays it in the UI
As explained above, we call both setAutoConfig and setCallbacks on the ServiceDependency, when setting up our DependencyManager component: apache .add(createServiceDependency() .setService(TemperatureService.class) .setAutoConfig(true) .setCallbacks("serviceAdded", "serviceRemoved")) This concludes our discussion about optional services
As before, please comment if you have any questions
Next time, we'll dive into injecting services with multiplicityThe Apache Felix Dependency Manager is a great tool that can mean a lot for ease of development of OSGi applications
Unfortunately, its documentation is not complete and probably generally a bit to concise for the OSGi beginner
This is really a shame, as it might keep people from using it, whilst they might benifit a lot from it
So i decided to write a series of articles about it, in an attempt to make it more accessible for the not-yet-so-experienced OSGi developer
Of course, at the moment of writing, this is still only a plan 😉 – but you can help me to stick to my plan by sending supporting comments and feedback
In this first article, we’ll start with a basic example
It might not be the most simple example, but it will show the “business case” for the DependendyManager
As the name suggests, it is all about dependencies
In a dynamic service architecture like OSGi, one should take into account that services can come and go dynamically
Of course, this holds for any Service Oriented Architecture, but it is often ignored
Handling such dynamics can be quite cumbersome, and this is exactly where the DependendyManager helps out
An example For the example, we’ll create a service that simulates a hardware device and second service that depends on it, in order to deliver some useful data
Don’t conclude that OSGi is only useful for controlling hardware, it’s just that it is a sample that is easy to understand
For the hardware simulating service, we’ll take the example of a temperature sensor
It provides a numerical value that represents the temperature of the sensor’s environment
To obtain the temparature in Celcius, the value has to be scaled according to a lineair function; this will be the responsiblity of a second service
The dynamic nature of this example lies in the fact that the hardware might not always be present
The sensor might be unplugged, maybe the hardware device needs external power supply that is (temporarily) switched off, or maybe the hardware is just broken
The point is: you cannot predict when the hardware is there, and if it’s there, you cannot rely on the fact that if will be there forever
So if you write a service that provides the temperature in Celcius, you must handle the case that the hardware is gone, or not “delivering”, and the best way to express this fact is to retract (unregister) the service
(If you’re not sure that this is the best way: what else could you do
Return a default or bogus value
That would be fooling the clients
Throw an exception
At least that would send the message that something is wrong, but what (usefull thing) can the client do with this exception
By withdrawing the service, you send (in time!) a very clear message (“i am not available anymore”) and the client can look for another service providing a similar service.) Let’s switch to code now
If you are a familiar with OSGi, the implementation of the hardware simulating service will not contain any surprises
The essence is outlined below and you can find the complete source code of the sensor bundle bitbucket
apache public class Activator implements BundleActivator { @Override public void start(BundleContext context) { context.registerService(SensorService.class, new SensorSimulator(), null); } private static class SensorSimulator implements SensorService { /** * Returns the measured value of the sensor
*/ public int getMeasurement() { return ..
} } @Override public void stop(BundleContext context) {} } It publishes a service called SensorService, which contains just one method: getMeasurement(), that simulates retrieving a sensor value from the hardware
You can find the complete code on bitbucket, as well as a pre-build bundle (directions for how to deploy the bundle in the Felix OSGi container at the end of the article)
Dependency injection The second bundle will provide the temperature-in-degrees-celcius service
Its service interface contains one method too: apache public interface TemperatureService { /** * Returns temperature in degrees Celcius */ public int getTemperature(); } The interesing part is of course how this second bundle manages its dependencies, i.e
keep track of the presence/absence of the SensorService that is needed to compute the actual temperature in degrees Celcius
This is all done in the Activator‘s start() method
We start with creating a DependencyManager component, the building block the DependencyManager can manage dependencies for: apache public class Activator extends DependencyActivatorBase { public void init(BundleContext bundleContext, DependencyManager dependencyManager) { Component component = dependencyManager.createComponent(); ..
} } You’ll notice two things in the code fragment above: we define a method called init instead of start, and the activator class is derived from DependencyActivatorBase
Actually, that class is the one implementing the Activator.start() and it calls the init() in the derived class (the well-known template pattern)
The base class will create a DependencyManager object for us, and hands it over to the init method
Next, we’ll declare the dependency on the SensorService: apache ..
ServiceDependency dependency = createServiceDependency(); dependency.setService(SensorService.class); dependency.setRequired(true); component.add(dependency); By setting the required flag to true, we express that we can't do without and instruct the DependencyManager not to start our component, when that service is not yet there
And of course, to stop our component when the service goes away
Now, let's add a real implementation to the component and add the component to the DependencyManager to get things started: apache component.setImplementation(TemperatureServiceImpl.class); dependencymanager.add(component); This is all we need, to have our component’s lifecycle tied to the availability of the SensorService
Once it appears the DependencyManager will instantiate an object of the class TemperatureServiceImpl and try to inject the required service into it
We say “try”, because the implementation must have a member variable to inject it in of course, e.g.: apache public class TemperatureServiceImpl implements TemperatureService { // Injected by DependencyManager, see Activator.init() private volatile SensorService sensor; ..
} Notice that the member must be declared as volatile (we’ll elaborate on that in a later article)
Temperature service We’re not completely finished yet, as we need more behaviour than just have our dependency injected: we want our service to be registered too
This turns out to be a really simple and minimal addition we have to make to the activator: just add apache component.setInterface(TemperatureService.class.getName(), null); Note that this works because we’ve set the implementation class before, and that our implementation class implements the TemperatureService interface
So when the required dependency becomes available, the DependencyManager will start our component and register the service it implements
Similarly, when the required dependency disappears, our service will immediately be unregistered by the DependencyManager
You might wonder what ‘start the component’ exactly means
Just add a start and stop method to the TemperatureServiceImpl class that prints something to System.out, e.g
apache void start() { System.out.println("TemperatureServiceImpl started"); } and you'll see what's going on
If you dislike the names of the start and stop method, or if the names clash with existing methods, you can name them differently - i'll explain the details later
Note that the DependencyManager calls these methods by reflection, thereby enabling the flexibility to name them differently
One more thing: most people that use the DependencyManager will write it a bit differently: apache Component component = dependencyManager.createComponent() .setInterface(TemperatureService.class.getName(), null) .setImplementation(TemperatureServiceImpl.class) .add(createServiceDependency() .setService(SensorService.class) .setRequired(true)); Don’t be offended by this fragment
It’s exactly the same as the more verbose lines explained above; it’s just smart use of the fact that most Dependencymanager methods return something usefull instead of void (the builder pattern)
Sample deployment in Felix To run the samples in an OSGi container, follow the steps below
Of course, the samples will run in any OSGi compliant framework, but we’ll use Apache Felix for the demo
download Felix from http://felix.apache.org/downloads.cgi#framework start it with java -jar bin/felix.jar (in the felix-framework home dir) download the DependencyManager from http://felix.apache.org/downloads.cgi#subprojects (you’ll only need the “Dependency Manager” jar) install it by issueing the following command on to gogo shell ("g! "): install file:/path/to/org.apache.felix.dependencymanager-3.1.0.jar download the OSGi compendium jar (containing all standard OSGi service interfaces) from http://mvnrepository.com/artifact/org.osgi/org.osgi.core/4.3.0 install that too download the sample bundles from bitbucket install each of the sample bundles by issueing the following command on to gogo shell ("g! "): install file:/path/to/bundle.jar type lb at the gogo shell to see all installed bundles: apache g! lb START LEVEL 1 ID|State |Level|Name 0|Active | 0|System Bundle (4.2.1) 1|Active | 1|Apache Felix Bundle Repository (1.6.6) 2|Active | 1|Apache Felix Gogo Command (0.12.0) 3|Active | 1|Apache Felix Gogo Runtime (0.10.0) 4|Active | 1|Apache Felix Gogo Shell (0.10.0) 5|Installed | 1|Apache Felix Dependency Manager (3.1.0) 6|Installed | 1|osgi.cmpn (4.2.0.200908310645) 7|Installed | 1|Dependency Manager Sample 1 - sensor service (1.0.0) 8|Installed | 1|Dependency Manager Sample 1 - temperature service (1.0.0) 9|Installed | 1|Dependency Manager Sample 1 - temperature display service (1.0.0) g! let Felix resolve (wire) all bundles by typing resolve 5 6 7 8 9 (assuming your bundles have the same bundle id as displayed above; if not, change command accordingly) type inspect c service 7 8 9 and you’ll see that the bundles haven’t published any service yet (of course, as the bundles aren’t even started) start the temperature bundle (8) and inspect again: still no services, which is correct, as the temperature depends on the sensor now start the sensor bundle (7) and once you inspect, you’ll see that now both bundles have their service published play with starting and stopping the bundles in different orders and see how the dependency manager will take care of publishing / retracting the temperature service you can start the display bundle also to have a tiny user-interface for calling the temperature service Conclusion By now, you have seen the basic use case for the DependencyManager: by declaring that our TemperatureService depends on the SensorService we get several things for free: the DependencyManager injects the SensorService into our implementation (TemperatureServiceImpl), the DependencyManager publishes our service (the TemperatureService) when the SensorService is available, and unregisters when it becomes unavailable, the DependencyManager notifies our implementation that its service is published / unregistered, by calling start versus stop, our implementation (the TemperatureServiceImpl) does not contain any OSGi related code, so it’s framework agnostic (which is usually consider a good thing) And all of this only for the cost of a few lines of (declarative style) code in our Activator
Meet the power of the Apache Felix DependencyManager! That’s it for today
Please respond if anything is unclear, and I’ll try to clarify
In the next article, we’ll talk about optional dependencies and callbacks, stay tuned!OSGi Configuration Admin command line client restored It has been lost for a long time, but i finally restored the Luminis OSGi Config Admin command line client! I can here you think
“Restored
Lost
What was this guy doing?” It’s a long story, and I don’t even know all the ugly details myself, but we (Luminis) used to host our open source projects on a separate server, with all that nice Atlassian collaboration tools like Confluence and Jira
It was a hosted environment, so we didn’t have to manage anything
Well, that’s what we thought
One of my favorite oneliners is: “the fact that the chances are low, doesn’t means it’s never happening”
How true
The server crashed and it turned out there was no backup
Oops, that hurts
Of course, i’m lazy, so the latest source was still on my laptop
But only the latest
Not a big deal, but I hate losing history
Worse: i never checked-in the documentation, because I’d written it right into Confluence; after all, that’s what a wiki is for, right
You know the feeling when your word processor crashes and makes you lose work: yes, you can recreate it, but you always end up with the nasty feeling that the original version was better written
I hate that too
It took me a few hours of digging the internet, but finally i found some traces of my docs in Google’s cache; enough to restore it quite easily
The source history is lost forever, but well, what the hack
I swore this will never happen again, so i made two promises
One: i’ll never use subversion again
I was already using Git now and then, but I never realized that having the whole repository on each system has more advantages than just speed and being able to work offline
Two: i’d better keep the documentation with the source
You can find the source and the renewed docs on bitbucket
All in one git repository
Please clone it, so I’ll have one more backup when something terrible happensSome time ago, I created the “TopThreads” plugin for JConsole, that helps you to determine why your Java application is causing such a high CPU load, by showing the most busy threads in your application and giving you the opportunity to inspect thread-stacktraces at the same time
It turned out to be quite usefull and from the responses I got, I can tell people find it still usefull today
A few days ago, I released a new version of this plugin, with one very usefull new feature: CPU usage
Top thread
If you’ve used the topthreads plugin, you probably seen this before: suddenly, a thread that is not supposed to be very busy, pops up at the top of the table with usage figures in the 90’s
You wonder WTF is going on, that this thread is taking so much CPU power, until you realize that this figure is only relative to the rest of the application threads
And if the application is hardly doing anything, threads that do a little more than anything might get alarming high figures (and red color)
After i ran into this pitfall a few times, i decided i needed to know an absolute usage figures too
If you enable this feature (settings -> show process cpu usage too), the top row of the table shows the CPU usage of the process as a whole
This is simply the sum of the CPU usage of all threads
The percentage shown in this row however, is the percentage this process is using the CPU, which should be approximately the same value a process viewer like top, Activity Monitor or the TaskManager would report
Although this is not always the case – more about that in a minute – it’s at least a good indication whether the process is busy or idle
And even though it may not always be as acurate as i would like it to be, it proved itself to be proficiant to help me avoid confusion
The usual suspect: the garbage collector In normal situations (whatever that me be… ;-)), the CPU usage figure is approximately the same as the figures other tools report
However, especially when the process is very busy, the CPU usage shown is far too low
After some testing, i’m rather confident that this is mainly caused by the garbage collector
As it turns out, TopThreads does not get information for all the JVM threads, which can easily be verified by comparing a thread dump with the thread listing in JConsole
For example, threads that never appear in JConsole (not in the TopThreads tab, but neither in the JConsole thread view) are the “Low Memory Detector”, compiler threads (HotSpot), “Signal Dispatcher” and “Surrogate Locker Thread (CMS)” and the garbage collector threads (the mark-and-sweep thread and the parallel gc threads)
I can image that some of these threads can put a lot of load on the CPU when the application is very busy
And one thing is for sure: the cpu cycles that are taken by these threads are not counted in the totals that the TopThreads plugin computes, simply because it doesn’t know about these
Despite these shortcomings, i find the new feature quite usefull myself
Let me know what you think
Other improvements in this release: the initial poll time is not fixed to 10 seconds, but depends on the (initial) number of threads
For small apps, the updates will be much more frequent
there are more preferences to set and these are moved to a separate settings dialog
Settings are stored using the Java Preferences API
improved stacktrace panel behavivour, including automatic scroll to the top
better handling of security exceptions, that might occur when connecting to a remote VM
Please let me know what you think, feedback is always welcome!