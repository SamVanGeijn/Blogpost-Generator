I’m a designer of digital products
The past 20 years my work was primarily focussed on GUI - Graphical User Interfaces
Until now
With the rise of voice-controlled digital assistents from Amazon, Google and the others, a new way of communicating with digital products has become available: the VUI - the Voice User Interface
We all know how to create conversations And now I’m confused
I still like to think that designers like me should be the ones that decide how a VUI should behave; what it should say, and when
But are we even equipped for designing for voice
As it doesn’t involve any visual elements, it seems to boil down to designing pure conversations
And why would you need a designer for that
We all know how to create conversations, everybody has them all the time
So, what does it take to design a good VUI
Looking for anwers, I first stumbled on some dear memories from my own youth
The internet’s predecessor Many years ago, yet not so very long ago, people used telephones solely for talking to each other over distance
No apps, no displays, just that single simple purpose
You dialed a number and got connected to the person on the other side
And so you could reach virtually any person, anytime, from anywhere
To chat, share experiences, to place an order or to get information; The telephone was in many ways the internet’s predecessor
My grandmother worked at 008, which was not, as you might guess from the name, in any way related to the British Secret Intelligence Service
No, 008 once was the number you dialed in the Netherlands when you needed “information”
By information the Dutch telecom provider basically meant telephone numbers; you called this service when you needed to contact someone and didn’t have the number or address
There were other services too: you dialed 002 for the time, 003 for the weather forecast and so you dialed 008 to get someones phone number
These phone services were one of the very first to introduce Voice User Interfaces, or ‘talking computers’ as people usually called them those days
Before that, 008 employed human operators for the task of looking up the numbers
My grandmother was one of them
In her first years as an operator she needed to look up the requested numbers in phone books; the frequently requested numbers she knew by heart
In 1979 computers were introduced at 008 and from that time on the human operator used computers to retrieve the requested information
A human operator as a proxy for the computer system
My grandmother used to tell me stories about people, mostly men, calling for other purposes than getting a telephone number
I remember one story about a man who had called and asked her for the time
My grandmother kindly told the man he had probably dialed the wrong number, that 002 was the number to get the time
The man had replied he knew what time it was, but that he wanted to ask at what time my grandmother would be off from work, and if she would care to join him for diner
My grandmother proudly told us that these male callers always kept inquiring about her age, because over the phone she apparently sounded like a twenty year old girl
While in fact she was in her late fifties, she had always kept them in the dark and played hard to get, just for the fun of sharing her adventures with her colleagues over lunch
And, years later, with her own grandchildren
The 008 ladies finally got replaced by voice-controlled computers in the late nineties
Computers that we could call and talk to, that got us information faster and cheaper than my grandmother could
As long as the caller kept to the script and didn’t ask for anything out of the ordinary, it was great technology
But I’ll tell you these computers were a lousy flirt compared to my grandmother
Disfunctional relationships It’s been about 20 years since computers took over from the 008 ladies and you would think voice recognition technology would have improved dramatically by now… Let’s look at this typical conversation between me and my car: Does any of this sound familiar to you
Or maybe I should just get a new car
But no, I’ve got a similar disfunctional relationship with Siri, Apple’s smart voice assistant
But I don’t blame Siri
It is still a kid
I don’t blame her just the way I don’t blame my own kids if they misinterpret my words or behave in a way that is not quite socially accepted
Siri, just like my kids, needs to learn by making mistakes
Human conversation appears to be so simple, but if you take a moment to think about it you’ll see that it is absurdly complex
Siri is still a kid I do blame my car by the way
Because my car is basically not so very different from the voice-controlled computer of 008 from the late nineties
It only knows a very limited set of commands and asks for very specific input at specific points during the interaction
That is no conversation at all
Siri on the other hand listens to everything we say and tries its best to interpret our intention
That, I think, is a remarkable step forward in techology
Even if it still often fails to grasp our true intention, it is unbelievable what it is already capable off
Human communication is complex because our language is so very ambiguous
Words have different meanings in different contexts, what we say literally is often not the same as what we mean
Humans are complex beings
We have tempers, become emotional, are always seeking self-confirmation
Even with the advanced state of technology of today, we are still a long way from real human-like conversation with computers
Watch this fragment from the movie HER
Now that is a real conversation! It is interesting to see two types of VUI in this fragment: the first is the voice that guides Theodore through the setup of the OS, which is not so very different from the way we are used to interact with computers today
After initialisation this other VUI turns up: the very human-like and instantly intriguing Samantha
Until we have reached that level of conversational and emotional skill in voice user interfaces, let us restrict from using the term ‘conversational interface’
Today we are still in the infancy of the VUI, this is still the era of voice commands
Still, these VUIs need to be designed well in order for us humans to be able to interact with them confidently
This is the era of voice commands So I’m back to my original quest of seeking what it takes to design a good VUI
I have described what I think we can expect from a VUI today, how that is very different from 20 years ago, and in what direction we might expect this technology to evolve
But that doesn’t answer my question
So, in order to find out what skills you need to create a good VUI today, I will just need to create one myself
I have found a use case that seems perfectly fit for this: booking a meeting room at our office
I will call it Bookaroo (just add an m and you’ll see why)
I will start with a rudimentary prototype that I’m planning to install in one of our meeting rooms to test and evaluate
In the next couple of months I will keep you updated on Bookaroo’s progress and my learnings about designing a VUI by posting these blogs
Hope you will like it!I recently built a Cordova application with Angular
The application periodically calls a server to get information
One of the requirements was that the client server communication had to be over SSL
I didn’t know much about the SSL protocol but I thought it should be a piece of cake
With most hosting providers it is done in just a few clicks
But this case is slightly different
The application must function without internet connection
the app communicates with an on site server in a local network by IP address
In this post I will cover what the main reasons are to use SSL, what problems arise when you want to use SSL in a local network and how I fixed it for a Cordova application
Everything that is described in this post will also work (even better) for native applications
Why we want to use SSL The two reasons why we want the app to communicate with the server over SSL are Encryption and Identification
Encryption Encryption is basically hiding what is sent from one computer to another
If a you don’t use SSL, the data will be transmitted as is and any computer on any of the networks between the client and the server can intercept and read the data
The SSL protocol puts a barrier around the communication
All the data that is sent from the client to the server and vice versa will first be changed before it gets transmitted
The data can still be intercepted by any computer on the network but they can’t read the data
The data is encrypted
Identification There can be a secure connection, but that does not necessarily mean that the client can trust the computer that is on the other end
To validate the identity of a server the SSL Protocol uses a certificate
In the certificate are details of the receiver/owner
To make the certificate reliable, there is a whole process behind obtaining a certificate
The organization has to communicate with another organization called a Certificate Authority (CA)
The organization asks a certificate for a specific domain, the CA will check domain ownership and look up the details of the organization
For some versions the CA will check more information of the business/organization but domain ownership is always mandatory
If the CA approves the request, they create a certificate for the organization and sign it with their private key
The certificate can than be installed on the server to which the domain points to
If a client connects through HTTPS to the server, the server sends its public key with its SSL certificate to the client
Once the client gets the certificate it will check the signature to make sure it’s valid
This is done by e.g
checking if it is signed by a trusted CA and the current hostname matches the domain in the certificate
If all checks succeed, a secure connection will be established
If not, (depending on the client) the request will fail
Problems To get a valid certificate we need to pass a CA
A certificate belongs to a domain and the CA checks the domain ownership
In a local network we don’t have a domain and we communicate to the server by IP address
Because of this, the domain ownership check cannot take place
A CA will never issue a certificate for a local IP address
But can’t we just use the certificate of the domain for our organization
We have a valid certificate
We can just installed that one on the server… Fortunately it is not that easy
When setting up the SSL connection, the client also check if the domain matches the domain on the certificate this is called ‘Hostname verification’
We communicate by an IP address, this does not match the domain and the connection fails
Our solution In order for the hostname verification to succeed, our certificate must contain the IP address
We can’t count on a CA so we have to make a certificate ourselves
This is possible and is called a ‘Self signed certificate’
This is a certificate that is not validated and signed by a CA, but by the organization itself
The IP address can be added by adding it as a ‘subjectAltName’ to the certificate
That way the hostname verification will succeed Earlier we said that the validation of a CA is used in the SSL checks
If the certificate is not signed by a CA that is known by the client, the check will fail
To make sure the client trusts our self signed certificate we need to pin the certificate in the application
In Cordova certificate pinning is not possible
The server connections are handled by the webview
Therefore it is not possible to intercept SSL connections to perform the check of the server’s certificate
True certificate pinning is only possible in native code
That means that every request must happens through native code instead of traditional XHR/AJAX requests
To make all this work, we used a cordova plugin to pin the certificate and made sure the application makes all requests using this plugin
For convenience we created an Angular httpClient interceptor that stops the normal requests and delegates it to the pluginIn this post I’ll take you with me on my journey into Serverless with AWS
Head first, because I won’t dive into details like what is Serverless, why should you or should you not want Serverless, what those Amazon Web Services (AWS) are, nor into Kotlin
I will just work out a concrete case
At our office we regularly hold knowledge sessions
Mostly around lunch time a colleague will tell and show something about a topic
In the morning of an upcoming knowledge session someone sends out an announcement via email to all our colleagues
Since most of us also are on Slack, we could announce it there
And this is something we could easily automate
In this post I am going to build a Slack bot that announces knowledge sessions in the morning
What do we need
A schedule of planned sessions Piece of code that sends a message to a Slack channel For the sake of brevity I will focus on the absolute minimum of these requirements
Amazon Web Services The following AWS services will be used: S3 to store the scheduled sessions
Lambda to retrieve data from store and send a message to Slack when needed
CloudWatch for logging/monitoring and triggering a scheduled event every morning
IAM (Identity and Access Management) One service that’s missing in the list above list is IAM
This service is an inherent part of AWS
Before starting with the other services first we’ll create a role for our App
This role should have permissions to: AmazonS3FullAccess AWSLambdaExecute CloudWatchLogsFullAccess This single role will be used for all services I’ll be creating next
S3 (Simple Storage Service) First step is to create a S3 bucket
S3 is an object storage that can be used to store and retrieve files
Open S3 from the AWS console and create a bucket with the name ‘upcoming-sessions’ and use the default settings (click next, next, next)
In this bucket upload a file ‘next-session.json’ with the following contents: javascript { "date": "2019-01-20", "presenter": "Rachid", "topic": "Creating a Slack announcer using AWS", "type": "Chalk & Talk" } Lambda Next step is to create a lambda which will read this file and send a message to Slack when needed
In the AWS management console go to: Lambda -> functions -> Create function
As name choose: “session-announcer”
Since I want to write the lambda in Kotlin for runtime I choose Java 8
For Role select “choose an existing role” and select the Role we just created
Please note that everything can be done via scripts using the AWS CLI as well
Creating a new lambda function would look like: dos aws lambda create-function \ --function-name session-announcer \ --runtime java8 \ --role arn:aws:iam::123456789:role/sessionAnnouncerRole \ --handler eu.luminis.chefke.event.ScheduledEventAnnouncer::handleRequest \ --zip-file announcer.jar Since we don’t have any code or jar file yet I won’t use this now
Coding in Kotlin Now it’s time for the fun part; coding the logic to retrieve the sessions from S3 and send a message to Slack when needed
The handleRequest function is the entrypoint of our code
This function should be invoked to run our code
java import com.amazonaws.services.lambda.runtime.Context import com.amazonaws.services.lambda.runtime.RequestHandler import com.amazonaws.services.lambda.runtime.events.APIGatewayProxyRequestEvent import com.amazonaws.services.lambda.runtime.events.APIGatewayProxyResponseEvent import com.amazonaws.services.s3.AmazonS3ClientBuilder import com.beust.klaxon.Klaxon import java.io.InputStream import java.time.LocalDate import java.time.format.DateTimeFormatter data class SlackResponseMessage(val text: String) data class LuminisEvent(val topic: String, val type: String, val presenter: String, val date: String) const val filename = "next-session.json" class SessionAnnouncer : RequestHandler<APIGatewayProxyRequestEvent, APIGatewayProxyResponseEvent> { val s3client = AmazonS3ClientBuilder.defaultClient() val restClient = RestClient() override fun handleRequest(event: APIGatewayProxyRequestEvent, context: Context): APIGatewayProxyResponseEvent { val today = LocalDate.now() getEvent(today)?.let { e -> val message = "Today we have a ${e.type} session about '${e.topic}' presented by ${e.presenter}." println("Sending the event to Slack: $message") val slackJsonMessage = Klaxon().toJsonString(SlackResponseMessage(message)) // POST the announcement as a JSON payload to the Slack webhook URL
restClient.post(Config.slackWebhook, slackJsonMessage) return APIGatewayProxyResponseEvent().apply { statusCode = 200 headers = mapOf("Content-type" to "text/plain") body = "Message sent to Slack: $message" } } val notFoundMessage = "No event found for ${today} to post to Slack" println(notFoundMessage) return APIGatewayProxyResponseEvent().apply { statusCode = 404 headers = mapOf("Content-type" to "text/plain") body = notFoundMessage } } /** * Retrieves JSON file from S3, parse it and return the Object when it's today
*/ private fun getEvent(day: LocalDate): LuminisEvent
{ getEventFromBucket()?.let { val event = Klaxon().parse<LuminisEvent>(it) event?.let { nextEvent -> val eventDay = LocalDate.parse(nextEvent.date, DateTimeFormatter.ISO_DATE) if(eventDay.equals(day)) { return nextEvent } } } return null } fun getEventFromBucket(): InputStream
{ val s3Bucket = Config.s3Bucket if(s3client.doesObjectExist(s3Bucket, filename)) { return s3client.getObject(s3Bucket, filename).objectContent } println("'$filename' does not exists in the bucket: $s3Bucket") return null } } There are several ways to write this code and publish it to AWS
For any serious coding I personally prefer to use my favorite IDE on my local machine
To keep build/test/package related logic in a central place I use Gradle
Luckily there’s a Gradle plugin for uploading the code to an AWS lambda
My build.gradle file is as follows: java import com.amazonaws.services.lambda.model.InvocationType import jp.classmethod.aws.gradle.lambda.AWSLambdaInvokeTask import jp.classmethod.aws.gradle.lambda.AWSLambdaMigrateFunctionTask buildscript { ext.kotlin_version = '1.2.31' repositories { mavenCentral() maven { url "https://plugins.gradle.org/m2/" } } dependencies { classpath 'com.github.jengelman.gradle.plugins:shadow:2.0.2' classpath "org.jetbrains.kotlin:kotlin-gradle-plugin:$kotlin_version" classpath "jp.classmethod.aws:gradle-aws-plugin:0.30" } } version '1.0-SNAPSHOT' apply plugin: 'com.github.johnrengelman.shadow' // To create a fatjar which can be uploaded to AWS apply plugin: 'jp.classmethod.aws' // Gradle tasks for AWS stuff apply plugin: 'jp.classmethod.aws.lambda' // Gradle tasks for deploying and running lambda's apply plugin: 'kotlin' repositories { jcenter() mavenCentral() } dependencies { implementation "org.jetbrains.kotlin:kotlin-stdlib-jdk8:$kotlin_version" // AWS API implementation 'com.amazonaws:aws-lambda-java-core:1.2.0' implementation 'com.amazonaws:aws-lambda-java-events:2.1.0' implementation 'com.amazonaws:aws-java-sdk-s3:1.11.308' // JSON parser for Kotlin implementation 'com.beust:klaxon:3.0.1' } compileKotlin { kotlinOptions.jvmTarget = "1.8" } lambda { region = "eu-west-1" } // Task to deploy the code to AWS task deployFunction(type: AWSLambdaMigrateFunctionTask, dependsOn: [shadowJar, test]) { functionName = "session-announcer" runtime = com.amazonaws.services.lambda.model.Runtime.Java8 role = "arn:aws:iam::${aws.accountId}:role/scheduleCntRole" zipFile = shadowJar.archivePath handler = "eu.luminis.blog.SessionAnnouncer::handleRequest" memorySize = 512 timeout = 20 } // Task to directly invoke the lambda in AWS task invokeFunction(type: AWSLambdaInvokeTask) { functionName = "session-announcer" invocationType = InvocationType.RequestResponse payload = "" doLast { println "Lambda function result: " + new String(invokeResult.payload.array()) } } On top the plugins are applied and the dependencies are declared, at the bottom there are two custom tasks; deployFunction and invokeFunction
The first should be executed to upload the code to AWS, and the latter can be used for directly invoking the code running at AWS
Note that in deployFunction we’ve specified the handler function of our lambda
Now let’s upload the code by executing the task deployFunction
In IntelliJ this can be done by expanding the tasks in the Gradle view and double click deployFunction
Or just open a Terminal in the root of the project and run ./gradlew session-announcer:deployFunction
NB: When using this Gradle plugin or AWS CLI for the first time you may need to authenticate first
When the function is deployed successfully we can try to invoke it with: ./gradlew session-announcer:invokeFunction Oh noes, I got an error: {"errorMessage":"Missing env var 'S3_BUCKET'!","errorType":"java.lang.IllegalStateException"} Of course, the code uses Config.s3Bucket which reads the bucket name from an environment variable because we don’t want to have any configuration in code
java object Config { val s3Bucket by lazy { getRequiredEnv("S3_BUCKET") } val slackWebhook by lazy { getRequiredEnv("SLACK_WEBHOOK_URL") } private fun getRequiredEnv(name: String): String { println("Retrieving environment variable: $name") return System.getenv(name) ?: throw IllegalStateException("Missing env var '$name'!") } } We should add two required environment variables
Open the AWS console and go to lambda -> session-announcer and scroll down to the section “Environment variables”
Here we can add the key/value pairs
So add the key S3_BUCKET and for value the name of the bucket we created, e.g
‘upcoming-sessions’
Before we can add the second environment variable, first a bit more about Slack
Slack integration In order to send messages to a Slack Channel you need to create a so called ‘Slack App‘ and install it in the Slack workspace
After you’ve added the app to a channel, a webhook URL is generated and can be retrieved from the Slack console via: https://api.slack.com
This URL is the second environment variable we need to configure for the AWS lambda
So go back to the AWS console and add to the section “Environment variables” a new key: SLACK_WEBHOOK_URL with the slack webhook URL as value
Now try to invoke the lambda again with: ./gradlew session-announcer:invokeFunction Hopefully you will either see something like: {"statusCode":200,"headers":{"Content-type":"text/plain"},"body":"Message sent to Slack: Today we have a Chalk & Talk session about 'Creating a Slack announcer using AWS' presented by Rachid."} Or: {"statusCode":404,"headers":{"Content-type":"text/plain"},"body":"No event found for 2019-01-20 to post to Slack"} To get more insights, the logs can be viewed in CloudWatch
In the AWS console go to CloudWatch -> Logs
There should be a log with the name /aws/lambda/session-announcer containing all the logs of our lambda
Scheduling Now we have a piece of code running as a lambda in AWS that can announce the sessions of the current day
Next step is to trigger this lambda every morning
This can be done with ‘Events’ in CloudWatch
In the AWS console go to CloudWatch -> Events -> Rules and click “create rule”
Then select Schedule (instead of Event pattern)
To let it run every morning at 7:30 AM you can enter the following CRON expression: 30 7 * * 
*
At the right-hand side click ‘Add target’ and for function select the name of our lambda: session-announcer
Click configure details, choose a name and Save the rule
The next morning at 7:30 AM local time of the AWS region the lambda will be triggered
For testing purposes you could trigger it every minute with this CRON expression: */1 * * * 
*
Quite quickly we have automated a simple task
When you want to play around with this yourself, all the code from this post can be found in this Git repository
Becoming an AWS developer In the beginning playing around with AWS can be quite overwhelming
When you want to get some serious knowledge I recommend the Udemy course AWS Certified Solutions Architect – Associate (most of the time priced around $10,-)
A lot of information is scattered on the internet via blog posts and in the Amazon documentation
But what I like about this course is that it is kept up-to-date and gives you a single cohorent story which is easy to follow
Wind up In this post I focused on the bare minimum, but actually I also built: REST interface to add sessions to the store; using API gateway and another lambda REST interface to get session from store; called by a custom Slack command and backed by another lambda Now we have a small working app, some improvements and new features I have in mind: Store the session in a data store like Dynamo DB Make it possible to schedule multiple sessions Automatically provisioning of the services with CloudFormationIn a lot of software that I’ve seen, the class model could be better
One of the problems I often see is that a class contains other “hidden” classes: a set of fields that really should be its own class
I will discuss where this originates from, and why it may cause problems
First, I will provide two common examples
The first is the address, a set of at least street, house number, and city, but often also containing ZIP code, state and country
In a lot of cases these will just be fields in another class such as customer
The second example is a date range; a period of time between a start date and an end date, for instance to indicate when a given rule is valid
Often, a date range is a start and end date in the class that it applies to
What is the problem anyway
So, why is it a problem if these are not represented as separate classes
First, it may lead to duplication of code
If you have several date ranges in your code, there will be several places where you are going to check if a given date is in that date range, or if another date range overlaps
Similarly, there may be validation and rendering methods for an address
All this code must also be tested, sou you get a lot of duplication in tests as well
Second, it makes the code less readable in many ways
If-statements comparing four dates from two date ranges is hard to understand, and tricky to debug
Quick, what does this do
java Java LocalDate dateRangeStart = ...; LocalDate dateRangeEnd = ...; LocalDate otherDateRangeStart = ...; LocalDate otherDateRangeStart = ...; if (dateRangeEnd.isAfter(otherDateRangeStart) && dateRangeStart.isBefore(otherDateRangeEnd)) { ..
} If a class contains multiple addresses, your field names will become less readable: java Java class Customer { private String firstName; private String surname; private String residenceAddressStreet; private String residenceAddressHouseNumber; private String residenceAddressCity; private String postalAddressStreet; private String postalAddressHouseNumber; private String postalAddressCity; ..
} 1 2 3 4 5 6 7 8 9 10 11 class Customer { private String firstName; private String surname; private String residenceAddressStreet; private String residenceAddressHouseNumber; private String residenceAddressCity; private String postalAddressStreet; private String postalAddressHouseNumber; private String postalAddressCity; ..
} This may be the right moment to consider why classes are modelled this way
There may be a lot of different reasons, but one of the most important ones I see is that an application blindly copies an external data model
If a database is provided, and the database has a table “customer” with street, house number and city fields, there will be a class Customer with those same fields
Similarly, if the interface to a front end specifies the address as fields in a larger object instead of a separate object, that model may easily become the back end model as well
Another reason may be that no suitable class is available
Java has great support for dates and durations, but not for a date range
So the easy way is to just add a start and end date to your class
Type safety Now, for a last warning: upon discovering the code duplication and readability problems, one might be tempted to create static helper methods in utility classes, because this has less impact on the code as a whole
Apart from all the reasons why utility classes should be avoided, I want to add one more specific to this case: since many fields of an address are Strings, any helper method will have a signature that does not check whether you pass the right fields: java class AddressHelper() { public static boolean isAddressValid(String street, String houseNumber, String city) { ..
} } // No compile error if (AddressHelper.isAddressValid(houseNumber, street, city) { ..
} Especially error-prone because different countries have a different order in which they present the address fields: compare the Dutch street – house number – ZIP code – city to the US house number – street – city – ZIP code
Using separate classes If you represent an address as a separate class, it is far easier to provide it to an external service in a type-safe manner
It also makes your code more readable
java class Address { private String street; private String houseNumber; private String city; } class Customer { private String firstName; private String surname; private Address residenceAddress; private Address postalAddress; } class AddressService() { public boolean isAddressValid(Address address) { ..
} } if (addressService.isAddressValid(customer.getResidenceAddress())) { ..
} To use date ranges, you probably want to have your own class (or use one from a library) to contain all the related logic
This is both far easier to test and makes your code a lot more readable
java class DateRange { private LocalDate start; private LocalDate end; ..
public boolean overlaps(DateRange other) { return end.isAfter(other.getStart()) && start.isBefore(other.getEnd()); } } if (dateRange.overlaps(otherDateRange)) { ..
} In closing Refactoring may incur significant cost to the development project, and the later you start the more it will cost
However, operational cost is significant over the lifetime of a software product, and solid, readable code will drive that cost down
Experience will make it easier to spot these problems earlier, but even if you run into the problems late in a project, it is worth considering refactoring your class modelMet Crostini is het mogelijk om linux apps te installeren op je Chromebook
Via Crostini kan je een Chromebook omtoveren tot een ware development laptop.Mijn Chromebook ondersteunt momenteel nog geen Crostini
Tot de tijd dat Crostini nog niet tot de mogelijkheden behoort, ben ik opzoek gegaan naar een alternatief
Uiteindelijk ben ik terecht gekomen op Chromebrew waarmee het me gelukt is om Kotlin bestanden te bouwen en uit te voeren
Wat is Chromebrew
Chromebrew is een package manager voor Chrome OS
Het is vergelijkbaar met Homebrew voor MacOS
Hierdoor kan je packages installeren die standaard niet door Google geleverd worden
Voorwaarden Je hebt een Chromebook nodig waar developer mode is geactiveerd
Hierdoor heb je toegang tot de shell van je Chromebook
Installatie van Chromebrew Chromebrew gaan we installeren via de shell
Door de toetscombinatie CTRL + ALT + T (net als in Linux) kunnen we de Terminal openen
Type shell om, jawel, de shell te openen
Krijg je een ERROR: unknown command: shell melding, dan is Developer Mode niet geactiveerd op je Chromebook
Nu we in de shell zitten kunnen we de installatie script downloaden en uitvoeren
Dit doen we met de volgende commando: java $ wget -q -O - https://raw.github.com/skycocker/chromebrew/master/install.sh | bash Na een paar minuten zal Chromebrew geïnstalleerd zijn en kan het gebruikt worden met de keyword crew
Kotlin installeren Om te kijken of een package aanwezig is gebruik je crew search <package naam>
Voor kotlin geeft dit het volgende resultaat: java $ crew search kotlin kotlin: Statically typed programming language for modern multiplatform applications Nu we weten dat de Kotlin package aanwezig is, kunnen we het gaan installeren
Dit doen we met crew install kotlin
java $ crew install kotlin kotlin: Statically typed programming language for modern multiplatform applications https://kotlinlang.org/ version 1.2.21 The following packages also need to be installed: jdk8 Do you agree
[Y/n] Omdat Kotlin een afhankelijkheid heeft op java moet jdk8 ook geïnstalleerd worden
apache Do you agree
[Y/n] y Proceeding..
jdk8: The JDK is a development environment for building applications, applets, and components using the Java programming language
http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html version 8u181 Precompiled binary available, downloading..
Archive downloaded Unpacking archive, this may take awhile..
Installing..
Performing post-install..
Precompiled binary available, downloading..
Archive downloaded Unpacking archive, this may take awhile..
Installing..
Performing post-install..
Kotlin installed! Kotlin is geïnstalleerd en de volgende commando’s zijn nu beschikbaar: kotlin kotlin-dce-js kotlinc kotlin-js kotlinc-jvm Kotlin (script) runnen Om het simpel te houden gaan we een Kotlin script runnen
Dit kan door bijvoorbeeld met de standaard text editor een bestand aanmaken en opslaan als test.kts
In het bestand zet je: apache println("Hello Chromebook!") Vervolgens ga we met de shell naar de directory
Nu gaan we het bestand compilen en runnen: apache $ kotlinc -script test.kts Hello Chromebook!Any webdeveloper who has worked with Angular as their front-end framework lately has probably encountered the RxJS (reactive extensions for javascript) library
This is because the angular http module is dependent on RxJS and this modules’ various methods for http put, post and patch requests return a RxJS ‘Observable’, which is the key component and concept of RxJS
I was, until recently, used to working with promises to handle asynchronous Javascript operations
Although I was familiar with the observer design pattern in general, the way RxJS works and how to work with it confused me at first
I decided to write down my recent experience with RxJS in the form of this blog
This is partly to benefit my own learning process and partly hoping that it may aid that of the reader
Alternatively, I hope it will peak the readers’ interest in making use of RxJS when writing code
RxJS turns out to be an elegant and useful tool when structuring javascript applications that need to handle asynchronous processes
As such, I feel it is useful for any javascript developer to take note of
So what is a RxJS observable
Let’s start off with the most basic example of handling a http get request may look like in an Angular app, taken straight from the angular.io documentation: javascript getConfigResponse(): Observable<HttpResponse<Config>> { return this.http.get<Config>( this.configUrl, { observe: 'response' }); } Let’s try to put into words what this code is for: This is a piece of typescript that shows a method getConfigResponse(), that returns an observable of a HttpResponse , that maps to a Config object’
In this piece of code, this.http is the angular http client
If we look at the get() methods of this ‘class’ we can see that they return an observable object from the rxjs library
And, courtesy of typescript, we can check out the typescript definitions of Observable ourselves
At the very top of the code in the Observable.d.ts interface it says the following about the Observable<T> class: javascript /** * A representation of any set of values over any amount of time
This is the most basic building block * of RxJS
* @class Observable<T> */ The official RxJS documentation describes an observable like this: “An observable represents the idea of an invokable collection of future values or events”
So we can infer that the Observable class is at the heart of RxJS and that this building block always functions as a representation, or wrapper, of something else
Moreover, it represents the things it wraps as values or events, that are bound to become available at some unknown time in the future
Let’s explore some examples of RxJS observables used in code from an actual application! In the interface definition of Observable we see the definitions of some methods: subscribe() and pipe() are the most interesting ones and will be discussed shortly
Apparently, a RxJS observable can be interacted with via a limited set of methods defined on the Observable class
And how should I use this thing
javascript import { Observable } from 'rxjs/Rx'; @Injectable() export class AuthService { ...//other AuthService code private userSettings: UserSettings; ...//other AuthService code public getProfile(): Observable<UserSettings> { if (!this.userSettings) { return this.http.get<UserSettings>(this.config.userServiceUrl + '/users/usersettings', { withCredentials: true }).do(userSettings=> { this.userSettings= userSettings; }); } return Observable.create(this.userSettings); } } This code fragment is already illustrative of the powerful things we can do with RxJS observables
Here, we return an observable of some user’s userprofile settings (note that userId is not handled via url in this specific case)
If there are no user settings in memory yet, we assign the object returned from the http response to the userSettings variable of the class, then return an observable of the user settings for further interaction
If instead there are user settings in memory already, we simply instantiate a new observable of the existing userSettings with Observable.create()
The latter is a very handy aspect of RxJS observables: You can wrap anything in the observable and subsequently program against that observable in a reactive manner
Thus easily mixing synchronous with asynchronous code
The .do method is the other new concept in the above piece of code
This method is not present on Observable (that returns from this.http.get<UserSettings>()) itself
Rather this ‘operator’ gets imported from ‘rxjs/operators’
The official RxJS documentation describes pretty clearly what this specific operator is for: “Perform a side effect for every emission on the source Observable, but return an Observable that is identical to the source”
RxJS has a large number of operators similar to .do, that can filter, buffer, delay, map (etc., etc.) your observables and always create a new observable as output
Observable.pipe() that I mentioned before is present on the Observable object itself, but otherwise functions more or less the same as an operator in that it returns a new observable based on the input of an existing one
The new Observable in the case of .pipe () is projected to from the old one, by passing through the functions defined in the pipe
I’ll refer to this external article for readers who want more info on piping observables
Now let’s cover how to actually observe a RxJs Observable
This is where, gasp, observable.subscribe() comes into play
I’ll give a hypothetical example based on the code above for getProfile(): javascript @Component export class SomeViewComponent { constructor (private authService: AuthService){ } private userSettingsOnScreen(): void { this.authService.getProfile().subscribe(userSettings => { this.backGroundColor = userSettings.preferences.backGroundColor; }); } The subscribe method in the above example takes a function as method parameter
This function kind of acts like an observer in a classic observer design pattern and in fact the RxJS documentation actually calls this function the ‘observer’ as well
This makes a lot of sense given that: You can subscribe multiple of these functions to the same Observable You can unsubscribe these methods from the Observable The ‘observer’ function(s) can listen to only 3 kind of ‘events’ emitted by the Observable and those are: ‘onnext’: The Observable delivering it’s wrapped value (as in the above example)
‘onerror’: The Observable delivering an error result due to an unhandled exception
‘oncompleted’: All onnext calls have been completed and the observer function has finished
The onnext callback confused me for a while, but it makes more sense knowing you can do this (example taken from here): javascript public Listener(name: string): void { return { onNext: function (v) { console.log(name + ".onNext:", v) }, onError: function (err) { console.log(name + ".onError:", err.message) }, onCompleted: function () { console.log(name + ".onCompleted!") }, } let obs = Observable.of(1, 2) obs.subscribe(Listener("obs")); //output obs.onNext: 1 obs.onNext: 2 obs.onCompleted! Not only can we subscribe multiple times to a single Observable but, as the example above should clarify, we can also observe multiple things in sequence with the same Observable
Pretty nice! I'll round up this write-up with showing error handling in RxJS: javascript // Try three times with the retry operator, to get the data and then give up var observableOfResponse = this.http.get('someurl').retry(2); var subscription = observableOfResponse.subscribe( data => console.log(data), err => console.log(err) ); // catch a potential error after 2 retries with the catch operator var observableOfResponse = get('someurl').retry(2).catch(trySomethingElse()); var subscription = observableOfResponse.subscribe( data => { // Displays the data from the URL or cached data console.log(data); } ); Here we see that we can either let the error (maybe a code 500?) happen like in the top example, or catch and manage it like in the bottom one
It’s pretty intuitive, once you know what operators are available
Note that in the first scenario an exception still gets thrown
Rounding up RxJS is an extensive subject and deserves more attention than I’m able to give in this little blog
I have tried in this article to condense a vast amount of information into a short overview that aimed to cover the core ideas of the library but at the same time be very practical: The information above is enough to start working with it immediately
So rounding up, I’ll say good luck and have fun to anyone using RxJS in their front-end
I know I willSoftware is eating your smartphone
Or something along those lines
So it is not surprising there is a lot to be said about mobile app development and especially about all the non-native frameworks that are around these days
We know it’s hard to find your way in all these voices so we have made a neat overview of our experience and findings for your convenience
But before we dive into it, let’s start with some clichés: Cliché #1: The world of mobile app development is ever changing
Face ID, ARCore, Jetpack, Swift 4: There is no shortage of ‘new’ things for mobile app users or developers and keeping track of everything can easily become a day-job
Thankfully there is a large community of writers (like me) that will help you navigate the landscape
But your hands are on the wheel and it’s your car when you crash it into a tree
In other words, you have to make the tough choices
The best thing we can do is guide you along the way
Cliché #2: Software development is about balancing trade-offs
A lot of frameworks and technologies for software development are selling their product or service like it’s the best thing since sliced bread
Especially the last couple of years I see more and more of their websites turned into a polished and well-tuned commercial website
With marketing slogans, customer blurbs, and video commercials
But every practiced software developer knows there is no such thing as a holy grail
Context is everything and when searching for the best tool for the job, it’s always good to look at both
Otherwise, you will be lured into a solution that has too many trade-offs because of clever, superficial marketing campaigns
Cliché #3: Android is better/worse than iOS
The usefulness of this discussion is zero
When you’re building a serious mobile app there is no “or” between Android and iOS
Sure, they have different market shares but you can’t neglect one over the other because of personal preference
Any serious app developer should target both
The hybrid: Toaster or skinjob
I think the Battlestar Galactica analogy is somewhat apt
Android as the sturdy and rough metal robots
iOS as the most human-like but still artificial machine
Whatever your point of view, supporting competing platforms for your app can be a bit frustrating
Writing and maintaining the same set of features multiple times has a real impact on development costs and time to market
It feels like waste so it’s not surprising a lot of hybrid app frameworks are available to mitigate these deficiencies
When you’re willing to leave the native ecosystem behind, another set of choices become available accompanied with their own pros and cons
Download the overview here At Luminis, we have developed a lot of mobile apps with native and various hybrid frameworks
We understand the difficulties, especially when you want to put things into context
To help you out we have summarized our insights in this high-level rundown
It’s especially useful when you have some functional requirements available and a rough idea of the user experience you seek
At the very least you will get some foreknowledge of the impact on user experience and development costs of these frameworksOops, I guess I should have tested that title better
I wasn’t expecting that many viewers to concurrently view this blog
If only I had performed load testing beforehand! Hopefully it’s not too late for you and your application
Learn from my mitsake and test how your application will react when many users access it
When building your application, you probably test your application in a lot of ways, such as unit testing or simply just running the application and checking if it does remotely what you expect it to do
If this succeeds you are happy
Hooray for you, it works for one person! The title of this blog also looked good when only one person was viewing it! Of course, you’re not in the business of making web applications that will only be used by just one person
Your app is going to be a success with millions of users
Can your application handle it
I don’t know, you don’t know…nobody knows! Read the rest of this blog post to see how we tested an application we are working on at Luminis and found out for ourselves
Why and how
Why
Here’s why! I work on a project team at Luminis and our job is to make sure that users of a certain type of connected device can always communicate with said device via their smart phones
There are thousands of these devices online at the moment and that number is just going to keep increasing
So instead of just hoping for the best, we decided to take matters into our own hands and see for ourselves just how robust and scalable our web applications really are
How
Here’s How: Apache JMeter JMeter WebSocket Plugin by my colleague Peter Doornbosch Docker Amazon Web Services (Elastic Container service, Fargate, S3 and CloudWatch) A lot of swearing when things don’t go the way you expect Apache JMeter I’ll let the JMeter doc speak for itself: “The Apache JMeter™ application is open source software, a 100% pure Java application designed to load test functional behavior and measure performance
It was originally designed for testing Web Applications but has since expanded to other test functions.” I’m sure that description got you in the mood to start using JMeter immediately (/endSarcasm)
The best way to look at JMeter for now is to think of it as a REST client that you can set up to do multiple requests at the same time instead of just performing one
You can create multiple threads that will run concurrently, configure the requests in many ways and view the results afterwards in many types of views
Download and install it from here: https://jmeter.apache.org/ When you startup JMeter, it looks like this: I know what you’re thinking
“Mine doesn’t/won’t look like that, I have a Windows PC”
Don’t worry, it works there too (provided you have Java installed)
It’s just Java! There are a lot of things you can do in JMeter, however I’m going to focus on the essentials you need to perform a load test on your REST endpoints
In the next section I will also show how to perform load tests if you use WebSockets
Let’s just ignore all of the buttons for a while and just focus on the item called “Test Plan”, which is highlighted in the first image
You can leave the name as it is, I personally never change it
I just change the name of the .jmx file, which is the extension for JMeter scripts
Thread Group Right clicking on the Test plan will bring up the context menu
The first thing you want to do is create a thread group, which will be used to create multiple threads making it possible to make requests in parallel: The most important settings here are: Number of threads (users): Number of threads that will be started when this test is started
What exactly these threads will do will be defined later on
Ramp-Up Period (in seconds): The number of seconds before all threads are started
JMeter will try to spread the start of the thread evenly within the ramp-up period
So if Ramp-Up is set to 1 second and Number of threads is 5, JMeter will start a thread every 0.2 seconds
Loop count:How many times you want the threads to repeat whatever it is you want it to do
I usually leave this set to 1 and just put loops later on in the chain
By default, the thread group is set with 1 thread, 1 second ramp up and 1 loop count
Let’s leave that like that for now
HTTP Request Sampler Now right click on the thread group and create a HTTP Request sampler: This should (maybe) look familiar! It kind of looks like every REST client ever, such as Postman
There’s a button labeled “advanced”
I never clicked it, neither should you (yet)
Fill in the following: Protocol [http]:http —- Can also be https if you have a certificate installed
Server name or IP: localhost —- It can also be a remote IP or your server name if you have one registered
Port number: 8151 —- Or any port your application is listening to
Method: GET —- This can also be any other HTTP method
If you select POST, you can also add the POST body
Path: <<path to an endpoint you want to test>> —- Example: /alexa/ temperature Parameters: Here you can add query parameters for your request, which would usually look like this in your URL /alexa/temperature?parameterName1=paramterValue1&parameterName2=paramterValue2 Body data: Where you can add the body data in whatever format you need to send your POST body data
Since we are going to test a GET endpoint, leave this empty for now
When you’re done, it should look like this: You could just press the play button now and it will perform exactly one GET request to the endpoint specified
However, you won’t get much feedback in JMeter at the moment
Let’s add a few more items to our test plan
HTTP Header Manager You might want to add some headers to your HTTP request
To do so, right click on the HTTP Request sampler and select HTTP Header Manager: Here you can add your HTTP headers
Some examples are “Authorization” where you might send an authentication token, or the “Content-Type” header when sending POST body data: View results We’re almost there! When running your JMeter script, you probably want to see the result of each action request
Right click on the Test Plan > Add > Listeners and add the following two items: View results in Tree View results in Table After running the script by pressing on the green play button, this is how these two views will display the results: Here we can immediately see that the request was successful due to the green icon next to it in the tree
The sampler results give us a lot of extra info, including the status code
We can also check the request we sent and the response data we get back by clicking on those tabs
Here we can see that the response data is 20.0, which is what I programmed my mock object to return: The table view looks like this: And if I run a request I know will fail, by sending an invalid token for example, then the table looks like this: If you want to clear all your results, click on the button at the top with the cog and two brooms (clear all)
User defined variables We’ve just added a lot of configuration in JMeter, however it is also possible to add a list of variables that you define yourself and use them throughout JMeter
This is needed for later on when we want to run JMeter as a script and not from the GUI
To do this, right click on the Test plan > Add > Config Element and select User defined variables
Let’s say I add the following variables that we previously entered into JMeter: Now that I have these four variables set up, I can refer to them in the following way: $(parameterName) This means that if I want to reference the “numberOfThreads” variable, I will add $(numberOfThreads) and it will use “1” in this case
With these variables, our Thread Group configuration looks like this: We’ll get back to these variables in a bit
CSV Data Set Config Up to this point, we have been running this request with a hardcoded authentication token
However, once we want to start running multiple requests and perform our load test, we might want to send a different authentication token per connection
This is possible by having a .csv file with these tokens and reading them in using the “CSV Data Set Config”
To add this, right click on the Test plan > Add > Config element and select “CSV Data Set Config”
Let’s say we have a .csv file that looks like this: 1, <<token1>> 2, <<token2>> 3, <<token3>> …etc If we set the CSV Data Set Config up in the following way, we can use the tokens per connection: Filename: Relative path to the .csv file (from the .jmx script) File-encoding: UTF-8 —- I’m not going to explain file encoding
Variable Names (comma-delimited): id, token Recycle on EOF?: False —- If set to true, if your numberOfThreads > numberOfTokensInCSVFile, it will startover from the top of the file when it runs out of tokens to use
Stop Thread on EOF?: True — This is because we don’t want to have more threads than the number of tokens we have in our .csv file What we’ve accomplished with this is that we’ve created two new variables, namely “id” and “token” which is available to use through JMeter
This is similar to the User Defined Variables and can be accessed in the same way ($(token) for example)
Our HTTP Header Manager config now looks like this: Now you should be ready to run multiple connections
Edit the numberOfThreads and rampUpTime in the User Defined Variables and see what happens! It probably won’t work in the first try and it will probably be your fault
Look at the errors in the view results listeners to see what the cause of the problem is
Run JMeter in Non-GUI mode We’ve seen how to setup a JMeter test and run it in the JMeter GUI
However, when you want to perform this at a large scale, you will probably want to run it on a server somewhere in a non-GUI mode as a script
In order to do this, you need to open your script one more time in JMeter and make a small adjustment
In the User Defined Variables, change the “numberOfThreads” value to: ${__P(numberOfThreads)} This means that the value for this variable will be passed on to the script when it is called
You can do this with all your variables, but for now let’s change this one
Navigate to the .jmx file location in the command line and run the following: apache ./apache-jmeter-3.3/bin/jmeter -n \ -t ./my_script.jmx \ -j ./ my_script.log \ -l ./my_script.xml \ -Jjmeter.save.saveservice.output_format=xml \ -Jjmeter.save.saveservice.response_data=true \ -Jjmeter.save.saveservice.samplerData=true \ -JnumberOfThreads=1 && \ echo -e "\n\n===== TEST LOGS =====\n\n" && \ cat my_script.log && \ echo -e "\n\n===== TEST RESULTS =====\n\n" && \ my_script.xml No, I don’t know what the Windows equivalent is of this
An explanation of what just happened: ./apache-jmeter-3.3/bin/jmeter: This is the location to my JMeter when running this script
You should set the path to your JMeter
-t: The location of the .jmx file
-j: The location the log file should be saved -l: The location the results .xml file should be saved save.saveservice.output_format: The format to save the results in
XML is a good choice because you can load it into the JMeter GUI results to view them
JnumberOfThreads: This is the value will be passing which will be mapped to the “numberOfThreads” variables in our script
If you want to pass more values, be sure to add a CAPITAL J before the variable name
The rest of the command is to just show output immediately to the command line
If you run this in the background, you can always follow the progress in the .log file
WebSockets I’ve started this story by explaining why our team looked into load testing
The connected devices use a WebSocket connection to connect to our backend application
In order to test this, we couldn’t use the HTTP Request Sampler
We needed a WebSocket Sampler
JMeter doesn’t come with such a sampler by default, so our colleague Peter Doornbosch decided to make his own JMeter Sampler
You can find instructions on how to install this sampler into JMeter on the GitHub page for his sampler: https://github.com/ptrd/jmeter-websocket-samplers
Be sure to give it a star! You can add the samplers in the same way you would add the HTTP Request Sampler
There are multiple WebSocket samplers you can use, however the one we use the most are the “WebSocket Open Connection” and the “WebSocket request-response Sampler”
The first allows us to open a WebSocket connection with a host
It is very straight-forward, similar to the HTTP request sampler
The “WebSocket request-response Sampler” allows us to send a message via the WebSocket connection created
You can send text or binary
Again, this is very straight-forward
For any more explanation on this Sampler and how to do more complicated things like Secury WebSockets, see the documentation on GitHub
There are a lot more things you can do with JMeter, however those will remain out of scope of this blog because it’s my blog and I said so
Docker I’m not going to explain what Docker is in this blog
If you don’t know or want a refresher, view this page: https://www.docker.com/what-docker Our Dockerfile looks like this: apache # Use a minimal base image with OpenJDK installed FROM openjdk:8-jre-alpine3.7 # Install packages RUN apk update && \ apk add ca-certificates wget python python-dev py-pip && \ update-ca-certificates && \ pip install --upgrade --user awscli # Set variables ENV JMETER_HOME=/usr/share/apache-jmeter \ JMETER_VERSION=3.3 \ WEB_SOCKET_SAMPLER_VERSION=1.2 \ TEST_SCRIPT_FILE=/var/jmeter/test.jmx \ TEST_LOG_FILE=/var/jmeter/test.log \ TEST_RESULTS_FILE=/var/jmeter/test-result.xml \ USE_CACHED_SSL_CONTEXT=false \ NUMBER_OF_THREADS=1000 \ RAMP_UP_TIME=25 \ CERTIFICATES_FILE=/var/jmeter/certificates.csv \ KEYSTORE_FILE=/var/jmeter/keystore.jks \ KEYSTORE_PASSWORD=secret \ HOST=your.host.com \ PORT=443 \ OPEN_CONNECTION_WAIT_TIME=5000 \ OPEN_CONNECTION_TIMEOUT=20000 \ OPEN_CONNECTION_READ_TIMEOUT=6000 \ NUMBER_OF_MESSAGES=8 \ DATA_TO_SEND=cafebabecafebabe \ BEFORE_SEND_DATA_WAIT_TIME=5000 \ SEND_DATA_WAIT_TIME=1000 \ SEND_DATA_READ_TIMEOUT=6000 \ CLOSE_CONNECTION_WAIT_TIME=5000 \ CLOSE_CONNECTION_READ_TIMEOUT=6000 \ AWS_ACCESS_KEY_ID=EXAMPLE \ AWS_SECRET_ACCESS_KEY=EXAMPLEKEY \ AWS_DEFAULT_REGION=eu-central-1 \ PATH="~/.local/bin:$PATH" \ JVM_ARGS="-Xms2048m -Xmx4096m -XX:NewSize=1024m -XX:MaxNewSize=2048m -Duser.timezone=UTC" # Install Apache JMeter RUN wget http://archive.apache.org/dist/jmeter/binaries/apache-jmeter-${JMETER_VERSION}.tgz && \ tar zxvf apache-jmeter-${JMETER_VERSION}.tgz && \ rm -f apache-jmeter-${JMETER_VERSION}.tgz && \ mv apache-jmeter-${JMETER_VERSION} ${JMETER_HOME} # Install WebSocket samplers RUN wget https://bitbucket.org/pjtr/jmeter-websocket-samplers/downloads/JMeterWebSocketSamplers-${WEB_SOCKET_SAMPLER_VERSION}.jar && \ mv JMeterWebSocketSamplers-${WEB_SOCKET_SAMPLER_VERSION}.jar ${JMETER_HOME}/lib/ext # Copy test plan COPY NonGUITests.jmx ${TEST_SCRIPT_FILE} # Copy keystore and table COPY certs.jks ${KEYSTORE_FILE} COPY certs.csv ${CERTIFICATES_FILE} # Expose port EXPOSE 443 # The main command, where several things happen: # - Empty the log and result files # - Start the JMeter script # - Echo the log and result files' contents CMD echo -n > $TEST_LOG_FILE && \ echo -n > $TEST_RESULTS_FILE && \ export PATH=~/.local/bin:$PATH && \ $JMETER_HOME/bin/jmeter -n \ -t=$TEST_SCRIPT_FILE \ -j=$TEST_LOG_FILE \ -l=$TEST_RESULTS_FILE \ -Djavax.net.ssl.keyStore=$KEYSTORE_FILE \ -Djavax.net.ssl.keyStorePassword=$KEYSTORE_PASSWORD \ -Jhttps.use.cached.ssl.context=$USE_CACHED_SSL_CONTEXT \ -Jjmeter.save.saveservice.output_format=xml \ -Jjmeter.save.saveservice.response_data=true \ -Jjmeter.save.saveservice.samplerData=true \ -JnumberOfThreads=$NUMBER_OF_THREADS \ -JrampUpTime=$RAMP_UP_TIME \ -JcertFile=$CERTIFICATES_FILE \ -Jhost=$HOST \ -Jport=$PORT \ -JopenConnectionWaitTime=$OPEN_CONNECTION_WAIT_TIME \ -JopenConnectionConnectTimeout=$OPEN_CONNECTION_TIMEOUT \ -JopenConnectionReadTimeout=$OPEN_CONNECTION_READ_TIMEOUT \ -JnumberOfMessages=$NUMBER_OF_MESSAGES \ -JdataToSend=$DATA_TO_SEND \ -JbeforeSendDataWaitTime=$BEFORE_SEND_DATA_WAIT_TIME \ -JsendDataWaitTime=$SEND_DATA_WAIT_TIME \ -JsendDataReadTimeout=$SEND_DATA_READ_TIMEOUT \ -JcloseConnectionWaitTime=$CLOSE_CONNECTION_WAIT_TIME \ -JcloseConnectionReadTimeout=$CLOSE_CONNECTION_READ_TIMEOUT && \ aws s3 cp $TEST_LOG_FILE s3://performance-test-logging/uploads/ && \ aws s3 cp $TEST_RESULTS_FILE s3://performance-test-logging/uploads/ && \ echo -e "\n\n===== TEST LOGS =====\n\n" && \ cat $TEST_LOG_FILE && \ echo -e "\n\n===== TEST RESULTS =====\n\n" && \ cat $TEST_RESULTS_FILE This file can be divided into 9 sections: Select the base Docker image
In this case it was a minimal base image with OpenJDK installed Some bootstrap things to be able to install everything we need later
Set the environment variables
These will be referenced later in the Docker file
Worth noting:You should add your Amazon keys in this section, so that the result and log file can be copied to Amazon S3
These keys need to be changed: apache AWS_ACCESS_KEY_ID=<<Your access key ID>> AWS_SECRET_ACCESS_KEY=<<Your secret access key>> AWS_DEFAULT_REGION=<<Your AWS region>> These should be available in account settings
Install Apache JMeter
Install the WebSocket Samplers made by Peter Doornbosch Copy the test plan (.jmx) to the location indicated in the environment variable
In this case when we build the Docker image, it is in the same directory as the Docker file
Copy some keystore information needed for WSS Expose port 443
This statement doesn’t actually do anything
It is just for documentation
(see: https://docs.docker.com/engine/reference/builder/#expose) The main command where we run JMeter with all the configurations and values we want to pass
This is similar to the command we used earlier to run JMeter as a script (non-GUI)
What we also do here is use the Amazon CLI to copy our JMeter log and result files to Amazon s3 (storage)
This will be explained in the next section
This is it for the Docker part of things
Hopefully there is enough information in this section for you to setup your own Dockerfile
In the next section, we will see how to build the Docker image and upload it to Amazon and run it there
Amazon Web Services Alright, so now you know how to use JMeter to design your test script and how to create a Docker image that sets up an environment needed to run your script
The reason you would want to run these kinds of tests in a Cloud service such as Amazon Web Services (AWS) in the first place is because your Personal Computer has its limits
For a MacBook Pro for example, we could only simulate around 2100 WebSocket connections before we started getting errors stating that “no more native threads could be created”
AWS gives us the ability to run Docker containers on Amazon EC2 clusters which will run the tests for us
In order to do this, you will first need to sign up with AWS: https://portal.aws.amazon.com/billing/signup#/start Once you have your account ready, log in to aws.amazon.com and you will hopefully see something like this: In the search bar, type “Elastic Container Service” and select it
This will be the service we will use to run our Docker container
Elastic Container Service The Elastic Container service is an AWS service that allows us to run our Docker containers in the Cloud
There are three things we are going to discuss regarding the Elastic Container Service: Repository: Where we save our Docker images Task Definitions: Where we define how to run our Docker containers Clusters: Where we start a cluster of VM’s, which will run the Tasks containing the Docker container which contains our JMeter script
This is what it looks like: Repositories Select “Repositories” to get started
As mentioned, this is where we will create a repository to push our Docker image to
Whenever we make a change to our Docker image, we can push to the same repository
Select “Create repository”
First thing you have to do is think of a good name for your repository: Now this next page I personally really like
It is a page which all the Docker commands needed to get your Docker image pushed to this newly created repository
Follow the instructions on this page: It’s always nice not having to think too much… If everything on this page goes well, you should see the following: With the table at the bottom showing all the versions of the image that have been pushed
If you want to push to this repository again, just click on the “View Push Commands” button to see the Docker commands again
I never really used any of the other tabs or buttons on this page, so let’s ignore those
Task Definitions Go to Task Definitions and select “Create a new Task Definition”
When prompted to select a compatibility type, choose “Fargate”
On the next page, simply enter a valid name for the task and scroll to the bottom where the button “add container” is situated
Ignore all the other settings on this page
Click on that button and a modal will show up
Fill in the following: Container name: You’re good at this by now Image: This can be found by going to repositories, clicking on your repository and copying the Repository URI (See last image) Port mappings: 443 (if you are using secure, otherwise 80) Environment Variables: Here you can overwrite any variables that are set in the Docker file
Scroll back up to the Docker section and notice that the Docker file was configured for 1000 threads
If I add an environment variables here with name “NUMBER_OF_THREADS” and set the value to 1500, the test will run with 1500 instead of the default set in the Docker file
Log configuration: Make sure Auto-configure CloudWatch logs is on
Leave everything else open/unchanged
Once you are done with this, click on “Add” to close the container settings modal and then click on “Create” to create the task definition
The new task definition should now show up in the list of task definitions
Now we’re ready to run our test! Clusters Now we’re going to start a cluster of EC2 instances, which will run our task we just defined
Navigate to Clusters and select “Create cluster”
You will be prompted with three options
I usually go with EC2 Linux + Networking
Give the cluster a name and leave everything else the way it is and select “Create”
Note:If you are going to be running a lot of requests and notice later on that the EC2 instance type does not have enough memory or CPU, you can select a larger instance type.See this page for more info: https://aws.amazon.com/ec2/instance-types/ If everything works out you should see a lot of green: Select “View Cluster” to go to the Cluster management page: The last thing to do is to open the “Task” tab, select run new task and fill in the following: Launch type: FARGATE Task definition: The task we defined in the previous sub-section Cluster: The cluster we just created Number of tasks: 1 —- You could add more, however this would mean that you would have the n times EC2 instances performing the same operations
Cluster VPC:Select the first one Subnets: Select the first one The rest can be left unedited
Note: You can still overwrite environment variables for this run specifically by clicking on “Advanced Options” and clicking on “Container Overrides”
You can change any of the values there
Select “Run Task” to finally run your test
Viola! Your test is now running: NOTE:The JMeter Log file and .xml file will be copied to S3 “bucket” specified in the Docker file: aws s3 cp $TEST_LOG_FILE s3://performance-test-logging/uploads/ && \ aws s3 cp $TEST_RESULTS_FILE s3://performance-test-logging/uploads/ && \ S3 is another AWS service used for storage
You can find it under the list of services, in the same way that you found the Elastic Container service
CloudWatch Remember when I told you to make sure that auto config was on for Cloud Watch
This is where it comes in handy
CloudWatch is a service that offers many monitoring capabilities, including log streaming and storing
In this case, we are interested in following our tests as they are being run
Click on the Amazon logo to go back to the screen with all the services, type in CloudWatch and select it
It should look something like this: On the left navigation bar, select “Logs”
You should see a table with log groups
One of the log groups should have the name of your Task Definition that you defined earlier
Click on it to see a list of logs for test runs that you have performed
When clicking on one, you will be able to see the logging of that test
It is also possible to the see the live logging as the test is being performed: As the test is running, the log file becomes pretty big and hard to follow
Here are some keywords you can type into the “filter events” text field in order to get some useful info of the test that is currently running: “summary”: This will show you the JMeter summar of the test, including the number of error percentages
“jmeter”: To see a list of all the parameters used for this test “error”: To see a list of all the errors
You can click on an error log to see more info
Some advice based on experience Here is a list of some things we figured out while running these tests on our application: Feedback is important
Make sure you have enough feedback to be sure that your tests are being performed the way you expect
Some of the things you can do we already covered in this blog, such as setting up CloudWatch and copying the .log and .xml files to s3 to be viewed later on
Other valuable feedback is logging in your application
Set it to Debug mode if possible during these tests
If you have any type of monitoring that can check the server and how many open TCP connections it has would also be great for proving that your tests are successful if you are using WebSockets
You might get errors because of your client rather than your application on the server
If you try to do too many requests or create too many websocket connections from one AWS EC2 instance, you might start to run into client side errors
For us, we could only run around 5000 WebSocket connections per instance
What we ended up doing was creating separate containers and task definitions per 5000 connections and running those tasks simultaneously
Don’t only test your application for number of connections/request, but also test the connection/request rate it can handle
This means decreasing the ramp up time in order to increase the number of connections/requests per second
This is important to know, as your applications might be challenged with this connection rate in the future
For us it was important, since restarting the application meant that all connected devices would start connecting within a certain time period
A single TCP port can handle 64k open connections
If you want to perform a test where you will need more than this, you will have to use another port and have your application listen to both ports
You will also need to have a load balancer of some kind which can distribute the load between the ports
Conclusion I hope this blog has given you an idea of what is possible when combining these three technologies
I know in a lot of the sections I simply told you to do something without giving much of an explanation as to what everything does
There are a lot of things you can do with JMeter, Docker and AWS and I encourage you to look them up and find what works for you
This setup worked for our case, and we are planning on running a test with 100k connections in the near future using this stack
Thanks for reading!Android devices hebben een terug-knop
Dit heeft het voordeel voor de gebruiker om gemakkelijk terug te kunnen waar je vandaan kwam
Voor Android apps is het aan de developer op de juiste manier met de back button om te gaan
Bij het maken van native apps voor Android gaat dat min of meer vanzelf, omdat daar gewerkt wordt vanuit de gedachte van child views, waarbij je met de back button naar de parent navigeert
Bij het maken van apps met bijvoorbeeld Ionic en Cordova moet hier echter bewust over worden nagedacht en logica voor worden geschreven
De terug-knop werkt niet altijd zoals verwacht Om het probleem duidelijk te maken ga ik uit van een eenvoudige app met een menu om naar de verschillende views binnen de app te kunnen navigeren
Het menu heeft een aantal items, bijvoorbeeld ‘dashboard’, ‘aanbiedingen’, ‘instellingen’ en ‘profiel’
De gebruiker bevindt zich op het dashboard en klikt in het menu op ‘instellingen’ De app navigeert naar de instellingen view De gebruiker klikt nu op de terug-knop en verwacht terug te komen op het dashboard Een Angular applicatie navigeert m.b.v
de url
Klikken op de back button brengt de gebruiker naar de vorige url
Maar wat gebeurt er als de gebruiker een paar keer tussen twee pagina’s in het menu klikt; De gebruiker bevindt zich op de instellingen view en klikt in het menu op ‘profiel’ De gebruiker vindt daar niet wat hij zoekt en klikt in het menu op ‘instellingen’ Ook bij ‘instellingen’ vindt hij niet wat hij zoekt en gaat via het menu toch weer terug naar ‘profiel’ Bij ‘profiel’ herinnert hij zich opeens dat hij iets zag bij ‘instellingen’ wat hij zoekt en gaat via het menu opnieuw naar ‘instellingen’
Als de gebruiker uiteindelijk heeft gevonden wat hij zoekt en klaar is, klikt ie op de terug-knop van het device en verwacht terug te komen op het dashboard Omdat een Angular applicatie navigeert m.b.v
de url zal de gebruiker bij het klikken op de back button niet naar het dashboard gaan, maar eerst een aantal keer heen en weer navigeren tussen ‘instellingen’ en ‘profiel’ voordat hij uiteindelijk op het dashboard belandt
Dat is ongewenst/onverwacht gedrag van de applicatie! Hoe gaan we om met navigeren tussen views
In een Angular Cordova applicatie kan een eventListener op de terug-knop worden gezet
javascript document.addEventListener("backbutton", (ev) => { ev.preventDefault(); }, false); Met deze code snippet wordt voorkomen dat heen en weer genavigeerd wordt
Maar dit is niet genoeg, want nu doet de back button helemaal niks meer
Hiermee wordt een hardware knop van het device helemaal uitgeschakeld en dat is nogal een rigoreuze beslissing
Er zijn verschillende manier om dit aan te pakken; We zouden voor elke view kunnen definiëren wat de parent view is
Klikken op de terug-knop moet de gebruiker dan niet naar de vorige view brengen maar altijd naar de parent
Dit werkt niet als een pagina vanaf 2 pagina’s bereikbaar is; wat is dan de parent
Zolang de applicatie een eenvoudige navigatie structuur heeft, waarbij er een dashboard bestaat, overzicht (level 1) pagina’s en detail (level 2) pagina’s, kunnen we voor elke view definiëren welk level het heeft
In dat geval zijn er 3 mogelijkheden; Als de huidige pagina een detail (level 2) pagina is, gebruik dan de history.back() om uit te zoeken waar de gebruiker vandaan kwam
Als de huidige pagina een level 1 pagina is, ga dan naar het dashboard
Als de huidige pagina het dashboard is, vraag de gebruiker dan of hij de applicatie wil afsluiten
Hiermee kan dan de volgende route logica worden opgesteld; javascript document.addEventListener("backbutton", (ev) => { ev.preventDefault(); historyService.onBack(); }, false); javascript export class HistoryService { onBack(): void { // if page is level 2
then go back to previous page // else if current page is dashboard
then open exit app confirm modal // else go to dashboard const currentRoute = getCurrentRoute() if (currentRoute.level > 1) {// current view is a level 2 view history.back(); } else if (currentRoute.default) {// current view is the dashboard this.confirmExitApp(); } else { this.navigateToDefaultView(); } } this.getCurrentRoute(): Route { // this function should return the current route as een object with it's level // this logic goes beyond the scope of this article } confirmExitApp(): void { // this function should open a confirm modal with the question to exit the app // on confirm close the app with navigator.app.exitApp(); // on dismiss close the modal // this logic goes beyond the scope of this article } navigateToDefaultView(): void { // this function should navigate to the dashboard // this logic goes beyond the scope of this article } } Deze logica werkt alleen als het menu alleen level 1 pagina’s en eventueel het dashboard bevat level 2 pagina’s alleen bereikbaar zijn vanaf level 1 pagina’s Een level 2 pagina kan vanaf 2 verschillende level 1 pagina’s bereikbaar zijn, zolang de level 2 pagina maar niet vanaf een andere level 2 pagina bereikbaar is
Deze eenvoudige navigatie structuur heeft ook als voordeel dat een gebruiker nog begrijpt waar hij zich bevindt
Als de applicatie level 2, level 3 en misschien zelfs level 4 pagina’s zou hebben, of als een level 3 pagina bereikbaar is vanaf een level 2 pagina die bereikbaar is vanaf een level 4 pagina, dan wordt het moeilijker voor de gebruiker voor te stellen waar in de applicatie hij zich bevindt
Natuurlijk zullen er situaties zijn waar toch een level 3 pagina bereikbaar is
Al is het voor de gebruikersbeleving af te raden, zolang de level 2 pagina alleen vanaf de level 3 pagina bereikbaar is met de terug-knop of met een button die van history.back() gebruik maakt, blijft de route logica werken
State changes en navigeren binnen een modal Voorbeeld 1: De gebruiker gaat zijn wachtwoord aanpassen bij ‘profiel’; De gebruiker klikt in het menu op ‘profiel’ Op de profiel view klikt de gebruiker op een knop ‘wachtwoord aanpassen’ Dan opent een modal met 2 invoervelden om een nieuw wachtwoord in te voeren De gebruiker vult een nieuw wachtwoord in en klikt op een knop ‘verder gaan’ De modal toont dan een scherm waarin hij zijn oude wachtwoord moet invullen ter bevestiging Op dat moment wil de gebruiker toch een ander nieuw wachtwoord kiezen en klikt op de back button van het device De gebruiker verwacht dan dat de modal geopend blijft en de 2 invoervelden om een nieuw wachtwoord in te voeren worden getoond Voorbeeld 2: De gebruiker wil een lijst met aanbiedingen als bulk aanpassen; De gebruiker klikt in het menu op ‘aanbiedingen’ Op de aanbiedingen overzichtsview klikt de gebruiker op een knop ‘aanbiedingen aanpassen’ Dan verandert de state van de view in een ‘edit’ state, waarbij verwijder- en sorteerknopjes verschijnen Op dat moment bedenkt de gebruiker zich en klikt op de terug-knop De gebruiker verwacht dan dat de ‘edit’ state verandert in de ‘view’ state waarmee hij op de pagina terecht kwam Helaas zullen de verwachtingen in beide voorbeelden niet uitkomen en zal de applicatie netjes, zoals bedacht naar het dashboard gaan
Om dit soort gevallen te kunnen afvangen willen we een lijst bijhouden waarin staat wat er op welk moment moet gebeuren als op de back button wordt geklikt
Soms zal genavigeerd moeten worden naar een andere view, in andere gevallen moet een bepaalde state veranderen naar de vorige state of moet een modal sluiten
En ook hier geldt weer dat als je een paar keer tussen 2 states wisselt, dat klikken op de terug-knop niet opnieuw in omgekeerde volgorde tussen de states gaat wisselen
En nog specifieker; als je op een ‘bewerken’-knop klikt en dan op ‘opslaan’ dan moet de terug-knop niet naar de ‘bewerken’ state gaan, maar de terug functionaliteit uitvoeren die uitgevoerd zou worden voordat je op de ‘bewerken’-knop klikte
Een ‘history stack’ Door een ‘history stack’ bij te houden kunnen we definiëren dat bijvoorbeeld bij het openen van een modal de stack wordt opgehoogd met een functionaliteit die de modal sluit
En bij het sluiten van de modal zouden we alles van de stack kunnen verwijderen tot en met de functionaliteit die de modal sluit
De logica voor de back button wordt uitgebreid met een check of er een history stack bestaat
Als die bestaat, dan wordt de laatste functionaliteit in de history stack uitgevoerd en dit item wordt vervolgens verwijderd van de stack
javascript export class HistoryService { private historyStack: HistoryStackItem[] = []; onBack(): void { // if history stack exist, run last callBack and remove this item from stack // else if page is level 2
then go back to previous page // else if current page is dashboard
then open exit app confirm modal // else go to dashboard const currentRoute = getCurrentRoute(); if (this.historyStack && this.historyStack.length) { const lastItemOnStack = this.historyStack[this.historyStack.length - 1]; if (lastItemOnStack.popByBackButton) { this.historyStack.pop(); } lastItemOnStack.callback(); } else if (currentRoute.level > 1) {// current view is a level 2 view history.back(); } else if (currentRoute.default) {// current view is the dashboard this.confirmExitApp(); } else { this.navigateToDefaultView(); } } setHistoryStack(callBack): void { this.historyStack.push(this.createHistoryStackItem(callBack, true); } removeHistoryStack(): void { this.historyStack = []; } private createHistoryStackItem(callBack, popByBackButton): HistoryStackItem { return { callBack: callBack, popByBackButton: popByBackButton }; } ..
} De setHistoryStack kan worden aangeroepen bij het openen van een modal of het veranderen van een state
Ook kan de ‘history stack’ worden geleegd om ervoor te zorgen dat daarna weer genavigeerd wordt op basis van level n views
Terug-knop afhandelen is eenvoudig De history service is redelijk eenvoudig en effectief! Hiermee vang je de meeste situaties af, waarin de verwachting bij het klikken op de terug-knop afwijkt van history.back();When I was a young backend developer SOA was hot
Every enterprise application consists of web services exchanging XML messages over SOAP
The functionality offered by these web services were described in a WSDL file (Web Services Description Language)
This WSDL is the contract both parties use to communicate with each other
It specifies how the service can be called, what parameters it expects, and what data structures it returns
Normally there were two ways we would work with WSDL’s
Contract last We just started coding the functionality in Java
With the frameworks we used we just add some annotations or comply to a certain convention and the web service was published
From the code a WSDL could be generated and threw over the fence to the other party, good luck with it
Contract first When working contract first the WSDL is created beforehand
And at the moment the WSDL got finished we started implementing, mostly by starting with generating Java code from the WSDL
I remember we could always have fiery discussions in the team whether to work contract first or contract last, but in the end we always had a contract
When we delivered the web services, the WSDL was indisputable the single source of truth, everyone was happy
In the past everything was better, right
REST Fast-forward a few years and all new services should be RESTful with JSON
Instead of those verbose XML files we transfer lightweight JSON files
When developing my first REST endpoints I loved the conciseness and flexibility, but deep down I also missed the firmness I got used to when working with WSDL’s
Some WSDL-like tooling for REST arose in the form of WADL (Web Application Description Language)
This describes the REST endpoints, parameters, data structures in XML quite similar to WSDL
After working with that flexible and concise JSON, putting a lot of effort to create a WADL in XML just didn’t felt right
So I took a look at it, but it never occurred to me to really use this
Until today I never came across a WADL in the wild wild REST world
Swagger Around 2011 Swagger came around
Now we could add the Swagger library to our project that provides some annotations
Based on these annotations documentation about the REST interface could be generated like this: With minimal extra effort (adding some annotations) we’re almost at the point of contract last development of REST endpoints
In addition Swagger gives us an easy-to-use tool to test the REST endpoints
Also useful for other parties who write software to integrate with this interface
But still, this is no contract
We have useful documentation of the REST interface for humans, but it still lacks a format that is indisputable, unambiguously, and preferably machine readable
OpenAPI So this is where the OpenAPI Specification (OAS) kicks in
Originally developed by Swagger and meanwhile donated to the Open API Initiative
With the OpenAPI Specification we can create a single interface file as contract in JSON or YAML
From this interface file documentation of methods, parameters and models can be generated
Also REST clients and servers for most major languages/frameworks can be generated
Getting started The easiest way to get in touch with OpenAPI is by opening the online editor at: editor.swagger.io
By default this loads the pet store API which is a comprehensible sample project
The specification (in YAML) can be edited in the left pane of the editor
The righthand side contains the documentation and is refreshed on the fly
Seeing your changes and their effects being applied in real time is a great way of developing your contract! When working on your private project you may not want to edit your specification in the public editor
This same editor can simply be installed locally via NPM
To get started I created this Git repo that provides a decent starting point
If you have any questions, file a new issue in Github or ask it here below in the comments
Final thought I gave a brief starting point and now it’s up to you to play around and use it for real
Most of us programmers are a lazy bunch and we prefer to automate as much as possible
Today I will cheer on this habit and tell you to “never ever write your REST interface code by hand”
Let it be generated based on an indisputable contract! References: swagger.io apis.guru/awesome-openapi3 list of tools around the Open API Specification from code generators to editors swagger.io/blog/difference-between-swagger-and-openapiIn product development advanced tooling and so-called ‘proven' development methods can sometimes cause more problems than they solve
For instance when they distract the development team from what they should really be focussing on: the end result, the product itself
Although Scrum provides a healthy framework that minimises risk and maximises agility of the development process, I’ve seen Scrum teams completely go astray at the expense of the product
In this blog I will outline a recurring pattern I have noticed in Scrum development teams and I will share some of my ideas on how teams can keep the right focus within Scrum
When I think about product development and Scrum, I see that the product and the development team often live in separate worlds
The world of the product is ruled by business and user goals, while the world of the development team is ruled by sprint goals
These are not the same kind of goals
For instance, a user goal could be to easily order a coffee-to-go, while a sprint goal could be to implement single sign-on
Like I said: different worlds
During this whole process the development team is often ‘trapped’ inside their own sprint bubble
So the development process goes something like this: During a sprint new functionality is added to the product and gets released to the market after it’s considered done
This new functionality triggers some kind of response of the business and/or the users
This response (or the lack of it) might then be translated back into new product requirements that are fed back to the development team
During this whole process the development team is often ‘trapped’ inside their own sprint bubble
They are lulled to sleep by what I call ‘the Scrum Pendulum’; the constant rhythm of the sprints and the hyper-focus on the sprint goals
Most of the time they are unaware of the effect of their work on the actual product, unaware also of the effect the product has on the business or the users
The Scrum Pendulum creates a ‘detached’ development team, that might very well be meeting its sprint goals, but has no idea whether it is moving the product in the right direction
This means you could also regard the PO as a SPOF: a Single Point Of Failure
Most of the time the only link between the development team and the product is the product owner (PO)
The PO has a pretty crucial role in the development process; she (or he) is responsible for translating business and user goals to product requirements and also for deciding whether new functionality that has been developed is fit te be released to the market
So you can imagine that if the PO fails, the whole process will fail
This means you could also regard the PO as a SPOF: a Single Point Of Failure
And that is something I think you need to avoid in product development
As a team you should define what ‘success’ means
So what can a development team do to prevent the Scum Pendulum from happening
I think it is key to take a shared responsibility for the end result, the product
Even if this was not explicitly asked by your PO, manager or client
As a professional engineer, designer or tester I believe you have a responsibility that goes beyond the development team
As a team you should define what ‘success’ means: when do you think your team has delivered a good job
You might not all agree on this, which is even more reason to discuss this with your team members and create a common understanding of ‘success’
For example, for a team ‘success’ might mean a minimum of 4 stars in the App Store rating (if it is an app you are developing)
It could also be something like acceptable response times during heavy traffic or a good overall product usability
It doesn’t matter what you as a team define as ‘success’, the point is that you start thinking about product quality beyond the sprint boundaries
Defining success is a useful exercise in itself, but it becomes even more valuable when the development team measures it’s success using their own defined standard
This doesn’t need to take a lot of time or be very complex
Checking the app store rating and comments will often give a lot of insight
Also, a quick-and-dirty user test is really simple to conduct and doesn’t need a lot of preparation
The results might not be 100% representative, they do give some level of insight
Which is a lot more to go on then when the development team would forever remain inside their sprint bubble
An important last detail is of course to use the insights to learn and improve the work inside the sprints, and everybody will benefit
Don’t let Scrum (or any other development method or tool) replace common sense
This team’s definition and constant evaluation of it’s own success creates an ‘outer loop’ around the Scrum process that provides a backup for failing (or even non-existing) product owners
But more importantly, it prevents the team from getting too detached from the product and the world of the business and the users
Don’t let Scrum (or any other method or tool) replace common sense
And don’t hide behind your sprint commitment; product quality and success are a shared responsibility
Want to learn more about how to avoid the pitfalls of tooling and methods in product development
I’ll be sharing my thoughts on this at the DevCon 2018 (April 12, Ede) and CodeMotion Amsterdam 2018 (May 8, Amsterdam) conferences
Or sign up for the training Product Thinking at the Luminis AcademyDisplaying a lot of data in a grid can be a hazard
The browser can show performance issues when the DOM gets too large
But still we want to present our data to the user in the most friendly way, and it has to look awesome and give a great experience at the same time
Is it possible to have over 1000 rows and 1000 columns in your DOM ready to be scrolled by the user
In this blog post I’m going to show you what the problem is with large data tables, how to fix it with available solutions, and as an alternative I’m going to show the concept of virtualization of data using the CSS grid
Unwanted behaviour Let’s create a small application to expose the wound
I’ve created a blank Angular application using the Angular-cli tool
Within this application, I create a table with 100 rows, and 100 columns
The browser doesn’t seem to have any trouble rendering this data, and scrolling through the data goes nice and easy
To get to a point that the browser needs serious time to render the table, I increase the rows and columns to 1000 each
Now the browser needs to work hard to render the table
When I clocked it, it took around 30 seconds to render the table, and I’m unable to scroll fluently through the table
Check out this plunker to see what happens
If it loads without any issues, try to increase the table rows and columns in app.component.ts
javascript private totalColumns = 1000; private totalRows = 1000; Solution This unwanted behavior is caused by adding to much elements to the DOM
To fix it, we need to reduce the DOM size
This can be achieved by virtualization of the data
There are libraries available which make this possible, for example: AG-grid Angular2 virtual scroll NGX infinite scroll Next to these out of the box solutions, I want to share a concept of virtualization and scrolling through this large amount of data by using the CSS grid
In this concept, virtualization on x- and y-axis can be achieved
Positioning of the data will be taken care of by the CSS grid
Data virtualization with the CSS grid The unwanted behaviour was exposed by creating a table with 1000 rows and 1000 columns
Instead of positioning the data in a table, I use the CSS grid which positions the data by the CSS properties; grid-column and grid-row
In this example, I create a CSS grid which specifies the size of an implicitly-created grid column, and grid row
MDN web docs – grid-auto-columns MDN web docs – grid-auto-rows javascript 'grid-auto-columns': this.columnWidth + 'px', 'grid-auto-rows': this.rowHeight + 'px' The CSS grid creates columns and rows based on the elements that are positioned within the grid
I add 1 element that spans the entire grid to give the grid a body within the DOM
This provides the container element, which holds the CSS grid, with overflow information in order to scroll
MDN web docs – grid-column MDN web docs – grid-row javascript 'grid-row': '1 / span ' + this.totalRows, 'grid-column': '1 / span ' + this.totalColumns ** The keyword ‘span’ isn’t required, you can also define the span by only using the ‘/’
The grid is now ready to position the data
For adding the data to the DOM, I define the visible cells by taking the size of the container element and divide it through the column width and the row height used in the grid definition
When calculating the visible cells, I add some buffer cells to make sure the user always sees data
javascript const containerWidth = this.el.nativeElement.firstElementChild.offsetWidth; const containerHeight = this.el.nativeElement.firstElementChild.offsetHeight; this.totalVisibleColumns = Math.ceil(containerWidth / this.columnWidth) + bufferCells; this.totalVisibleRows = Math.ceil(containerHeight / this.rowHeight) + bufferCells; At this point we have: A CSS grid with a total row and column span where we can scroll through Columns and rows to position the data Elements in the DOM to fill the container with data When the user scrolls, the data scrolls out of view and the user is left with an empty canvas
To get new data in view for the user, I bind to the scroll event on the container element
When the scroll event triggers, I accomplish to following: Calculate the first visible row based on the scroll offset and the row height Calculate the first visible column based on the scroll offset and the column width Determine whether rows should be moved and update Determine whether columns should be moved and updated The scroll offset can be used to determine how many data rows and/or columns are scrolled out of view
The data rows and columns that are scrolled out of view need to be replaced with new data
html <div class="container" (scroll)="onScroll($event)"> javascript public onScroll(event: Event): void { const firstVisibleRow = Math.ceil((event.srcElement.scrollTop) / this.rowHeight); const firstVisibleCol = Math.ceil((event.srcElement.scrollLeft) / this.columnWidth); if (firstVisibleRow !== this.lastRowIndex && firstVisibleRow + this.totalVisibleRows <= this.totalRows + 1) { this.shiftToRow(firstVisibleRow); this.lastRowIndex = firstVisibleRow; } if (firstVisibleCol !== this.lastColIndex && firstVisibleCol + this.totalVisibleColumns <= this.totalColumns + 1) { this.shiftToCol(firstVisibleCol); this.lastColIndex = firstVisibleCol; } } ** I created virtualization for rows, and later I duplicated the code to virtualise the columns as well
This code can be optimised, but for explaining and showing the concept I made it quick and dirty
All the information is there
The location of the first row that needs to be shown, the first column that needs to be shown, and the total of visible rows and columns
I update the elements within the DOM on 3 points; column index, row index, and data to display
javascript private shiftToRow(row: number): void { let gridCellIndex = 0; for (let i = 1; i <= this.totalVisibleColumns; i++) { let shiftToRow = row; const colIndex = this.lastColIndex + i; for (let j = 1; j <= this.totalVisibleRows; j++) { const dataCell = this.getDataCellByPosition(colIndex, shiftToRow); const gridCell = this.gridCells[gridCellIndex]; if (dataCell) { gridCell.col = colIndex; gridCell.row = shiftToRow; gridCell.data = dataCell.id; } shiftToRow++; gridCellIndex++; } } } Conclusion There are a lot of ways to keep the size of the DOM to a limit to make sure the browser doesn’t get performance issues
Out of the box solutions offer a quick way to make this happen
The concept I’m offering allows you to add 2 dimensional virtualization of data
Only the data and some CSS properties need to be updated during scrolling, which makes it perform fluently and easy to maintain
You can take a large step by clicking on a scroll location within the scrollbar or set the scroll offset through Javascript and the table loads the correct data, since it is based on an offset location
So if you need a solution where the position of the data is key, the CSS grid with data virtualization is what you are looking for! That’s all
You can play with the code hereNon-fiction books can be insightful, but I have noticed that I’d expect those insights to come back to me right when they would be helpful –– like when there’s an obstacle in a project, I will have an “a-ha” moment and remember that great idea I have read about years ago
In reality, not so much
So I’m trying something new
After reading a book, I’ll try to write down the most important stuff that could be useful some day in my digital notepad
I did this with the book Digital Transformation Book by David L
Rogers as well, a book I’ve written about before (for an introduction of the book, start there)
My notes quickly got annotated with my own experiences, and as I thought that would be valuable to share, I started to write this post
Rogers discusses in his book the digital transformation that requires pre-internet companies to rethink their business and way of doing things
Two themes are repeated throughout, and are obviously connected: Be agile and adapt; and have a company culture that is acceptable of change
Although primarily aimed at (C-level) managers, there is definitely some good stuff in it for designers and developers regarding those themes
Everything Is Marketing In this digital age the network of customers and stakeholders has grown and become more complex
Customers are no longer passive consumers that are just at the receiving end; they now interact with the brand, the market and each other
This also means that the number of touch points in the customer journey is growing, turning marketing into a company-wide activity
For the customer there is only one experience with the company: Using the product, visiting the company’s Twitter page or asking for support by email, it is all the same brand to the customer
All that should be in sync to such a degree that the customer will have consistent experience of the brand (and preferably a positive one)
In traditional companies where different departments take care of specific parts of the brand, it will be hard to achieve this
Those walls needs to be taken down, the silos dismantled
What can you do regarding this one brand perspective
To be more sensitive to the outside world and more customer-centric
Quite easily actually: Work together within your team beyond your own expertise to create a great product or service
We call this product thinking and my fellow designer Ivo Domburg wrote about this earlier in Dutch
On top of that, you can talk to people of other departments, outside of your team
Ask the people at customer service what kind of issues customers are experiencing with the product
Get to know what content is created for the website and social media accounts
There is no excuse here, you can literally walk over to the next room or floor to find these insights
Developing Great Products Let us zoom in on the development process
Concepts like working agile and having a culture that is acceptable of change should make sense to us as developers and designers
However, some companies or clients are less familiar with this mindset
There are some ways to do your part to help your company or client improve on this, even within a project
Rogers describes in his book experimental processes and principles based on what he observed at successful companies, which I have remixed using my own experience into the following five principles that can guide you in that process
Question The Assumptions In the beginning of a project, assumptions are made and new ideas are born founded on hunches, not data
That is a good thing, because you want that outside of the box creativity
But accepting these assumptions as the truth without verifying them is just gambling the whole project on a brainwave
These assumptions sometimes become a dogma within the company –– never questioned but also never proven to be right
Test those assumptions and various ideas early in the project and learn from them
Even if ideas are proven worthless, it could still lead to other insights and new ideas
Innovation means you wonder into unknown territories, so all you know could be worth being challenged
And this is not only true for new products but works evenly well if you are working on a new feature or improving an existing product
Iterate From The Start, And Get Consumer Feedback Quickly It happens quite often that weeks are spent on requirements and scoping of a project, resulting in a hefty backlog
Everything is pretty much set in stone, and it is hard to change direction if the first consumer response is timid
Therefore it is better to bring the concept as quickly as possible to real customers to get feedback and iterate on that
And work on the main concept here, don’t linger on the details –– if the concept doesn’t stick, it probably won’t be due to a detail! When doing this, speak to real or potential customers to get that feedback
Present your concept preferably in a form that is realistic enough for a consumer, even if it is still in the early stages
The person you show it to needs to get a pretty good sense of what the product or service is, how it could work and what the benefits for him or her would be
Not everyone can interpret abstract storyboards or wireframes the right way
Kick-off With An Intensive To kickstart a project without loads of requirements, we at Luminis do, as we call it, an Intensive with our client
It is short: It all happens in one day
With around five people (some designers and engineers from us and a few experts from the client, and perhaps even a end-user) we have a small and productive team
In the morning we start with the domain and marketplace, and the client discusses the potential he or she sees
In the afternoon we challenge assumptions, generate ideas and finish with some concepts or a first prototype at the end of the day
The Intensive not only helps to get us quickly into the context, it helps the client to get an outside perspective and some solutions that might work
And all within a day, making it fast and cheap
The results then can be shown to customers to get the first feedback
Fall In Love With The Problem, And Not The Solution Maybe the biggest reason the Intensive works is because it helps us and the client to understand the problem better – that is also why you should start with the marketplace and customer (and not that new technology everyone’s talking about)
The result of the Intensive itself is used to learn more about the problem
If the project continues after this kick-off session, we “throw” away the solution and continue with the problem
Falling in love with the problem helps you to have the right focus
It makes you consider what customers really want or are struggling with; that is where you should start
Do you already have feedback on existing products
Great, start from there
Can you observe the customer in the specific context you are interested in, even better
This helps you to see the bigger picture and it could even unexpectedly provide a solution
Falling in love with the solution is a trap
It limits the number of solutions you consider (because why look further if the team’s really happy with the solution on the table?), so it risks leaving a potentially better solution to be undiscovered
Loving a solution prevents you also from letting go of it when the response on it is too moderate or a better solution comes along
If an idea get’s the team really excited, it is worth putting it aside for a few days before jumping on it
Keep looking for other, maybe better, possibilities
The rush you get from that great idea could very well be worn off when you have another look at it two days later
Solving the right problem right is the goal to create truly great products
Fail Smart Finding the right solution and developing a great product won’t happen all the time
Some things may not work, and you and your team fail
It is important for all to expect this as a possible outcome and it shouldn’t be regarded as a negative event per se
When it happens, it does require a moment of reflection: Why did it go wrong, and did it happen as early and as cheaply as possible
And with answers to these questions, it is possible to improve the process
Keep in mind that you now only know what doesn’t work
Reflection is good but don’t let the failure limit the creativity or turn into a bureaucratic rule that becomes an obstacle in this or the next project
And reconsider sharing it with your colleagues and other teams
The next time the context could differ so much that it renders the learnings of the failure useless in that case
Process failures are more relevant to share than product failures
Make Great Products Whether if you work at a company that existed before the internet or work at a startup, the lessons described above could help you equally well
The concepts, processes and principles are valuable for all those involved in the development of digital products and services
A lot is about being open-minded, breaking through dogmas, and understanding the real issue the consumer has that requires a solution
Focussing on the problem, loving it, understanding it, and than solving it with a great product, that should keep you going and innovatingRecently, a customer reported that the performance of the web application we are building, an AngularJS front end using REST services from a Java back end, is unsatisfactory
I had a fun time finding out what the problem was, understanding what was causing it and thinking up solutions
I’d like to share the experience with you
What is the problem
“Bad performance” can mean many things
Actual response times are only part of the story; an application that shows feedback while gathering a response can make you wait longer before it is perceived as slow
When users know their bandwith is limited, for instance on a mobile device on a slower network, they anticipate a slower site
But of course, it is always possible that the services on which your site relies are just too slow
So, first order of business: ask the reporter what he means
What action did he perform, and how long did it take
How long did he expect it to take
It turned out there were several problem areas, not all of which were actually related to performance
I will describe two of these areas in more detail: loading of the AngularJS application loading of list views Loading of the AngularJS application Our AngularJS application has to perform several tasks before it can render the first view
This takes time, and during this time the end user is staring at a blank page
Finding out what happens To figure out what tasks are executed and which of these can most easily be sped up I opened a fresh Chrome window, trashed all the caches, opened the network panel and loaded the main page: This is interesting
There is a flurry of activity at the start, followed by 300 ms of silence, after which a lot more is loaded
What does it mean
Among the first items to be loaded is our JavaScript library, including AngularJS and other components
This is a large file, so it takes several hundreds of milliseconds to get loaded
After that, the AngularJS application is initializing (nothing happens) and after that, additional data is downloaded as the AngularJS application is building its first view
The largest of these is a translations file
The application won’t show anything until the translations are loaded, so the limiting path is: Loading of the JavaScript libraries Initialization of the AngularJS application Retrieving the translations file Improvements The initialization of the AngularJS application is not an easy one to speed up, so I’ll concentrate on the other two parts
The time it takes to download a JavaScript library is largely determined by its size
The library was already minified during the build
However, when a project is running for some time, it is not uncommon to have solved the same problem in two different ways, using two different libraries, or to end up with unused libraries
As it turned out, we were using two different libraries for file uploads
Choosing a single library for the uploads and removing libraries that are no longer used reduced the size of the final, minified JavaScript file and improved load time
The contents of the translations file were all used
However, since this is a plain text file, it was a prime candidate for zipped transfer
AngularJS handles zipped files out of the box, so the only thing we needed to do was add an @org.apache.cxf.annotations.GZIP annotation to the Apache CXF resource in the back end, which reduced the download time from several hundred milliseconds to ~10 ms
List views List views are always tricky, especially if there is no limit to the number of elements that may show up in the list, as was the case in some of our services
However, that turned out to not even be the biggest problem
Feedback: always When opening a page with a list, the application would show a spinner to indicate that the contents of the list were still being retrieved
The spinner would be removed, and the list shown, when the variable containing the list data was present on the scope: java <double-bounce-spinner ng-if="projects === undefined"></double-bounce-spinner> <div ng-if="projects !== undefined"> <ul> <li ng-repeat="project in projects"> <div><!-- project data here --></div> </li> </ul> </div> However, the controller that retrieves the projects had no error handling: java projectService.query() .then(function (projects) { $scope.projects = projects; }); So, if the back end service returned any error, the projects variable would not be set and the spinner would keep spinning - which made the testers conclude that the application was very slow! You should always provide feedback on the outcome of a back end call
At the very least, a spinner suggesting that data is retrieved should always be removed when a response is received
Showing the spinner should depend on whether a call is being performed, not an the presence of the result: java <double-bounce-spinner ng-if="loadingProjects"></double-bounce-spinner> java $scope.loadingProjects = true; projectService.query() .then(function(projects) { $scope.projects = projects; }) .finally(function() { $scope.loadingProjects = false; }); Even better would be to provide an error message if the back end returned an error: java $scope.loadingProjects = true; projectService.query() .then(function(projects) { $scope.projects = projects; }) .then(function(error) { // Process this to show an appropriate error message in the view }) .finally(function() { $scope.loadingProjects = false; }); Slow REST services Of course, there’s still the problem of REST services simply taking a long time to return a response
To monitor the response times for these services, we added simple metrics to all the REST services using Java Simon
This is a simple framework to gather performance data
You request a Stopwatch and use it to measure the call durations on the resource; it will then provide you with minumum, maximum and mean call durations: java @GET @Produces(MediaType.APPLICATION_JSON) public Response getProjects( final Split split = SimonManager.getStopwatch("getProjects").start(); final Response response; try { response = Response.ok(projectManagement.findAll()).build(); } finally { split.stop(); } return response; } java Stopwatch stopwatch = SimonManager.getStopwatch("getProjects"); long minimumDuration = stopwatch.min(); // minimum duration in nanoseconds long maximumDuration = stopwatch.max(); // maximum duration in nanoseconds long meanDuration = stopwatch.mean(); // mean duration in nanoseconds We then used Prometheus to provide access to the measurements
There are of course many reasons for a service to be slow, so I won’t go into details here, but this setup provided insight into which services are consistently slow so we could focus improvement efforts on those servicesJust recently I’ve completed a UI redesign for a client’s existing application
The app is part of a bigger solution used at warehouse floors for picking orders
Throughout the years the application has grown with multiple customisations and fixes
When a customer opted for a new feature that was incompatible with this legacy and the UI, it became clear to the client that an overhaul was needed
As the designer for this project, the assignment for me was two-fold: Develop a UI that is flexible enough for customer-specific functionality, and create a clear interface that is usable for new users with as little explanation as possible while maintaining a low error-rate regarding picking
Our client already had some good ideas on the new app but I suggested to go into the field as well: How is the existing solution used and what do its users think about it
I hopped in a car with a sales rep to one of their long-term customers to gain these insights
Observing the users and the picking process was definitely helpful to understand the context better and see what is relevant in the app
However, the pickers worked lightning fast and had low error-rates, and even after talking to them, I only got one true issue with the current solution: An often-used button was so small that while working fast, it required an annoyingly slow and precise press by the picker, disrupting their flow
This is a crucial moment in the design process
Are you satisfied with your research
Visiting one customer and observing several end-users is in the end still n=1
Things like their picking process and their company culture determine a lot about what is observed during a visit, and are obviously very different at another company
It is always a good idea to have more observations to get a better sense of the product or service, and it is best to make sure the next customer or end-user differs enough from the first to cover the whole spectrum of the product’s users
In this case, the long-term customer had embraced the solution a long time ago, embedded it completely in their way of working and worked around problems they’d totally forgot about by now
So, the sales rep and I visited a second customer that was the opposite of this customer
Instead of being familiar with the solution, having permanent employees and running the warehouse for their own online shops, the second customer just started with this picking solution, they hired temporary workers and offered the warehouse services solely to other online shops
This second visit resulted in a very different afternoon
Being new to the app, they had several remarks and even suggestions for improving it
Not only did this visit confirm the necessity of the redesign as well, it also provided sufficient insights to make sure the new app would fit the domain and the use cases
For me, and likely most designers, starting the creative process with users, observations and tests is necessary to ground the solution in reality – to create a satisfying product or service that offers true improvement and fulfils a need
The experience of the above-mentioned project shows that context makes all the difference
Select a diverse range of users for your research: Don’t stick to one observation, you’re likely to miss out on a lot of insights for making a truly great productLast week my colleague Piet claimed: “You shouldn’t need several hours to understand what a method, class or package does”
Since unit tests are written in classes and methods, the same holds here
Welcome to the next episode of “reducing mental effort for software developers”
In this post I will lay out how AssertJ can help to reduce the mental effort needed while reading and writing test code, and as a bonus how it reduces the effort needed for understanding results of failing tests
AssertJ is a library that provides fluent assertions for Java
Before I dive into the fluent part, let’s start with some examples of assertions
Suppose you want to check that a String is of a certain value
In JUnit this will be done in the following way: Clean code reads like well-written prose java assertEquals("expected", "result"); In natural language this statement can be described as: “assert that expected and result are equal”
The same check with AssertJ can be done with: java assertThat("result").isEqualTo("expected"); Comparing to JUnit, the two values are in a reversed order
With assertThat() you specify which value you want to check, followed by isEqualTo() you specify to which value it should comply
Now the statement is expressed in a way closely to that of natural language
If you would strip the punctuation marks and “de-CamelCase” it, you’ll get the sentence: “assert that result is equal to expected”
My English may not be perfect, but this statements sounds a lot more like a sane and natural sentence
Because the Strings of these two examples are unequal, these tests will fail with the message: java org.junit.ComparisonFailure: Expected :expected Actual :result Sometimes I come across unit tests where expected and result are swapped like this: java assertEquals("result", "expected"); This is correct, but can be confusing when you’ve broken some tests and reading the message: java org.junit.ComparisonFailure: Expected :result Actual :expected In this example it’s quite obvious that something is wrong in the test, but imagine that in more obscure situations you’ll need a lot more mental effort before you find out what’s wrong and why the test is failing
AssertJ does not offer bullet proof protection against these kind of programming errors, but it will reduce the chance
A bell should ring when you read or write: java assertThat("expected").isEqualTo("result"); We don’t want to know if our expectation is correct! We want to know if the result is correct, i.e
that it meets our expectation
These equals checks are simple examples to make a clear difference between plain JUnit and the fluent assertions of AssertJ
The real power of fluent kicks in when applying multiple assertions in one single statement
For example: java assertThat(alphabet) .isNotNull() .containsIgnoringCase("A") .startsWith("abc") .endsWith("xyz"); As we’ve seen before, this statement reads like natural language
In JUnit on the other hand the equivalent test will read like: java assertNotNull(alphabet); assertTrue(alphabet.toUpperCase().contains("A")); assertTrue(alphabet.startsWith("abc")); assertTrue(alphabet.endsWith("xyz")); Apart from needing four separate statements, we now discover that JUnit provides quite a limited API
Bluntly, JUnit can check that something is true/false or that something is null (or not)
Using only JUnit we can’t say: “check that this String contains the character A”
We have to use the contains method of Java’s String class, and then check that its result is true
Let’s zoom in on the example of contains()
The JUnit the test: java assertTrue("abc".contains("A")); will fail with the message: java java.lang.AssertionError at org.junit.Assert.fail(Assert.java:86) at org.junit.Assert.assertTrue(Assert.java:41) at org.junit.Assert.assertTrue(Assert.java:52) at assertj.Strings.contains_junit(StringsTest.java:34) ..
This does not give away any information about what is wrong
Something went wrong with contains, but what String was tested
And what did we expect it to contain
When this happens while running the test in your IDE you hopefully can click somewhere so that it jumps to the line where it failed (line 34 in StringsTest.java) so you can find the error by looking at the assertion statement
But when reading the test results report from a Continuous Integration server on the other hand you have no context..
With Fluent Assertions the same test would be written as: java assertThat("abc").contains("A"); Because we exactly tell what we want to test (that “abc” contains the character A), AssertJ has enough information to tell us what went wrong
So this test fails with the message: java java.lang.AssertionError: Expecting: <"abc"> to contain: <"A"> Both in your IDE as on the CI server this will save a lot of time and mental effort because you see what’s wrong in a glance
We’ve now seen how we can write better readable tests which give more information when a test fails
Until now I only gave examples with Strings, but AssertJ provides API’s for more data types
All examples can be found on AssertJ’s website, but let me highlight another commonly used data type
Collections Suppose we want to test this List of Strings: java List numberList = Arrays.asList("One", "Two"); In JUnit this will look like: java assertEquals(Arrays.asList("Two"), numberList); And this fails with the message: java Expected :[Two] Actual :[One, Two] Using AssertJ the same would look like: java assertThat(numberList).containsExactly("Two"); and this fails with the message: java Actual and expected should have same size but actual size was: <2> while expected size was: <1> Actual was: <["One", "Two"]> Expected was: <["Two"]> So AssertJ tells us that the size is incorrect
Nice, we do not have the scan all the elements to find out what the difference is ourselves
Another example where the size is equal, but the ordering is different
JUnit’s: java assertEquals(Arrays.asList("Two", "One"), numberList); will fail with: java Expected :[Two, One] Actual :[One, Two] While AssertJ’s: java assertThat(numberList).containsExactly("Two", "One"); will fail with: java Actual and expected have the same elements but not in the same order, at index 0 actual element was: <"One"> whereas expected element was: <"Two"> In these examples the lists only contained two elements, but when the list is larger, it will get hard to find out which element is missing, or to see the difference
A last example where the difference in Collections is a bit more obscure
Suppose we want to check if the following List of numbers correctly counts up: java List largeNumberList = Arrays.asList(1, 2, 2, 4, 5); JUnit's: java assertEquals(Arrays.asList(1, 2, 3, 4, 5), largeNumberList); will fail with: java Expected :[1, 2, 3, 4, 5] Actual :[1, 2, 2, 4, 5] Unless you become happy from playing a game of spot the difference this results in needless occupation of your mental capacity
And that while AssertJ's: java assertThat(largeNumberList).containsExactly(1, 2, 3, 4, 5); fails with: java Expecting: <[1, 2, 2, 4, 5]> to contain exactly (and in same order): <[1, 2, 3, 4, 5]> but could not find the following elements: <[3]> In a glance we see what is wrong
Again, when Collections tends to be larger in size, these kind of failure messages are only getting more helpful
Why not Hamcrest
Well fair point
Hamcrest core has been included in JUnit since version 4.4 and tests using the hamcrest API look a lot more like AssertJ than that they look like plain JUnit
Also the failure messages are better than in Plain JUnit
But in my opinion Hamcrest does both these jobs not as well as AssertJ
Let’s compare the two
Comparing Strings with Hamcrest: java assertThat("abc", containsString("A")); fails with: Expected: a string containing "A" but: was "abc" At least we see the expected (containing “A”) and actual ( “abc” ) here, so that’s better than JUnit
At this point Hamcrest still reads like natural language just like the Fluent Assertions
But let’s get back on the example with multiple assertions on the letters of the alphabet String
With Fluent Assertions we saw: java assertThat("abc") .isNotNull() .startsWith("abc") .endsWith("xyz"); which fails with: java Expecting: <"abc"> to end with: <"xyz"> The equivalent in Hamcrest will look like: java assertThat("abc", allOf( is(notNullValue()), startsWith("abc"), endsWith("xyz"))); and fails with: java Expected: (is not null and a string starting with "abc" and a string ending with "xyz") but: a string ending with "xyz" was "abc" Decide for yourself which failure message requires less effort to understand what is tested and what went wrong
As we can see in the test itself Hamcrest provides a prefix notation like API to perform multiple assertions
This requires the reader to create a mental model of a stack with the operators like allOf() and is() while understanding the different assertions
With the given example this may sound exaggerated, but in more complex situations this requires quite some mental effort
As I said in the beginning only hamcrest-core is part of JUnit, which is quite limited
When you want to test collections for example you need to add hamcrest-all to your project
And when already adding an extra dependency to your project anyway, why not choose assertj
Last release of Hamcrest dates back to 2012, while AssertJ is more actively developed (may 2017) and supports Java8 features
Last reason why I think AssertJ is the best, the only and nothing but the best is code completion
Additional advantage of its Fluent API is that we can simply use code completion to explore all the possibilities
Without the the need for memorizing the whole API or the need for cheat sheets
Getting Started The website of AssertJ is full of examples and instructions on how to include AssertJ in your project
For an extensive set of examples see the assertj-examples tests project on Github
When you’re using Eclipse, see this tip to get code completion
You could do the same for Mockito by the way 😉 While the examples in this post were in Java with the AssertJ library, the same ideas apply for other languages
See for example fluentassertions.com for .NET
After reading this, I hope you’re even more devoted to create code that is simple and direct
Or as Grady Booch, author of Object Oriented Analysis and Design with Applications, said:Gebruikers die jouw dienst even stressvol vinden als een horrorfilm is hoogstwaarschijnlijk het laatste wat je wil
Onderstaande illustratie uit een onderzoek beschrijft precies dat
Er is aangetoond dat de hoeveelheid stress die gebruikers ervaren bij het gebruik van te trage (mobiele) applicaties ongeveer gelijk staat aan het kijken van een horrorfilm
Dit voorbeeld laat zien dat juist techneuten even goed invloed uitoefenen op de gebruikerservaring
Door de performance te verbeteren kun je het gebruik dus een stuk aangenamer maken
Naast vlotte databases en snel geladen schermen, kun je ook op andere manieren de gebruikerservaring beïnvloeden
En je hoeft geen doorgewinterde UX-designer of onderzoeker te zijn om dat te doen
In april dit jaar was er in het CineMec Ede alweer het derde, en uitverkochte, Luminis DevCon event plaats
Tijdens deze conferentie heb ik verteld hoe je inzicht kan krijgen in het gebruik van je dienst door een gebruiker
In het verlengde van deze presentatie ga ik hier iets verder in op het online meten van gedrag
Enkele inzichten zijn snel en eenvoudig te verkrijgen en er zijn tools zijn die daarbij helpen
Zo kan jij ook beter begrijpen waaróm gebruikers iets (niet) doen
Ik ga vooral in op de praktische toepassing van deze tooling en biedt wat aanknopingspunten die helpen om zelf een basic analyse te kunnen doen en nuttige inzichten te vergaren
Mijn presentatie kun je trouwens hier terug kijken
Meten is weten Eerst nog kort iets over het meten
Dit is grofweg op te delen in kwantitatief en kwalitatief
Heel plat gezegd; kwantitatief gaat over de wat-vraag
Wat gebeurt er en hoe vaak
Bijvoorbeeld: een specifieke pagina heeft een hoge bounce rate
Oftewel , die pagina is vaak het eindstation van een bezoek
Dit kun je achterhalen door een web analytics tool als Google Analytics, de meest bekende
Kwalitatief daarentegen gaat over het waarom van dit gedrag
Wat maakt dat die pagina vaak het eindstation is van bezoekers
Om hier antwoorden op te kunnen geven heb je dus een kwalitatieve metrics tool nodig
Hieronder geef ik een voorbeeld van een dergelijke tool en enkele meet methodes die hier in naar voren komen
Het meetinstrument: Hotjar Om iets te meten heb je een meetinstrument nodig
Nu zijn er naast de web analytics tools in software land steeds meer diensten die kwalitatieve data vastleggen
Om er maar een aantal te noemen: Visual Website Optimizer, ABTasty, EyeQuant, Hotjar, Crazy Egg, Usabilla en Fibeo
De tool die ik hiervoor gebruik is Hotjar
Het fijne van deze toepassing is dat het niet alleen heatmaps van klik- en scroll gedrag maakt maar daarnaast ook inzicht biedt door formulier analyse en schermopnames
Daarnaast kun je ook eenvoudig polls en surveys inrichten en directe feedback vragen met Incoming, dat nog in beta is
Er zit veel functionaliteit in voor weinig geld
De betaalde versies zijn vergeleken met andere tools zeker niet duur
Tools zoals Crazy Egg richten zich vaak maar op 1 methode voor een hogere prijs
Er zijn wel kritische noten te vinden over de datakwalitateit vergeleken met web analytics tools
Daarnaast is het koppelen met tools als Google Analytics niet bepaald plug and play, terwijl daar juist de kracht zit
Hotjar is ook laagdrempelig omdat het een gratis Basic account biedt waarmee je zo goed als alle functionaliteit tot je beschikking hebt
De beperking zit hem in het verkeer: de ‘sample rate’ is gelimiteerd op 2000 pageviews per dag, dus het aantal pageviews waarop de data gebaseerd wordt
En voor recordings is 100 de bovengrens
Toch voldoende om al iets te gaan meten
Het opzetten van een account en project is snel gedaan
Een keer registreren, wat basisgegevens van je website invoeren en een stukje script plaatsen
Deels afhankelijk van op welk platform je website draait
Voorbeeld van het stukje script dat je in de code moet plaatsen om Hotjar werkend te krijgen
Je kunt overigens meerdere websites onderbrengen in 1 account, die dan elk een eigen tracking ID hebben
Vervolgens check je via de portal of je code werkt en kun je aan de slag
Verder spreekt deze tool voor een groot deel voor zich
Er is voor Hotjar uitgebreide documentatie aanwezig
Hieronder ga ik met name in op de methodes zelf en hoe je hier bevindingen uit kan halen voor het verbeteren van je service
Heatmaps Heatmaps zijn een visualisatie van interactie zoals klikken en muisbewegingen in een webpagina of een applicatie
Vaak aangeduid met een kleurverloop van blauw naar rood dat weinig en veel representeert
Dit kunnen muis click- of muis move-maps zijn, een registratie van respectievelijk kliks en muisbewegingen
Maar ook scroll maps
In Hotjar kun je heatmaps aanzetten op specifieke pagina’s en deze laten stoppen als een limiet is bereikt
Bijvoorbeeld na 1000 geregistreerde views
Als je de heatmaps eenmaal binnen hebt: hoe moet je deze nu interpreteren
Wat zeggen die vlekken nou
Doorgaans zijn er een aantal patronen die te herkennen zijn: Iets is klikbaar maar er wordt niet op geklikt
Als je knoppen of elementen hebt die niet aangeklikt worden kan aan heel veel zaken liggen en is afhankelijk van de context: worden juist andere zaken veel aangeklikt
Ziet het er niet klikbaar uit
Een trend die al een tijdje loopt is dat knoppen nogal plat worden vormgegeven met een outline border
Ziet er heel stijlvol uit maar grote kans dat men dit niet ziet als een knop
Is je huisstijl voornamelijk rood / wit
Vermijd die kleuren voor je belangrijke interacties, pak de meest contrasterende kleur want die valt meer op
Dit zijn in de praktijk bewezen conventies
Iets is niet klikbaar maar men klikt er wel degelijk op
Soms wordt iets vormgegeven alsof het een klikbaar element is omdat het onbedoeld op een button lijkt bijvoorbeeld
Je ziet in heatmaps terug dat deze elementen aangeklikt worden, echter zit er geen functionaliteit achter
Zorg ervoor dat deze vorm aangepast wordt om onnodig klikken en daarmee frustratie te voorkomen
Aan de andere kant: je kunt het juist wel functioneel maken door er interactie aan te koppelen
Zo zag ik op onze DevCon pagina een klein aantal clicks op een animerend muis icoontje dat aangeeft dat men kan scrollen voor meer inhoud
Klikken doet echter niks
Je zou hier dus een scroll anker van kunnen maken
Ander voorbeeld: bij iets als een artikel overzicht verwacht men vaak dat zowel titel, afbeelding en soms een paar regels tekst als geheel een klikbare link is naar het volledige item
Beperk daarom niet de interactie tot een titel en ‘lees meer’ link alleen
Dit is allemaal context afhankelijk natuurlijk en des te belangrijker om dit te meten en toetsen
Voorbeeld van een heatmap op mobile Scrollmaps Een scrollmap laat zien tot waar op een pagina er wordt gescrold
In Hotjar is dit weergegeven door een kleurverloop en percentages
Ook is te zien waar gemiddeld genomen de vouw van een pagina zit
Overigens, dat ‘above the fold’ de belangrijkste elementen horen te staan is inmiddels achterhaald
Voor een lange onepager is het soms lastig een scrollmap te genereren als de navigatie met ankers werkt
Immers werken de links in de navigatie als nieuwe paginas terwijl je in feite naar beneden scrollt
Als men niet ver genoeg de pagina door scrollt kan dat een paar dingen betekenen: Men haakt af op de inhoud: deze is niet relevant of er staat te veel tekst
Het helpt om dit te dubbelchecken met recordings die je kunt maken
Die licht ik verderop toe
Of richt hiervoor een gerichte poll in op die pagina die op een laagdrempelige manier feedback kan geven
Houd dit kort en simpel voor de gebruiker
Men kan ten onrechte denken dat het einde van de pagina is bereikt
Er is te veel witruime of men heeft geen visuele indicatie van de verdere aanwezige inhoud
Er zijn tegenwoordig veel pagina’s te zien die royaal van witruimte zijn voorzien maar dat kan dus nadelig werken
De opbouw van een item wordt onderbroken door iets wat er niet mee te maken heeft
Bijvoorbeeld een advertentieblok dat over de hele breedte van een blogitem doorloopt
Formulier analyse Je kunt ook formulieren analyseren door bij te houden hoe deze afgerond worden
Door een visualisatie kun je zien op welke plekken in een formulier gebruikers afhaken
Voorbeeld van een form analysis
Rode cijfers geven lange invultijden of veel correcties aan
Wat je hier onder andere uit kan halen: Het is te lang, als je veel velden hebt en je gestaag afhakers ziet, is het wellicht nodig groeperingen toe te passen
Knip het formulier op in een aantal (visueel) logische stappen
Dat maakt de drempel voor een gebruiker een stuk lager
Een veld is te complex om in te vullen
Het is niet helder wat er verwacht wordt in te vullen, of je bent juist te strikt over de juiste notatie
Denk aan datum notaties, postcodes en de ergste: verplichte informatie vragen van gebruikers die ze niet paraat hebben of doorgaans niet graag geven
Screen Recordings Om concrete voorbeelden van interactie te zien kun je in Hotjar schermopnames bekijken van bezoekers
Dit zijn geanonimiseerde screen recordings van interactie die je redelijk goed kunt filteren op pagina’s, devices, browsers, duur enzovoort
Je kunt deze ook annoteren
Het voelt wel een beetje voyeuristisch als je dit bekijkt
Wel goed om in de instellingen keystroke data uit te zetten, anders leg je vast wat gebruikers daadwerkelijk invoeren
Een valkuil van een screenrecording is dat het nogal aan interpretatie onderhevig is
Soms zijn er lange pauzes zonder interactie te zien
Bedenk maar eens wat het zou kunnen betekenen: is deze persoon op het scherm aan het lezen, iets in een ander scherm aan het doen of even naar de wc
Je ziet veel gebeuren in recordings maar oordeel niet te snel over de intenties van de gebruiker
Native toepassingen De tool en methodes zoals hierboven beschreven richten zich vooral op websites
Er zijn steeds meer services die ook in staat zijn soortgelijke metingen op native applicaties te doen
AB Tasty is al eens genoemd, verder zijn er diensten als App see en UXcam die dit kunnen
Zelf heb ik met de native metrics iets minder ervaring, maar in feite blijven de methodieken staan
Succes met inzichten vergaren! Headerfoto door: Caroline Methot on UnsplashAngularJS is a huge framework and already has many performance enhancements built in
Despite that, it’s very easy to write code in AngularJS that will slow down your application
To avoid this, it is important to understand what causes an AngularJS application to slow down and to be aware of trade-offs that are made in the development process
In this post I describe things that I have learned from developing AngularJS applications that will hopefully enable you to build faster applications
First of all, a quick win to boost your applications performance
By default AngularJS attaches information about binding and scopes to DOM nodes and adds CSS classes to data-bound elements
The application doesn’t need this information but it is added for debugging tools like Batarang to work properly
You can disable this by one simple rule in your application config
java app.config(['$compileProvider', function ($compileProvider) { $compileProvider.debugInfoEnabled(false); }]); If you want to temporarily enable the debug information just open the debug console and call this method directly in the console: java angular.reloadWithDebugInfo(); Watchers in AngularJS “With great power comes great responsibility” AngularJS provides the $watch API to observe changes in the application
To keep track of changes you need to set a watcher
Watchers are created on: $scope.$watch
{{ }} type bindings
Directives like ngShow, ngClass, ngBindHtml, etc
Isolated scope variables: scope: { foo: ’=’ }
filters
Basically: everything in AngularJS uses watchers
AngularJS uses dirty checking, this means it goes through every watcher to check if they need to be updated
This is called the digest loop
If a watcher relies on another watcher, the digest loop is called again, to make sure that all of the changes have propagated
It will continue to do so, until all of the watchers have been updated
The digest loop runs on: User actions (ngClick, ngModel, ngChange)
$http responses
promises are resolved
using $timeout/$interval
calling $scope.$apply() of $scope.$digest()
Basically: the digest loop is called a lot
The “Magic” of AngularJS works great but it is fairly easy to add so many watchers, that your app will slow down
Especially when watchers doing too much work
With this in mind we will go on and talk about some easy changes to make our applications faster! Avoid a deep watch By default, the $watch() function only checks object reference equality
This means that within each $digest, AngularJS will check to see if the new and old values pointing to the same object
A normal watch is really fast and preferred from a performance point of view
What if you want to perform some action when a modification happened to an object or array
You can switch to a deep watch
This means that within each $digest, AngularJS will check the whole tree to see if the structure or values are changed
This could be very expensive
A collection watch could be a better solution
It works like a deep watch, except it only checks the first level of object’s properties and checks for modifications to an array like adding, removing, replacing, and reordering items
Collection watches are used internally by Angular in the ngRepeat directive Another effective alternative that I recommend is using one-way dataflow
This can be accomplished by making a copy of the watched object, modify the copy and then change the variable to point to the copy
Than a normal (shallow) watch will do the job
Use track by in ng-repeat ngRepeat uses $watchCollection to detect changes in collections
When a change happens, ngRepeat makes the corresponding changes to the DOM
Let’s say you want to refresh a flight list with new flight data
The obvious implementation for this refresh would be something like this: apache $scope.flights = serverData.flights; This would cause ngRepeat to remove all li elements of existing tasks and create them again, which might be expensive if we have a lot of flights or a complex li template
This happens because AngularJS adds a $$hashkey property to every flight object
When you replace the flights with the exact same flights from the server the $$hashkey is missing and ngRepeat won’t know they represent the same elements
The solution Use ngRepeat’s track by clause
It allows you to specify your own key for ngRepeat to identify objects, instead of just generating a unique hashkey
The solution wil look like: apache <div ng-repeat=”flight in flights track by flight.id”> Now ngRepeat reuse DOM elements for objects with the same id
Bind once Use bind once where possible
If bind once is used, Angular will wait for a value to stabilize after it’s first series of digest cycles, and will use that value to render the DOM element
After that, Angular will remove the watcher and forget about that binding
This will minimize the watchers and thereby lightens the $digest loop
From AngularJS 1.3 there is the :: notation to allow one time binding
apache <p>{{::name}}</p> <ul> <li ng-repeat="user in ::users"></li> </ul> Use ng-if instead of ng-show/ng-hide Maybe this is obvious but i think it’s worth mentioning it
Use ng-if where possible
The ng-show/ng-hide directives just hide the element by setting it’s style property “display” to none
The element still exists in the DOM including every watcher inside it
Ng-if removes the element and watchers completely and generates them again when neededModern day software programming is an increasingly wide and complex field
An ‘ideal’ full-stack developer would probably be one that started developing applications at a young age, developing knowledge and understanding of a broad number of technologies
She or he would have received formal education in different programming paradigms in several different languages, as well as in (relational) databases, communication protocols, hardware, security strategies, front-end technologies, etcetera
Having first learned what a string and an integer are some 3 years ago, I am not such a developer
I know many other colleagues and friends for which software development was a taste that they acquired at a later age
There could be several reasons why software development is a popular choice of career switch
It is a relatively young profession, so perhaps some people are only now ‘discovering’ it
Perhaps people have a (probably unwarranted) fear that the profession is boring, or too difficult
Whatever the reason, people who want to learn building software applications professionally today will initially have to find a way through an overwhelming number of technologies and concepts
You have little choice in this case, but to simply start somewhere
For me that somewhere was (primarily) ASP.Net and C#
After following some (open) university courses about object-oriented programming (in Java) and subsequently landing a job as junior C# developer, I quickly felt somewhat competent in basic programming in C#
Classes, interfaces, inheritance, and those sorts of things
The C# and Java languages of course have their differences (that I won’t discuss here), but I would argue that they are conceptually similar
Some knowledge of object-oriented and class based programming was a narrow window of entrance into a wide field of technologies
To contribute to the development process of full web applications, I had to at least develop some basic ability to (amongst others) work with things like HTML and HTTP/Ajax
And: Javascript
Superficially, Javascript seems somewhat like C# (except for some obvious differences like strong typing and classes)
But Javascript has been a somewhat rough experience for me so far
After taking some time recently to get more familiar with the core concepts of plain Javascript as a language however, things start making more sense
Let’s now zoom in on those aspects of Javascript that may be most confusing to C# and or Java developers! Javascript is actually, in many ways, very much different from C# or Java and I believe that a basic, structured, overview of the most fundamental differences might help junior Javascript developers, like me, with the following things: Some understanding of the inner workings of popular libraries we tend to use like Typescript, various module loaders, Angular, etc
More awareness of good and bad coding practices when writing Javascript
Detecting and understanding ‘bugs’ in Javascript code that we could not understand if we treat it like it was, say, C#
Powerful features that are unique to Javascript and that we can use to our advantage
Every data container in Javascript is either an ‘object’ or a ‘primitive’ (and that’s all) Primitives in Javascript are not objects and they are always 1 of the following 5 things: number, string, bool, null and undefined
String, bool and null are somewhat similar to how they work in C#/Java
One notable thing though, is that Javascript has no stack and heap separation in terms of memory allocation
In other words, there is no such thing as a struct in Javascript
Thus, the implication of data being captured in either an object or a primitive is minimal in memory terms
Also, the ‘under-water’ way in which Javascript enables methods to be called on a string literal (e.g
string.length), is completely different from C# and Java
I’ll come back to that later when talking about object prototypes
Javascript numbers are always 64-bit floating point variables
There is only the one type of number (so no double, decimal, etc.)
All objects declared in the Javascript language always have this exact structure: { key1: value1, key2: value2, key3: value3, // ..etc, key100: value100, // …etc } The values in this case can be primitives, or other objects
Note that functions in Javascript are objects themselves, just like arrays
Compared to C#/Java objects, Javascript objects have no type (by default) and do not need to instantiate a class
The undefined primitive is the value that variables receive when they are declared but have not yet received a value
Somewhat confusingly, you can also assign the value undefined manually, like so: var x = undefined; var y = {key1: undefined}; Doing this is considered bad practice but it is good to understand the difference between an undefined variable, a variable with the value null and a ‘unknown’ variable
In Java and C#, declared but unassigned variables have a default value (null for reference types, 0 for Integer, false for bool etc.)
In Javascript this default value is undefined for everything
Null is typically used in Javascript when you want to explicitly not assign any value to something (a method could typically return null for example, as opposed to undefined)
Finally, when a variable that has not been declared at all is used during execution of the code, the compiler will throw an unknown variable exception
Program flow in Javascript is single threaded, starts with a root ‘this’ object and flows from top to bottom in the second phase of its execution
Ok, that header is certainly a mouthful
Please bear with me and I’ll explain
Program flow in Javascript is fundamentally different from C# and Java and some insight into how it works is very helpful in understanding when things in your code happen and why
Code execution (compilation) of a Javascript app proceeds in two phases
In the first phase, the Javascript compiler prepares the ‘execution context(s)’
In the second phase those execution contexts are, well, executed
I.e
it is translated by the compiler into code that the computer can understand
The first of the execution contexts that is prepared by the compiler is called the global execution context
This is the outer scope of your Javascript app
It always comes with a global object that is called ‘this’
In the case of Javascript executed in a web-page, ‘this’ is (in the global scope) normally the window object
Furthermore, each ‘function’ in your app gets its own execution context
These execution contexts are prepared for execution in the order in which they are nested under the outer scope, and from top to bottom
Because this may be easier to follow with some visual example, I refer to the following post that I found on the web and that explains this topic very well: https://cedric-dumont.com/2015/10/31/compilation-and-execution-phases-of-simple-javascript-code The ‘this’ variable for each execution context, by default, points to whatever object is currently in scope
That essentially means that ‘this’ initially points to the object on which the function which encapsulates the current execution context is called
The nature of ‘this’ in Javascript is extremely confusing, especially for developers used to C# or Java
However, the following document does a fair job of explaining it: https://developer.mozilla.org/nl/docs/Web/JavaScript/Reference/Operators/this In the second phase, once all execution contexts are prepared, the code on them is then executed for each context in sequential order and on a single thread
Javascript code, unlike C# and Java applications, has no classes or methods that can start new threads
Variables that have been declared but that have not received a value at the time of code execution will have the default value (undefined), as explained earlier
Note that this can be for any reason: You may have forgotten to assign the variable, or the variable may get assigned only after a promise has resolved and that promise has not returned yet
The same holds true for variables that are assigned to a function
The following code for example will throw an exception because x is undefined, just as if x were any other object (Remember, there is only one type of object in Javascript)
… x(); var x = function(){ console.log(‘Called x!’);}; … By contrast, the following code will work, and show ‘Called x!’ on the console: … x(); function x() { console.log(‘Called x!’);}; … This last type of function is evaluated as soon as the execution context in which it resides is generated and will be placed on that scope as an object-function called x
The new keyword in Javascript is used when creating new instances of objects using constructor functions
Not when simply creating a new object by defining it and assigning it to a variable
The ‘new’ keyword, much like the ‘this’ keyword is a strange beast in Javascript
Perhaps this is even more true because it was introduced to Javascript at a later point in time, with the intention to make the instantiation of new objects look and feel like regular Java
What makes it confusing is that, in practice, it does something completely different and is not comparable with the new keyword in C# or Java
Let’s start by explaining what ‘new’ does in Javascript
Consider the following ‘constructor’ function and assume that you placed this function on the global execution context: function Person(first, last, age, eye) { this.firstName = first; this.lastName = last; this.age = age; this.eyeColor = eye; } The reason this type of function is called a constructor function is that it can be used to create new instances of objects that look like this: { firstname: ‘firstname’ lastname: ‘lastname’, age: number, eyecolor: ‘value’, __Proto__: { contructor: function Person(first, last, age, eye) } To accomplish this, we need to simply write: var x = new Person(‘first’, ‘last’, number, ‘eye’); What we certainly do not want to do is write the following: var x = Person(‘first’, ‘last’, number, ‘eye’)
If you fail to see why this is, you should probably read the former paragraph about program flow and the ‘this’ keyword once more
Because it is an easy mistake for a programmer to make, declaring the Person function with a capital P is an unusual but important convention here
Normally functions and variables in Javascript always start with a lowercase letter
(Incidentally, another difference with Java and C#)
I will talk about the _Proto_ (‘prototype’) object a bit more in the last paragraph
For now, it is important to see that objects you create with a ‘new’ keyword retain a reference to (a copy of) the function that created them via this prototype object
All ‘children’ that you create off the constructor function have a reference to the same prototype object
This is the basis for prototypical inheritance in Javascript
A completely different mechanism to inheritance as we know it in C# and Java
Using object prototypes in Javascript to your advantage Each object in Javascript has a hidden reference to a prototype object, with Object.prototype always being at the end of that reference chain
All properties and methods of an object’s prototype, are accessible by its ‘children’ as well
Let’s say we create a new object in the normal way, like so: var normalObject = { quality: ‘normal’, type: ’object’ }; This normalObject has, by default, a reference to Object.prototype and that is why you can call a method like ‘toString()’ on it
Primitives do not have a prototype object, but strangely you can call methods on them
This is made possible due to Javascript very briefly creating the necessary object representation of that primitive, use that objects’ String.prototype methods, then garbage-collect the object immediately after
For example: ‘test’.charAt(0) will be briefly converted to new String(‘test’).charAt(0) upon executing the code
Because all child objects made by a constructor function share the same prototype object, you can do funny things, that are somewhat unexpected when you are used to Java/C#
You can, for example, extend commonly used object prototypes like Array.protoype, Window.prototype ans String.prototype (each of which has its own reference to Object.prototype), with you own methods*
This feels almost like extension methods in C#
You can also create an object with the new operator, then change its prototype to an entirely different object, to create various interesting effects
It is fun to play around with and I would encourage the reader to experiment, to get a good feeling of object behavior in Javascript
*If you are using typescript, remember to also extend the type-definition interface, to get this to work
Closing words Javascript is an interesting language that has its weaknesses but certainly has a lot of strengths as well
Because Javascript is the backbone of nearly all client-side code in web-applications, often with complex frameworks like typescript and angular built on top of it, I believe there is a lot of value in understanding how it works on a fundamental level for nearly every developer
While it looks somewhat like Java and C# on a superficial level, I hope I have given a good impression of the points at which it is very different
I also hope that this write-up was of some use to other people learning Javascript as a secondary languageYou don’t want your passwords and other secrets stored in your source code
Why
A password should not be coupled to a specific version of your application because when a password or other secret needs to be changed, the application must be redeployed
And if your version control system gets hacked, your secrets will leak
ASP.NET Core has a solution to store secrets outside the repository during development
It’s called user secrets’ and in this post, I’m going to show what they are and how to use them
Setting up Create a new ASP.NET Core Web Application for Windows, Linux and macOS
Install the following NuGet package: Microsoft.Extensions.Configuration.UserSecrets Add following code to the constructor in the startup class
user secrets .net core builder.AddUserSecrets C# if (env.IsDevelopment()) { builder.AddUserSecrets(); } 1234 if (env.IsDevelopment()){ builder.AddUserSecrets();} Create a new class called AppSecrets
You can choose a different name if you like but for this example I’ll use this name
This class contains all the properties, you want to put in your user secrets
You can also use objects as properties
user secrets asp.net core appsecrets C# public class AppSecrets { public string MySecret { get; set; } } 1234 public class AppSecrets{ public string MySecret { get; set; }} Add the following line of code to the ConfigureServices method in the startup class
user secrets asp.net core configure services C# services.Configure(Configuration); 1 services.Configure(Configuration); Adding user secrets Now we’re done setting things up so it’s time to add a user secret
User secrets are defined in a file called secrets.json which is stored in: Windows: %APPDATA%\microsoft\UserSecrets\\secrets.json Linux: ~/.microsoft/usersecrets//secrets.json Mac: ~/.microsoft/usersecrets//secrets.json As you can see, the secrets.json file is not stored in your repository
The file is NOT encrypted so user secrets should only be used for development purposes! The easiest way to open and edit the user secrets in Visual Studio is by right clicking your project and clicking ‘manage user secrets’
The UserSecretId, you see in the path is defined in the csproj file or in the project.json if you are using an older version of .NET Core
This id is unique to your app
Changing this id will generate a new, empty secrets.json file
To add a user secret open the secrets.json file and paste the following code into it
user secrets asp.net core secrets.json { "MySecret": "Password123!" } 123 { "MySecret": "Password123!"} Retrieving user secrets Now that we have added a user secret, it’s time to retrieve it in our MVC Controller
The user secrets are retrieved the same way as the Configuration
For this example, I’m using the default HomeController
Replace the default Index method in the HomeController by the following code: user secrets asp.net core controller C# public string MySecret { get; set; } public HomeController(IOptions optionsAccessor) { MySecret = optionsAccessor.Value.MySecret; } public IActionResult Index() { ViewBag.MySecret = MySecret; return View(); } 1234567891011 public string MySecret { get; set; }public HomeController(IOptions optionsAccessor){ MySecret = optionsAccessor.Value.MySecret;} public IActionResult Index(){ ViewBag.MySecret = MySecret; return View();} Paste the following code somewhere in the Views/Home/Index.cshtml
user secrets asp.net core view C# @ViewBag.MySecret 1 @ViewBag.MySecret Now you can see your user secret on the homepage of your app
Off course, normally we wouldn’t show secrets on our webpages, but this is just for demo purposes so you can see it’s working
Command prompt The last part is about managing your user secrets from the command prompt
To do this, add the following line of XML to your csproj file between an Item Group element
user secrets asp.net core tools csproj XHTML <DotNetCliToolReference Include="Microsoft.Extensions.SecretManager.Tools" Version="1.0.0-msbuild3-final" /> 1 <DotNetCliToolReference Include="Microsoft.Extensions.SecretManager.Tools" Version="1.0.0-msbuild3-final" /> Now you can open a command prompt window and navigate to your project folder
There are a couple of commands and I’m going to show some of them
dotnet user-secrets –help Executing this command will show you information about the user secrets command line tool
dotnet user-secrets set SecondSecret Password Adds a new user secret with the key SecondSecret and value Password to your project
dotnet user-secrets list lists all user secrets in your project
dotnet user-secrets remove SecondSecret Removes SecondSecret from your project
dot net user-secret clear Clears all secrets from your projectPrologue At Luminis Arnhem we are currently working together with Nedap on the MACE project, a prototype implementation of a site access control system
In the MACE scenario, the users can access a physical location/open a door using a digital identity card on their mobile device
These digital identities replace those cumbersome physical cards that either take up space in your wallet or always seem to disappear when you need them
The MACE mobile application receives these identities from a server application, which are then sent to a MACE reader device via Bluetooth Low Energy (BLE), Near Field Communication (NFC) or can be read by the reader via a Quick Response (QR) code
The idea is that if the correct identity is read by the reader, i.e
an identity that has the authority to gain access to the location guarded by the reader, then the user gains access to a physical location
In order to create an initial prototype we needed to delve into Android BLE
In this blog I will describe how BLE works in the MACE scenario as well as how the implementation looks like in Android
For more information on the MACE project itself, visit the MACE homepage
On BLE There are two roles in BLE communication
These are the central and peripheral roles
The central role is responsible for scanning for advertisements which are made by peripherals
Once the central and peripheral are in range of each other, the central will start to receive these advertisements and can choose to connect to a peripheral
Think of it as the central being a job seeker and a peripheral being one of those recruiters on LinkedIn
What I’m trying to say about peripherals is that they continuously (in most cases) advertise without even knowing if the central has any interest or use for its services
Luckily, the central can use a filter to only see advertisements that it finds interesting
This is done based on a UUID filter, which on the application level only shows advertisements that contain a service with that UUID
Each advertisement contains (at least) the following information: –Bluetooth Device Address: Similar to a MAC address used to identify a BLE device
–Device name: A custom name for the peripheral device which can be configured
–RSSI value: The Received Signal Strength Indicator in decibel per meter
This is a value that indicates the strength of the signal, ranging from -100 to 0
The closer the RSSI is to 0, the better the signal is
–List of Services: A list of services that are provided by the peripheral
In the BLE scenario, a service is a collection of characteristics
A characteristic contains a single value and a number of descriptors
Descriptors describe the value contained in the characteristic, such as a max length or type
The image above illustrates the hierarchy of these concepts
The line between Peripheral and Service depicts the “Peripheral has one or many Services” relationship
The rest of this blog will describe the building blocks for creating an Android BLE central implementation, such as scanning for advertisements, processing an advertisement, connecting to a device (peripheral) and reading and writing, from and to a BLE peripheral
We will not discuss BLE in more depth, as that is not the goal of this blog
Main act: Android implementation Ask nicely The first thing we need to do is to ask for permission to turn on Bluetooth on the mobile device and use it
For Android versions lower than 6.0, we only need to add the permissions to the Android Manifest: <uses-permission android:name=“android.permission.BLUETOOTH"/> (ble_permission.png) For BLE on Android 6.0 or higher, we noticed that our app was crashing when trying to use the Bluetooth functionality
It turns out that you need to add one of the following permissions to the Android Manifest: <uses-permission android:name="android.permission.ACCESS_COARSE_LOCATION"/> <uses-permission android:name="android.permission.ACCESS_FINE_LOCATION"/> These permissions are required for determining the rough or precise location of the mobile device respectively
You might be wondering: Why do I need these permissions that are related to location to turn on BLE
Good question
I believe it has something to do with the fact that the signal strength of your mobile device to a BLE peripheral can be used in combination with other sources to determine your location
I have not yet looked further into this
However in the MACE application we do use the RSSI value to estimate how far you are from the MACE reader, so it is not implausible
As of Android 6.0, users now get prompted on whether to give an app certain permissions while the app is running, instead of simply accepting all permissions when installing the app
We have to change our code to ask the user for this permission the first time they try to use the BLE functionality in the app: if (activity.checkSelfPermission(Manifest.permission.ACCESS_COARSE_LOCATION) != PackageManager.PERMISSION_GRANTED) { activity.requestPermissions(new String[]{Manifest.permission.ACCESS_COARSE_LOCATION}, 1); } This code checks if the permission is already granted
If that is not the case, the user will be prompted with the request to give the ACCESS_COARSE_LOCATION permission
To handle the result of this action, you need to override the onRequestPermissionResult method
See this page for more info
In case Bluetooth is turned off on the mobile device, we also need to ask the user if we can turn it on
To do this, we need an instance of the Bluetooth adapter
We will discuss this is the next section
Bluetooth turn-ons Now that we have politely asked for permission from the user to use Bluetooth, it is time to get to work
The first thing we should do is get the Bluetooth Adapter instance
The Bluetooth Adapter instance can be retrieved using the BluetoothManager
If there is no BluetoothManager present, this means that the device does not support Bluetooth
An example: BluetoothManager bluetoothManager = (BluetoothManager) context.getSystemService(Context.BLUETOOTH_SERVICE); if (bluetoothManager == null){ //Handle this issue
Report to the user that the device does not support BLE } else { BluetoothAdapter adapter = bluetoothManager.getAdapter(); } Once we have the adapter, we can get the party started
As mentioned before, we first need to check if Bluetooth is turned on for the mobile device
If this is not the case, we will ask the user to turn this on
This can be done in the following way: if(adapter != null && !adapter.isEnabled()){ Intent intent = new Intent(BluetoothAdapter.ACTION_REQUEST_ENABLE); activity.startActivityForResult(intent, 1); }else{ System.out.println("BLE on!"); //do BLE stuff } The handling of this request is done in the same way as when we asked for the permissions in the previous section
From this point on, we know that BLE is up and running on the mobile device
Hello…is it me you’re looking for
It’s time to start scanning! When we start the scanner, the mobile device will start receiving advertisements of BLE peripherals around it (range varies per device)
With the BluetoothAdapter we retrieved in the previous section, you can get an instance of a BLE scanner
We need to supply this scanner with at least an implementation of a ScanCallback, which describes what we should do with the results of the scan
Optionally, we can also supply the scanner with filters and settings
The filters help us specify our search in order to find the devices we are looking for
The settings are used to determine how the scanning should be performed
This is a simple example of how to do all of this and get the scanner running: public void startScanning(){ BluetoothLeScanner scanner = adapter.getBluetoothLeScanner(); ScanSettings scanSettings = new ScanSettings.Builder().setScanMode(ScanSettings.SCAN_MODE_LOW_LATENCY).build(); List<ScanFilter> scanFilters = Arrays.asList( new ScanFilter.Builder() .setServiceUuid(ParcelUuid.fromString("some uuid")) .build()); scanner.startScan(scanFilters, scanSettings, new MyScanCallback()); } public class MyScanCallback extends ScanCallback { @Override public void onScanResult(int callbackType, final ScanResult result) { //Do something with results } @Override public void onBatchScanResults(List<ScanResult> results) { //Do something with batch of results } @Override public void onScanFailed(int errorCode) { //Handle error } } A few things to note: -There are a few scan modes available
SCAN_MODE_LOW_LATENCY has the highest frequency of scanning and will thus cause more battery drain
SCAN_MODE_LOW_POWER has the lowest frequency and is best for battery usage
This mode would mostly be used for background scanning or if a low frequency of results is acceptable for your application
SCAN_MODE_BALANCED is somewhere in between the previously mentioned modes
SCAN_MODE_OPPORTUNISTIC is a special scan mode, where the application itself does not scan but instead listens in on other applications scan results
-The scan filter in this case is used for only detecting advertisements that contain a certain uuid as a service
All other advertisements will be ignored
-The ScanCallback has a onBatchScanResults function, which is only called when a flush method is called on the scanner
The scanner has the ability to queue scan results before calling the callback method, however I have not looked further into this
-In a real application, you might want to keep a reference to the scanner so that you can stop it from scanning when this is necessary
So many advertisements… A ScanResult object is returned for each advertisement scanned
The frequency of advertisements received is dependent on the scan mode
The scan callback will get called for each advertisement
This can become overwhelming
In the MACE project, we stop scanning as soon as we find a device we want to connect with
The ScanResult object contains some information on the Device such as the BLE address, the service uuids and the name given to the device
The ScanResult object also contains the RSSI, which tells you roughly how close the mobile device is to the peripheral device the advertisement belongs to
With the information in the ScanResult it is possible to do a second-level filtering, such as checking if the peripheral is close enough using RSSI or checking if the device name is what you expect it to be
If everything checks out, we can proceed to connecting to the peripheral
Give it everything you gatt! The concept of two BLE devices communicating with each other via services and characteristics is called Generic Attribute Profile, GATT for short
In order to use this in the mobile application, we need a BluetoothGatt instance
This can be obtained in the following way: @Override public void onScanResult(int callbackType, final ScanResult result) { BluetoothDevice device = adapter.getRemoteDevice(result.getDevice().getAddress()); BluetoothGatt gatt = device.connectGatt(mContext, false, new myGattCallBack()); } We first get an instance of the device using the BLE address obtained in the scan result
Using this device object we then call the connectGatt function, which gives us a Bluetooth Gatt instance
The first parameter in the connectGatt method is the Android App context
The second parameter is indicates whether or not we should automatically connect to the device once it appears
This concept is also known as having devices paired with each other
Since we have commitment issues, we set this to false
The last parameter is the callback which is called by the BluetoothGatt instance when there is a response from the peripheral
The callback should extend the BluetoothGattCallback class
You should create your own implementation and override (at least) the following methods: -onConnectionStateChange(BluetoothGatt gatt, int status, int newState) -onServiceDiscovered(BluetoothGatt gatt, int status) -onCharacteristicRead(BluetoothGatt gatt, BluetoothGattCharacteristic characteristic, int status) Once you call the connectGATT method, the onConnectionStateChange method will be called
If the newState value is 2 (Connected), we can carry on
If this is not the case, then we cannot communicate with the peripheral and all other operations on the gatt instance will fail
Assuming we have connected successfully with the peripheral device, we can now trigger the service discovery by calling: gatt.discoverServices(); which asks the peripheral device for a list of all services
These services are then loaded into the gatt instance
The onServiceDiscovered method will be called with a status of 0 if it was successful
If this is the case, we can call: List<BluetoothGattService> services = gatt.getServices(); since the gatt instance now has these services after the discovery
Note that these services also contain a list of characteristics
From this point on it is a matter of finding the right characteristic you are looking for within the right service to read from or write to
These are not the characteristics you are looking for… It is important to make an agreement as to which UUIDS will be used for which services and characteristics on the peripheral device
These UUIDS need to be known on the mobile application
Using these UUIDS, we can loop through the services in the following way: private String serviceUUID = "xyz"; private String characteristicUUID = "xyz"; private BluetoothGattCharacteristic characteristic = null; private void findCharacteristic(List<BluetoothGattService> services){ for(BluetoothGattService service: services){ if(service.getUuid().toString().equalsIgnoreCase(serviceUUID){ for(BluetoothGattCharacteristic serviceCharacteristic : service.getCharacteristics()){ if(serviceCharacteristic.getUuid().toString().equalsIgnoreCase(characteristicUUID)) { characteristic = serviceCharacteristic; } } } } } Now we actually have an object that represents the characteristic that we want to read from or write to
Change the world around you We have reached the final step in this blog
What we want to do is finally read from and write to a peripheral device
We do this by using the characteristic object obtained in the previous section in combination with our GATT instance
To read a data in the characteristic, we simply call: boolean successfullyRead = gatt.readCharacteristic(characteristic); @Override public void onCharacteristicRead(BluetoothGatt gatt, BluetoothGattCharacteristic characteristic, int status){ byte[] characteristicValue = characteristic.getValue(); //Do something with the value } The onCharacteristicRead method is called with the characteristic
You can get the value in the characteristic by calling characteristic.getValue(), which returns a byte array with the value that the characteristic contains
To write to the characteristic, we first have to set the value we want to write into the characteristic object we obtained
Once this is set, we can call write characteristic using the gatt instance: byte[] valueToWrite = new byte[8]; Arrays.fill(valueToWrite, (byte) 0x00); characteristic.setValue(valueToWrite); boolean successfullyWritten = gatt.writeCharacteristic(characteristic); The boolean successfullyWritten is true if the write action was successful, otherwise it returns false
Epilogue You now have the basic building blocks to build an Android BLE central implementation
What you do from here on out is up to your imagination
Of course we couldn’t cover everything in this blog
Certain things such as threading, error handling and battery optimisation were omitted in the hope to keep this entry concise
These topics will need to be covered in another blog
In any case, you now know the essentials
Have fun and remember, scan responsiblyTwo blogposts in one week
Yes! I was so excited about this subject I could not stop myself from sharing this with the rest of the world
As a C# developer, I actually love the async and await feature that that language offers
I think it makes asynchronous code a lot easier to write and to read (although you do need know what is happening under the hood)
Asynchronous code is not unique to C#
It also happens a lot when you are writing JavaScript /TypeScript
When you are using Angular 1, you will probably know that Angular uses promises to represent a piece of work that finishes in the future
Angular promises and the Angular $q service that you can use to work with promises are based on the CommonJS specification for promises
These promises are working fine, but they can cause your code to become less readable because of all the callbacks that are happening when doing promise programming, especially when you have nested asynchronous calls together with failure handlers for the promises
See the screen shot below of some asynchronous code from an application I am currently working on
On itself this is not one of the worst parts, but you can see some nesting going on and some then-calls with anonymous functions
I would love to rewrite this to the new async/await features of Typescript 2.1
Also take note of the way I create a promise that resolves immediately, using $q.when()
Async/await was available before Typescript 2.1 but it was only available when targeting ECMA 6 or ECMA 2015
Typescript 2.1 adds support for down level async functions, meaning that we can also target ES 5 or 3
When using async/await on a ES 5 or 3 environment we have to make sure we have an ES 6 compatible promise implementation
The generated JavaScript, when using async/wait is based on the ES 6 Promise specification
So we need to polyfill the Window.Promise constructor function
But we can’t just use any polyfill because in an Angular application, the default $q promises also call $apply when a promise resolves to refresh the UI
If we would grab just a polyfill from the internet we have to add the $apply calls ourselves
That would be a lot of work
The trick to solve this is really easy! Luckily angular comes with an ES 6 polyfill out of the box! If you look at the documentation for $q, you will see that it also has an ES 6 compatible interface
Instead of using the defer api when creating promises, you can also use the ES 6 way of creating promises
You can actually use the new keyword on $q and pass it a function to create a new promise
Or instead of using $q.when to create a promise that resolves immediately you can also use $q.resolve, just like in the ES 6 specification
So now that we have established that we can use $q to polyfill the window.Promise, here is the way to do it
In the run method of my main module I injected the following piece of code: The entire run method looks like this (just to give some context) The promise polyfilling happens on the last line of the run method
So that takes care of the runtime stuff
Runtime, the generated async code has everything it needs to run without errors
But compile time we are still not there
Typescript does not know that we have a valid Promise implementation when targeting an ECMA version lower than 6
The Typescript compiler has a new option for this
The –lib option
You can pass this option an array of strings, to tell the compiler which standard declaration files the compiler should be using when compiling
I am compiling my application by using gulp and gulp-typescript, so my gulp file has been modified to look like this: You can see the list of strings I pass to the lib option
The ES2015.Promise string does the trick for the promises
Normally this list is populated for you when you specify a target
But when you pass the lib option yourself, this does not happen so I also have to include the other declaration files that I want to be there (for accessing the DOM for example)
Now we have everything in place to rewrite the code that we saw earlier is to async/await
Here it is! This looks a bit more readable! There are no more anonymous functions for hooking then handlers to promises
I can also use the normal try catch constructs if I want instead of the failure handlers
Also notice The Promise.resolve() call I now use to create a promise that completes
This more ES 6 style
I also modified the services to return Promise instead of ng.IPromise to make sure the compiler does not complain, runtime the types are interchangeable
One last tip
If you are using gulp-typescript to compile, make sure you use version 3! As version 2.x grabs it’s own version of typescript (you can override this), but version 3 will grab the typescript version you have in your package.json
Happy asynchronous coding! ChrisWelcome! This time I wanted to share some of my experiences with the performance tooling of Visual Studio 2015 with you
This is going to be a short post, just to let some of you know what is there in Visual Studio, a lot of you will probably already know about this
A long running project of mine is a multiplayer stakeholder game that visualizes heat networks
The graphical interface is being developed by a third party but I and the company I work for are developing the central calculation engine that performs the complex algorithms and integrates some other components that are needed for getting all the data ready for the front-end
This is all working fine, but during the start of the new year we also needed to integrate the gas networks
Now both network types are very similar, on a more abstract level they are both networks that contain nodes (like hubs for pipes), pipes and connections (special nodes, that give entry into the network, usually houses are built near these)
So we thought, with some modification to the gas network data, we could just use all our existing algorithms and calculations (to calculate the needed pipe diameters for example) with the gas networks
There is one problem, the gas network are like 100 to 1000 times bigger than the heat networks
While everything we did was working fine it literally took ages
Responses were really getting slow, up to the point of one minute when starting a new gaming session
We really needed to something about this
The first tool that really helped me, was just the visual studio debugger
I took my time, placed breakpoints on the entry points on everything that was slow, and the debugger really did the rest
In the image above you can see the response time of this method
If you are interested in drilling down, just step in the method with the debugger, and on every step you get to see the response times
It’s great to really fast diagnose the slow parts of your application and do something about it
The values include debug overhead, but are nevertheless useful to get a sense of how different actions are related to each other in sense of time
When you are running the debugger, you also get another really cool window in visual studio
This window also has some really useful information
In the top most line, with pause sign in front of it, you see different break events recorded
Like when hitting a breakpoint
The response times that you see when stepping through a method, like the first image, are also recorded there
This part actually collects intellitrace events
If an exception happens for example, you can double click here and start historical debugging
I found this window the most useful for seeing the memory pressure and how many garbage collects were happing
You can also use this tool to diagnose memory leaks
You create a snapshot, then you do some stuff, and then you create another snapshot
In the figure below, I took two snapshots
One before starting a session, and one after
And then I compared them
The snapshots actually perform a garbage collect also, so this is the ideal way to see if objects are lingering
You can do all kinds of things with these snapshots
Compare them with more detail, drill into their heaps to see what’s there etc
But right from this picture you can draw a simple conclusion: There is a memory leak
If there wasn’t the two snapshots should contain roughly the same amount
So after my last actions, a lot of objects seem to be lingering, they are not garbage collected because they are rooted, they are reachable
For my application this was expected, each started sessions stays in memory until it’s done, all database IO is being done asynchronously by a separate process, to keep the performance of a running game high
When running the diff I get the following picture: I sorted this on which object types have been increased
Here you can see that a lot of session values are still there
This makes sense, since creating a new session will give us a lot new calculation values
But if I wanted to diagnose a leak, this would help me find it really , really fast
You can also view the paths to the root here (which explains how an object is reachable), view all it’s references, and even go to the code definition! Using the debugger only I managed to speed up the application at least 4 times, by finding slow parts and executing them a lot fewer times or speeding them up by implementing them a bit smarter
I also found out that a lot of slow parts came from searching lists of objects based on an unique value
Often it’s much faster to convert these to dictionaries and then use these to find the object or check if it exists
Never forget about the O(n) and O(1) notations you learned about in your data structures class! Happy coding! ChrisIn this post, I want to discuss some aspects of data retrieval with Linq-To-Entities and Entity Framework from a SQL database
Entity Framework is the most commonly used object relational mapper for .Net applications and it is great because in enables you to create complex data queries without having to write the SQL statements by hand
However, in this helpful automation also lies a danger of Linq generating very inefficient SQL that can potentially cripple the performance of your application
In my own experience, poorly structured queries are often a bottleneck for the performance of your web-application
Therefore, they are an especially relevant thing for developers to take good care of
Let’s try to better our ways and correctly use LINQ-to-Entities with the Entity Framework! Consider, for example, the following (somewhat contrived) piece of code
What a mess! The idea here is to retrieve some information concerning a company from the database, including some information of the users and laboratory locations attached to that company
The information retrieved here is arbitrary, it just servers as an example
The AllUsers(), AllLocations() and AllCompanies() methods just retrieve a set of those entities from the database using Entity Framework
I.e
I use this as a starting point for this article, because it demonstrates practically everything that a programmer could do wrong when using Linq-To-Entities
I will be spending the rest of this write-up to point out and subsequently fix the problems with it
Point 1: Only put data in memory that you actually need
Focus on this section: The call to ToArray() here queries the database for all known companies, selects all the data in all the columns of the Company table and then puts all that data into an array object
Wow
Not only does this not make use of the delayed loading capabilities of entity framework, it also retrieves much more information than we actually need
We are only interested in the company name, it’s laboratories and its users and not in all the other information that the company table might hold
It is sometimes tempting to use a method like ToArray() prematurely when you want to use methods on your object-to-be that Linq to Entities does not support but your actual object does
Try to prevent this because it is almost never necessary
Note that calling is fine, because of how Entity Framework works: The underlying query will not execute until you actually do something with the users, like putting them in an array
Point 2: Avoid querying the database inside loop structures
Focus on this section: In the way the code is set up currently, both of these lines will result in database queries and these queries are individually executed for each company in the database, because they are inside the companies.Select(c =>.
) statement! You definitively never want that
This kind of structure to retrieve data gives horrible performance even on relatively small data-sets
It is a surprisingly easy mistake to make and definitely worth looking out for
Point 3: Avoid querying multiple times when one query would suffice
This one is sort of related to the second point
Because this is not well represented by my initial example, I’ll refer to a stack overflow post by someone nicked Moby Disk instead: http://stackoverflow.com/questions/21408725/does-adding-to array-or-tolist-always-make-database-queries-faster
Consider the following example: Note that the query will be repeated here for both foreach statements
A better approach would be to use a single select to retrieve both the Name and OwnerName
(Much) worse on the other hand, would be to do a ToList() beforehand because of reasons explained above
Let’s refactor our own code to something a little more sensible now: This is a piece of code I might realistically write if the company data in the example would be what I need to retrieve for, let’s say, a controller endpoint
A couple of things go right this time: We retrieve only data that is really needed into memory (when calling SingleAsync())
There are no new queries inside loops
The code is fairly easy to read and understand because of its tree-like structure with the company table as ‘entry point’
Point 4: Don’t query too deep and don’t forget to use joins when looking for optimal performance
The second version of the code will work ‘o.k.’ for most situations but is still not optimal in terms of performance
It is easy to read and work with but if working with large data-sets you probably want to be using something like this instead: This is a little more verbose and returns a list of locations (laboratories in this case), including their company and user information
By (a) explicitly telling where to join the tables via the Join() methods, by (b) not querying more than one layer deep (user.Roles and user.Companies is as ‘deep’ as we go) and by (c) selecting no more data than we actually need, Linq-To-Entities will usually be able to generate the most accurate SQL and thus work as fast as possible! Hopefully this post will help you next time you find the performance of your Entity Framework powered data retrieval lacking, or help you spot common pitfalls when working with Linq-To-Entities!Sometimes I need a small tool to perform a simple task for me
One recent example is a simple test client that sends a SOAP request that in an integrated environment would be sent by an external party
The client needs to send an ID and the outcome (accepted/rejected) of a request that was sent in a previous stage
Now, there are several ways to achieve this
You could write a full web-based GUI, or, on the other end of the spectrum, use a tool like Boomerang
But if you’re already using Maven, you should also consider using the Exec Maven plugin
The Exec plugin is an easy way to execute some code; it is lot faster to set up than a standalone application, but you are still able to access normal Maven dependencies
Setting it up You can use this in an existing project or create a new project
Since my client is basically test code, I created a new project that depends on the project that contains the application code
java <project> <modelVersion>4.0.0</modelVersion> <parent> <groupId>eu.luminis.mvnexec</groupId> <artifactId>parent</artifactId> <version>1.0.0</version> </parent> <artifactId>mvn-exec-example</artifactId> <name>Exec Maven plugin example</name> <dependencies> <dependency> <groupId>eu.luminis.mvnexec</groupId> <artifactId>services</artifactId> <version>1.0.0</version> </dependency> </dependencies> </project> Add an executable class to the project that performs the action you need
You can use the standard arguments array if you need paramaters
My class takes three parameters: a customer ID, the outcome, and the endpoint URL
It then performs a SOAP call and prints the results to the console
java package eu.luminis.mvnexec.client; import eu.luminis.mvnexec.services.*; import javax.xml.ws.BindingProvider; import java.math.BigInteger; public class SoapClient { private final LuminisService service; public static void main(final String[] args) { final String url = args[2]; final SoapClient client = new SoapClient(url); final ServiceRequest request = new ServiceRequest(); request.setCustomerId(BigInteger.valueOf(Long.parseLong(args[0]))); request.setOutcome(Outcome.valueOf(args[1])); final ServiceResponse response = client.updateCustomer(request); System.out.println("Status: " + response.getStatus()); System.out.println("Message: " + response.getMessage()); } public SoapClient(final String endpointUrl) { service = new LumninisService(); this.endpointUrl = endpointUrl; } public ServiceResponse updateCustomer(final ServiceRequest request) { return getPort().updateCustomer(request); } private LuminisPort getPort() { final LuminisPort port = service.getLuminisPort(); final BindingProvider bp = (BindingProvider) port; bp.getRequestContext().put(BindingProvider.ENDPOINT_ADDRESS_PROPERTY, endpointUrl); return port; } } Add the Exec plugin to your pom; if you want to use arguments, you must specify them in the configuration of the plugin
With the classpath tag you automatically add all project dependencies to the classpath
java <project> ..
<build> <plugins> <plugin> <groupId>org.codehaus.mojo</groupId> <artifactId>exec-maven-plugin</artifactId> <version>1.3.2</version> <executions> <execution> <goals> <goal>exec</goal> </goals> </execution> </executions> <configuration> <executable>java</executable> <arguments> <argument>-classpath</argument> <classpath /> <argument>eu.luminis.mvnexec.client.SoapClient</argument> <argument>${customerId}</argument> <argument>${outcome}</argument> <argument>${serviceUrl}</argument> </arguments> </configuration> </plugin> </plugins> </build> ..
</project> Running your code Run the code by calling the exec:exec goal and providing the parameters you defined in the pom
You must provide all parameters that you defined! java mvn exec:exec -DcustomerId=1234 -Doutcome=REJECTED -DserviceUrl=http://localhost:8080/services/soap If one of the parameters is (almost) always the same for the user, you could define the value as a property in the user's Maven settings file (user_home/.m2/settings.xml) so you don't have to provide it every time you execute the code
java <settings> <profiles> <profile> <id>inject-services-url</id> <properties> <serviceUrl>http://localhost:8080/services/soap</serviceUrl> </properties> </profile> </profiles> <activeProfiles> <activeProfile>inject-services-url</activeProfile> </activeProfiles> </settings> You can still override the property by providing it to the Exec goal as beforeWhat is Xamarin
Xamarin is a platform for developing fully native applications for iOS, Android and Windows, with a shared code base
Xamarin apps are written in C# and can make use of most .NET libraries
Why Xamarin
Xamarin apps are native so the performance is great
Everything that can be done with native app development can be done with Xamarin
All native Android, iOS and Windows API’s are available
The UI is native
Xamarin Android apps use the same design guide lines as other Android apps
This is important because the user will have a similar user experience with your app as with other apps
Same goes for iOS and Windows apps
You only need to know or learn one language to develop for all three platforms
Some background information Some of you might be confused now because .NET only runs on Windows right
No that’s not completely true so I want to start with some background information about CLI and Mono
In 2000 Microsoft released the Common Language Infrastructure (CLI)
This is an open specification and is standardized by ISO and ECMA
The CLI describes how high-level language applications can run on different platforms without changing the code
There are a few implementations of the CLI like .NET Framework, .NET Core, Mono and more
The .NET Framework is the most used and most well-known implementation which only runs on Windows
Mono is a CLI implementation that runs on Windows, Linux, Mac and even Embedded systems
Mono makes it possible to write applications with WinForms, WCF, ADO.NET, Entity Framework and more in languages like C#, F#, VB.NET, Python and more
Mono is also available for iOS as Xamarin.iOS, previously named MonoTouch and Xamarin Android, previously named Mono for Android and MonoDroid
How does Xamarin work
When you scaffold a Xamarin app you get multiple projects
The number of projects you get depends on which platforms you want to support
The first project is a class library which contains all the code shared between all the supported platforms
You also have one project for each platform you want to support
Those projects contain all of the platform specific code
Xamarin contains binding for the iOS and Android SDKs
So Xamarin code looks pretty much like the code you would write when developing a native app
I’ve written a very simple Android example to show you what it looks like
It’s an app which fetches some information about countries from a webservice and show them in a list
It also has a switch
If the switch is turned on it shows the country codes and when turned off the app won’t show the country codes
The code should look very familiar to Android developers
code namespace XamarinCountries { [Activity(Label = "XamarinCountries", MainLauncher = true, Icon = "@drawable/icon")] public class MainActivity : Activity { private IEnumerable<Country> _countries; private Switch _countrySwitch; private ListView _countryList; protected override void OnCreate(Bundle bundle) { base.OnCreate(bundle); SetContentView(Resource.Layout.Main); _countryList = FindViewById<ListView>(Resource.Id.listCountries); _countrySwitch = FindViewById<Switch>(Resource.Id.swCountryCode); FetchCountries(); RedrawList(); _countrySwitch.Click += (sender, args) => { RedrawList(); }; } protected void RedrawList() { string[] items = new string[] { "No countries available" }; if (_countries != null && _countries.Count() > 0) { items = _countries.Select(c => c.Name + (_countrySwitch.Checked 
" - " + c.CountryCode : "")).ToArray(); } _countryList.Adapter = new ArrayAdapter<string>(this, Android.Resource.Layout.SimpleListItem1, items); } protected void FetchCountries() { CountryRepository repo = new CountryRepository(); this._countries = repo.GetCountries(); } } } code <?xml version="1.0" encoding="utf-8"?> <LinearLayout xmlns:android="http://schemas.android.com/apk/res/android" android:orientation="vertical" android:layout_width="match_parent" android:layout_height="match_parent" android:minWidth="25px" android:minHeight="25px"> <Switch android:layout_width="match_parent" android:layout_height="wrap_content" android:id="@+id/swCountryCode" android:text="Show country code" /> <ListView android:minWidth="25px" android:minHeight="25px" android:layout_width="match_parent" android:layout_height="match_parent" android:id="@+id/listCountries" /> </LinearLayout> But how about Windows
There is actually no such thing as Xamarin.Windows
Windows apps are developed the same way as you would normally develop native Windows apps
The only difference is that the app will use the shared code library which is also used by the Android and iOS apps
What do you need to get started with Xamarin
To develop with Xamarin you can use Windows or OS X
There are two IDEs available for Xamarin
The first one is Visual Studio which is only available for Windows and the second one is Xamarin Studio which is available for Mac and Windows
For developing Android and iOS apps you can use both Mac and Windows
But if you want to use Windows you still need a Mac for the iOS app because Xamarin relies on the iOS build tools which are only available for Mac
You’ll have to use the Mac as a build agent
You can connect the app by using a wizard in Visual Studio (see picture)
The Windows pc will send the code to the Mac and the Mac will create the iOS packages and will run the app on a simulator on the Mac, or an iOS device connected to the Mac
Developing Windows apps is only possible on Windows
If you still want to work on a Mac you can develop Android, iOS and shared code on the Mac and use source control to continue working on your Windows app on Windows
Click here for more info on this
Later, I will write a blogpost about Xamarin forms, a toolkit to create a shared UI for all three platforms and more information about the Xamarin platform
Sources: https://www.xamarin.com/ https://www.pluralsight.com/blog/tutorials/xamarin-webcast-qaSigning XML in .Net has been supported since I started working with .Net
Signing XML with SHA-256 signatures is a different story, this wasn’t supported out of the box and you had to write some obscure code
Luckily this will change with the release of .Net 4.6.2
In this post I will show how to sign XML with SHA-256 signatures before the release of .Net 4.6.2 and after
The first step in both scenario’s is to obtain a digital certificate which can be used for SHA-256 signing on Windows
A good to follow tutorial can be found here
After obtaining this certificate you can write code which uses the certificate to digitally sign XML
Before .Net 4.6.2 The first step is to register a SignatureDescription class that defines SHA-256 as the digest algorithm
.NET already contains a class called “RSAPKCS1SHA1SignatureDescription” that supports SHA1
We have to create a similar class for SHA-256: We need to register this class with the framework before we can use it: You can see it being registered on line 12, in the static initializer of our extension class
This class also contains a method which can be used to digitally sign an XML document with SHA-256: You can see the reference being made to SHA-256 on line 32 and on line 24
With this method you can sign XML documents using SHA-256, pre .Net 4.6.2
After .Net 4.6.2 After .Net 4.6.2 is released, we don’t need any of our custom classes, as both the SHA-256 description class and the namespace constants (they are present on the SignedXml class) are provided by the framework
This means that the modified method for .Net 4.6.2 will look like this: Conclusion With the release of .Net 4.2.6, the platform get’s a long awaited update to the Crypto api’s
You can change the references in the above method to references to SHA-384 or SHA-512 and those will also work
For a complete overview of what else is new in the Crypto api’s and other stuff, you can take a look hereHi everyone welcome to another blog post about something I recently discovered
As I was working on an application, and we were nearing our release deadline, we discovered we still had to choose a password policy
Until now, for development and testing purposes such a policy was not yet introduced in the application
The back-end part was easy, we use a cloud provider for our authorization, so we could just set it up there
But then came the front-end part
I immediately started thinking about regular expressions, as probably any developer would
We use regular expressions for pattern matching right
But very soon in my thinking process I also though it could not be done, because there is an order in a regular expression
That means that the regular expression engine moves through your string as it is evaluating your regex
I will give you a short example
Let’s say we want to match a password policy that says it should contain a small letter, capital case letter and a number
Something like Test22
You would probably start with a regex that says something like: JavaScript “[a-zA-Z0-9]” 1 “[a-zA-Z0-9]” But this regex actually also matches just test
Because this regex says that every character it encounters should be part of this list
So this probably won’t do
Let’s try something else: “[a-z][A-Z][0-9].+” 1 “[a-z][A-Z][0-9].+” This also won’t do, as this means the first characters of the password should be lower case, capital case, number and then everything is possible
This is the order I was talking about earlier
As the regex engine encounters the first character of the string, it matches it against the first construct in your regex, the second with the second, and so on
So what we actually need is a regex construct that does not move our Regex engine
A construct that matches different parts, but the order of these parts is not important
It turns out these constructs are actually there and supported by almost all of the engines that are out there
They are called Lookahead and Lookbehind constructs
Let’s have a look! “(?=bar)” 1 “(?=bar)” This is a very simple lookahead
This validates that everything after the current position of our regex engine should be the string “bar”
The syntax for a lookahead is (?=<regex>)
The part between the angle brackets can contain a regex
You can use a look ahead on every position in your regex
The regex above says :The start of the string should be followed by bar
But I can also do something like this: “test(?=bar)” 1 “test(?=bar)” This regex says the word test should be followed by bar
Now why would you use these constructs
Instead of when you write the above with a normal regular expression, after the regex engine hits the last “t” of the word test, it does not move
The engine takes note of the lookahead, it evaluates the part after “test” but does not move
What does this mean
Let’s make it more complex: “^test(?=.*bar)(?=.*foo)” 1 “^test(?=.*bar)(?=.*foo)” This does a couple of nice things
First, this has multiple lookaheads
You can actually combine them to do multiple evaluations, without moving the regex engine
In the above example both lookaheads will be evaluated from the last “t” of the word test
Both of the lookaheads must be true to make your entire regular expression match
Looking at the above string this regex says
Your string should start with “test” followed by “foo” and “bar” but the order does not matter! And this is something that is very hard to achieve without this construct
So the following string would match: testfoobar testbarfoo But not: test So this gives us the possibility to specify conditions, in which the order is not important, but they do all need to be satisfied
Exactly what we would need to enforce passwords
So let’s have a look at a simple password policy that says that a password should contain a number, a lowercase letter and a capital case letter
And it should be at least six characters long
Every condition will become a lookahead except for the length
^(?=.*[A-Z])(?=.*[0-9])(?=.*[a-z]).{8,}$ 1 ^(?=.*[A-Z])(?=.*[0-9])(?=.*[a-z]).{8,}$ This is it! Let’s explain this a little
The “^” says the start of the string
From the start of the string we will perform three lookaheads
.*[A-Z] Means from the start of the string there can be any number of characters but they should be followed by a capital letter
.*[0-9] Means from the start of the string there can be any number of characters but they should be followed by a number
.*[a-z] Means from the start of the string there can be any number of characters but they should be followed by a lower letter
Keep in mind, the order of these lookaheads do not matter! They all get evaluated from the start of our string, without changing the position of our engine
If they are all true, the lookahead part of our regex is satisfied
The lookahead part is followed by .{8,}$ As this is not part of a lookahead, this will change the position of our regex engine
So the engine will move up at least 8 positions of any character in our string and then the password should end
The $ sign, means the end of the string
And that’s it! An easy way to enforce password policies, this is very useful in an Angular front-end in combination with ng-pattern and ng-message to give a friendly message on your create a new user form
If we want to add extra conditions, for example at least one special character, we just add an extra lookahead
The entire regex becomes: “^(?=.*[A-Z])(?=.*[!@#$&*\^%\*\.])(?=.*[0-9])(?=.*[a-z]).{8,}$” 1 “^(?=.*[A-Z])(?=.*[!@#$&*\^%\*\.])(?=.*[0-9])(?=.*[a-z]).{8,}$” Thanks for reading! ChrisUsing the null-conditional operator can break existing code calling extension methods
In this post I’ll explain how this can happen
Since C# 6 we can use the null-conditional operator in our code to prevent NullReferenceExceptions like this: So no NullReferenceException is thrown on line 4 and the result of the whole expression becomes NULL
Therefore, the output of this program will be “NULL”
Now let’s try something else: The main difference is that the methodcall on line 4 is an own custom extension method and being an extension method, it can handle the NULL-case itself, which an instance method can never do: Well, this also produces “NULL” as output
This is exactly what we would expect, or, isn’t it
The “test” variable contains a NULL value so the extension method is never called, but as you can see in the implementation of this extension method, it explicitly handles the case where the instance is null (whether this is good or bad design is irrelevant at this point )
Of course, this will never happen as long as we use the null-conditional operator
Is this strange
I don’t think so, I think it’s the right decision to make the null-conditional operator work consistently across normal instance methods and extension methods
If you want to let the extension method handle the NULL-case itself, just don’t use the null-conditional operator
Moral of this story: Don’t go around and sprinkle the null-conditional operator around in your existing code without thinking about whether the method you are calling is an extension method (and thus in fact a static method) which already handles the NULL-case itself, because this might break something unexpected in your programIn most latest browsers it is now possible to deliver different images for different devices by only one image element, using the srcset attribute
This is important for preventing small devices loading unnecessarily large images, but still having sharp big images on large (retina) devices
Using srcset Multiple image sources can be suggested to a browser by size or by the device pixel ratio of the browser
Using the x unit (device pixel ratio of the browser) html <img srcset="http://placehold.it/500x150 1x, http://placehold.it/750x150 1.5x, http://placehold.it/1000x150 2x" src="http://placehold.it/500x150" alt="" width="500"> On retina screens with a device pixel ratio of 2, the image will show the source with width of 1000px
Using the w unit (size of the image source) html <img srcset="http://placehold.it/500x150 500w, http://placehold.it/1000x150 1000w" src="http://placehold.it/500x150" alt="" width="500"> Using w tells the browser the width of the image source
The browser can decide which width matches best for the device
The choice depends not only on the browsers viewport, but can depend on bandwidth and device hardware too
How to tell the browser what source it should choose, depending on size AND device ratio
Telling the browser how large an image source is by defining the size in w, is not enough for a browser to choose the best source, when the image is styled with CSS to, for example, 200px
In this case we use the size attribute, together with the srcset
html <img sizes="200px" srcset="http://placehold.it/200x150 200w, http://placehold.it/400x300 400w" src="http://placehold.it/200x150" alt=""> Because now we tell the browser what the size will be, after applying CSS styling, and we specify the width of each source, the browser can choose the best image
The image will always be displayed with a width of 200px, but a retina device will choose the source with 400w and a non-retina device will choose the 200w source
What is exactly the problem
The problem is that images are preloaded by the browser
This is at the moment the DOM is not even build yet
So the browser has no knowledge about the CSS styling which is calculated after the DOM has been build
So we have to give the browser some clue what the size of the image will be, after fully rendered and styled with CSS, for the browser to be able to pick the most suitable source from a list of sources with different widths
The sizes attribute is not the same as the width attribute
The difference lies in the responsive possibilities of the sizes attribute
Responsive fluid design For a responsive and/or fluid design a fixed size of, for example, 200px, does not suffice
A fluid design has sizes relative to the parent element
Which is not known when images are being preloaded, by the browser, before the DOM is ready and the view is rendered with CSS styling
However there is a unit that is known, before CSS is calculated
It is the view width
With the view width we can define a fluid (relative) size of the image
html <img sizes="100vw" srcset="http://placehold.it/200x150 200w, http://placehold.it/400x300 400w, http://placehold.it/600x450 600w, http://placehold.it/800x600 800w, http://placehold.it/1000x750 1000w, http://placehold.it/1200x900 1200w" src="http://placehold.it/300x150" alt=""> Media queries It is even possible to use media queries in the sizes attribute
This is needed when, for example, we have an image that has a width of 33% of the viewport, but on small devices a full width of 100%
html <img sizes="(min-width: 768px) 33vw, 100vw" srcset="http://placehold.it/200x150 200w, http://placehold.it/400x300 400w" src="http://placehold.it/300x150" alt=""> Here the sizes attribute is interpreted as the image being 33% of the viewport when the viewport is larger than 768px, otherwise it is 100%
What is going on
When we resize the browser window when viewing this page in Chrome we see in the next example that the breakpoint is a little surprising
html <img sizes="100vw" srcset="http://placehold.it/200x150 200w, http://placehold.it/400x300 400w, http://placehold.it/600x450 600w, http://placehold.it/800x600 800w, http://placehold.it/1000x750 1000w, http://placehold.it/1200x900 1200w" src="http://placehold.it/300x150" alt=""> When starting with a browser width of 150px on a retina device, Chrome chooses the 400w source as expected
But it switches to the 800w source at a browser width of 283px, which I find rather unexpected! I would have expected the breakpoint earlier
Another inexplicable thing happens at 388px viewport width, switching to the 1200w source
I would have expected it to happen later
I would expect the browser to switch source at viewport widths 200 and 400px
Because, to be able to render a sharp retina image at 201px viewport width, the browser needs an image larger than 400px
The same applies to the 1200w image, which is only needed for a viewport with a width from 401px
So what is going on hereHello again! It is time for my second blog post
Very soon after the first one, but that is because I am learning a lot these days and I am also using my blog as documentation
Today I want to have a look at unit testing a Windows Store App written in JavaScript and/or in typescript
Normally unit testing JavaScript applications is not that hard
You will need to have a couple of components ready to make it work but there are a lot of standard frameworks and tools at your disposal
JavaScript unit tests get handled by different kind of components
A unit test framework
You use this to describe and organize your tests
A test runner
This does discovers your unit tests and runs them
Browser
this can be a real browser with an UI or a headless browser like PhantomJS for example that will run your tests
A browser can be used for End to End testing, but it can also be needed to run all the necessary JavaScript
When you are not using NodeJS for example, you need something else to run the JavaScript
When you are using angular, you also need to run your tests in a browser
When you are creating Windows Store Apps
NodeJS or a browser won’t help
This because Windows runs your apps in a special sandbox that is not easily simulated by a Node, a normal browser or a headless browser
This makes unit testing Windows Store Apps created with typescript or JavaScript from your automated build almost impossible
I did some research, but I believe that the only way to run unit tests for your Windows Store App is to create a separate project that functions as a test app to run your tests
The Hilo sample Windows Store App from Microsoft also uses this approach to unit testing
So we are going to create a separate project in our Visual Studio Solution
This project contains all the files needed to run or JavaScript code
This means that you need to add the existing JavaScript files as links to your test project
I recommend creating just a blank Windows Store project
Below is screenshot of where you can find this well hidden option in Visual Studio
Once you have done this
All the files will appear as if they belong to your project but actually they don’t get copied
So any changes you make in a file will get shared between projects
This is what you want, all your changes will get tested
Next up is the choice of your JavaScript unit testing framework
I did quite a bit of research on this and I found mocha.js the best suitable for Windows Store apps
Mocha has great asynchronous method support
So it can handle promises who finish later in a unit test without thinking the unit test is already done
Secondly, it can easily be run inside a browser, and not only in NodeJs for example
If it can be run inside a browser, it can probably be run in an app
Mocha let’s you choose an assertion framework, I chose should.js because it has a nice fluid syntax as you will see in a couple of minutes
Because I am also working with RequireJS and Typescript it took some tweaking to get all these thing working together
Let’s have a look
First off is the HTML file
This is just a view to see our test results
It’s really simple
Here is the code
XHTML <html> <head> <meta charset="utf-8" /> <title>Test results</title> <!-- WinJS references --> <script src="//Microsoft.WinJS.2.0/js/base.js"></script> <script src="//Microsoft.WinJS.2.0/js/ui.js"></script> <link rel="stylesheet" type="text/css" href="/lib/mocha.css"> <link rel="stylesheet" type="text/css" href="/default.css"> <script src="/js/default.js"> </script> <script src="/lib/mocha.js"> </script> <script src="/lib/should.js"> </script> <script>mocha.setup('bdd')</script> <script type="text/JavaScript" src="bower_components/requirejs/require.js" data-main="jstest/test-main"></script> <link rel="stylesheet" href="css/extra/jquery.Jcrop.min.css"> <link rel="stylesheet" href="css/windows8.css" /> </head> <body> <div id="mocha"><p><a href=".">Index</a></p></div> <div id="messages"></div> <div id="fixtures"></div> </body> </html> 123456789101112131415161718192021222324 <html> <head> <meta charset="utf-8" /> <title>Test results</title> <!-- WinJS references --> <script src="//Microsoft.WinJS.2.0/js/base.js"></script> <script src="//Microsoft.WinJS.2.0/js/ui.js"></script> <link rel="stylesheet" type="text/css" href="/lib/mocha.css"> <link rel="stylesheet" type="text/css" href="/default.css"> <script src="/js/default.js"> </script> <script src="/lib/mocha.js"> </script> <script src="/lib/should.js"> </script> <script>mocha.setup('bdd')</script> <script type="text/JavaScript" src="bower_components/requirejs/require.js" data-main="jstest/test-main"></script> <link rel="stylesheet" href="css/extra/jquery.Jcrop.min.css"> <link rel="stylesheet" href="css/windows8.css" /> </head> <body> <div id="mocha"><p><a href=".">Index</a></p></div> <div id="messages"></div> <div id="fixtures"></div> </body></html> From line 6 till the style sheets you can see my included of mocha.js and should.js and also a mocha.setup() call
Mocha supports different kind unit testing styles
I like the behavior driven development style
You will see this reflected in the way I write the unit tests
I could have also loaded these libs via require, but I did not see the point
The rest will get loaded via requirejs, because the app I am creating tests for also uses requirejs
To see howto handle this with angular and typescript, see my spanvious blog post here
On line 12 you can see a couple of div’s
Mocha will use these to report the results
Let’s have a look at our test-main JavaScript file
JavaScript 'use strict'; require.config({ paths: { 'networkService':'/Win8Services/NetWorkService', 'firstTest': '/jstest/FirstTest', 'secondScreenControllertest': '/jstest/Services/secondScreenControllerTest', 'angular': '/bower_components/angular/angular', 'domready': '/bower_components/requirejs-domready/domReady', 'jquery': '/bower_components/jquery/jquery' }, shim: { 'angular': { exports: 'angular', deps: ['jquery'] } } }); require(['domready!', 'angular'], function (document, angular) { angular.module('win8Services',[]); require(['firstTest', 'secondScreenControllertest'], function () { //angular.bootstrap(document); mocha.run(); }); }); 12345678910111213141516171819202122232425262728 'use strict'; require.config({ paths: { 'networkService':'/Win8Services/NetWorkService', 'firstTest': '/jstest/FirstTest', 'secondScreenControllertest': '/jstest/Services/secondScreenControllerTest', 'angular': '/bower_components/angular/angular', 'domready': '/bower_components/requirejs-domready/domReady', 'jquery': '/bower_components/jquery/jquery' }, shim: { 'angular': { exports: 'angular', deps: ['jquery'] } }}); require(['domready!', 'angular'], function (document, angular) { angular.module('win8Services',[]); require(['firstTest', 'secondScreenControllertest'], function () { //angular.bootstrap(document); mocha.run(); });}); You can see the paths configuration for require starting on line three
What’s important here is that you config all libs you need, so you will find angular, domReady and your test files
You can see a configuration for my first test and a test for something that is called secondScreenController
On line 20 you can see the Require call, the callback will get executed after the dom has loaded and angular
In it on line 22 I create an angular module
I do this, because the services I am going to test also register them selves in that angular module as part of their code
Without this module being there, I will get an error during testing
After this module has been created, you can see on line 24 another require call
this call loads my unit test files
After they have all been in loaded in the dom I call mocha.run() on line 26
This will instruct mocha to go and discover my tests, run them and report the results in the html file
Let’s have a look at a very, very simple class
Have look below
export class HelloSayer { constructor(public name: string) { this.name = name; } SayHello(): string { return "Hello " + this.name; } } 12345678 export class HelloSayer { constructor(public name: string) { this.name = name; } SayHello(): string { return "Hello " + this.name; }} This is a really simple typescript class
The only thing worth noting is that on line 1 I use the export keyword to define this class in RequireJS module
I do they so I can load this class in our unit test via require
Now the interesting part
The unit test
/// <reference path="../typescriptdefenitions/mocha.d.ts" /> /// <reference path="../typescriptdefenitions/should.d.ts" /> import sayerModule = require('./HelloSayer'); describe("HelloSayer", function () { describe("constructor", function () { it("should have a default name", function () { var sayer: sayerModule.HelloSayer = new sayerModule.HelloSayer('chris'); sayer.name.should.equal("chris"); }); }); describe("SayHello", function () { it("should greet passed target", function () { var greetings = (new sayerModule.HelloSayer("Kate")).SayHello(); greetings.should.equal("Hello Kate"); }); }); }); 1234567891011121314151617181920212223 /// <reference path="../typescriptdefenitions/mocha.d.ts" />/// <reference path="../typescriptdefenitions/should.d.ts" /> import sayerModule = require('./HelloSayer'); describe("HelloSayer", function () { describe("constructor", function () { it("should have a default name", function () { var sayer: sayerModule.HelloSayer = new sayerModule.HelloSayer('chris'); sayer.name.should.equal("chris"); }); }); describe("SayHello", function () { it("should greet passed target", function () { var greetings = (new sayerModule.HelloSayer("Kate")).SayHello(); greetings.should.equal("Hello Kate"); }); });}); This is where typescript shines
On line 4 you can see the import statement for my class under test
This is why in the test-main.js I only have load the unit tests
The unit tests themselves load the class they are testing
Easy with typescript 
On line 1 and 2 you can see the references to .d files for mocha and should
Otherwise the typescript compiler will give you a lot of errors about the describe and should methods
These describe functions come from the fact that I set mocha up to use BDD earlier, you can also choose another style of unit testing
First I describe the class, than I describe it’s different methods
You can find the actual test code in the body of the it functions
on line 11 you can see the fluent syntax of should.js, it’s really nice
Should.js attaches a method to the prototype of Object, so every object will get a should method you can use
You will need to do some extra stuff for null references
You will see that in another test
That test is shown below
/// <reference path="../../typescriptdefenitions/mocha.d.ts" /> /// <reference path="../../typescriptdefenitions/should.d.ts" /> import secondScreenModule = require('../../Win8Services/ScreenService'); describe("ScreenService", function () { describe("constructor", function () { it("should return default instance (futile test test)", function () { var service: secondScreenModule.ScreenService = new secondScreenModule.DefaultSecondScreenService(); (service === null).should.equal(false); (service.SecondScreenShowing == true).should.equal(false); }); }); describe("openSecondScreen", function () { it("should have opend a second screen", function (done) { var service: secondScreenModule.ScreenService = new secondScreenModule.DefaultSecondScreenService(); service.openSecondScreen("ms-appx:///Win8Services/testSecond.html").then(() => { service.SecondScreenShowing.should.equal(true); done(); }); }); }); describe("postMessage", function () { it("should have closed the second screen", function (done) { var service: secondScreenModule.ScreenService = new secondScreenModule.DefaultSecondScreenService(); service.openSecondScreen("ms-appx:///Win8Services/testSecond.html").then(() => { service.SendMessageToSecondScreen('closed'); done(); }); }); }); }); 1234567891011121314151617181920212223242526272829303132333435363738394041 /// <reference path="../../typescriptdefenitions/mocha.d.ts" />/// <reference path="../../typescriptdefenitions/should.d.ts" /> import secondScreenModule = require('../../Win8Services/ScreenService'); describe("ScreenService", function () { describe("constructor", function () { it("should return default instance (futile test test)", function () { var service: secondScreenModule.ScreenService = new secondScreenModule.DefaultSecondScreenService(); (service === null).should.equal(false); (service.SecondScreenShowing == true).should.equal(false); }); }); describe("openSecondScreen", function () { it("should have opend a second screen", function (done) { var service: secondScreenModule.ScreenService = new secondScreenModule.DefaultSecondScreenService(); service.openSecondScreen("ms-appx:///Win8Services/testSecond.html").then(() => { service.SecondScreenShowing.should.equal(true); done(); }); }); }); describe("postMessage", function () { it("should have closed the second screen", function (done) { var service: secondScreenModule.ScreenService = new secondScreenModule.DefaultSecondScreenService(); service.openSecondScreen("ms-appx:///Win8Services/testSecond.html").then(() => { service.SendMessageToSecondScreen('closed'); done(); }); }); }); }); On line 11 you can see some null handling
This seems a bit weird but it is actually the recommended way when using should.js
On line 23 you see another cool feature of mocha
A call to the done() function that get’s passed in as a parameter for my test
This is used in unit tests that have async calls
Without the done() call mocha will treat the test as synchronous
If you do that call mocha will treat it as asynchronous and thus will wait with reporting until you actually flag the test done
To finish this post
This is what I get when I run my unit test Windows Store app! it is not ideal, but still better than nothing
I hope someone who wants unit testing in their Windows Store html JavaScript app finds this post useful
Until next time!Welcome to my first blog post for Luminis! This time I am not going to blog about .Net related stuff, as I started working on a project which uses all the above technologies together
It took a little time to get my head around it all and I bumped into quite a few things when combining these cool frameworks, so I wanted to share my findings with you all
First off: Typescript and Angular together Typescript is a cool new language that makes it a lot easier to write structured and object oriented JavaScript
If you come from a language like C# or Java you will feel right at home
It has classes, interfaces, strongly typedness and a lot more cool stuff to write your applications
Another cool feature is that it compiles to just JavaScript! So if you have been doing a lot of object oriented JavaScript, using the revealing module pattern or class patterns to write classes and modules you will be pleased to hear that typescript takes that pain away
I am not going to give a complete typescript tutorial or description, you can find those here http://www.typescriptlang.org/
Now take Angular, the cool MVC
MVVM JavaScript framework
Angular has modules, databinding, controllers, services, all the cool stuff we use in object oriented languages to separate our concerns
If you come from C# and XAML, Angular can definitely be compared to PRISM, only for JavaScript
And that is exactly my problem, you have a cool structured framework like Angular, with an unstructured, non-oo language like JavaScript
Let’s combine the two! Let’s create a very simple Angular web application
Almost no UI, just to understand what is going on
The tooling I am going to use is Visual Studio 2013, it get’s Angular and Typescript, but you could also use Web storm for example
Typescript is completely open source and already has a lot of support
In the figure below you see my attempt to set up a simple hello world controller, you can see Visual Studio, which is great tooling, does not has a lot of intellisense to offer me, even though I reference angular with an intellisense comment
This is due to the dynamic nature of JavaScript
In a minute I will show you the complete app, but this tooling trouble, is one of the problems Typescript aims to solve so stay with me 
Now on to the complete example
html <html ng-app="appModule" lang="en"> <head> <meta charset="utf-8" /> <title>Typescript HTML App</title> <link rel="stylesheet" href="app.css" type="text/css" /> <script src="Scripts/angular.js"></script> <script src="Home/js/appController.js"></script> </head> <body ng-controller="appController as ourController"> <h1>Typescript Angular App</h1> <div id="content">{{ourController.sayHello()}}</div> </body> </html> Notice a couple things about this html
First of the multiple script references
This can become a hassle
The controller has to come after Angular
The order of the scripts matter, when I create a lot of files and dependencies this becomes hard to manage
Will solve this later using RequireJs
Further more on line 8 I am using the as syntax to define a controller
This allows us to bind to properties defined on our class instead of properties defined on our $scope
Let’s have a look at the JavaScript: javascript /// <reference path="....Scriptsangular.js" /> var HelloWorldController = (function () { function HelloWorldController() { } HelloWorldController.prototype.sayHello = function () { return "Hello world!"; }; return HelloWorldController; })(); var module = angular.module('appModule', []); module.controller('appController',HelloWorldController); Notice the whole anonymous function part
This is the JavaScript pattern to define a class
In JavaScript a class is basically just another function
It get’s created in an anonymous function to prevent the global scope from polluting and in this anonymous function it also get’s populated with variables and methods
This is a lot of work for just a class! Let’s convert this sample to Typescript! javascript class appController { constructor() { this.name = "Chris"; } name: string sayHello(): string { return "Hello " + this.name; } } Look at this! If you come from C# or Java or C++ even you will feel right at home! Very nice
And this just compiles to plain JavaScript, so in the browser you will just reference the same .js file as before
Now let’s make use of angular
The controller still needs to get registered in Angular
As Typescript is a superset of JavaScript we can do the Angular registration below the class definition, or in a separate typescript file
The only problem is that we will get compiler errors as the typescript compiler does not know about Angular or, other normal JavaScript libraries
To get Typescript to play nice with Angular and other JavaScript libraries you can make use of so called definition files
These are basically descriptions of a JavaScript library so that the Typescript compiler knows which functions there are, what they return and parameters they have
You can create these, there is a big tutorial on these on typescriptlang
Luckily most of them are already there and you can download them from https://github.com/borisyankov/DefinitelyTyped
When you use Visual Studio 2013 you can right click on a JavaScript file and download the files from there
Let’s update our code with an Angular definition file
I created a screenshot of my Visual Studio just to let you see how cool this is 
In the top of the file I reference the Angular definition file
Now Typescript knows of the angular variable and gives me complete intellisense about types, function etc
You can see this function returns an ng.IModule instance, on which we also get complete intellisense
Here is the complete code
Keep in mind that this is just compile time
Run time the scripts still need to be included in the right order to make sure the angular global exists before our controller registration
javascript /// <reference path="../../scripts/typings/angularjs/angular.d.ts" /> class appController { constructor() { this.name = "Chris"; } name: string sayHello(): string { return "Hello " + this.name; } } var appMpdule: ng.IModule = angular.module("appModule", []); appMpdule.controller("appController", appController); What is also cool, is that if define parameters in our constructor, they will get inject by angular
We could let inject Angular all kinds of registered services just like with normal controller functions! Let’s create a cool alert service which our controllers can use
Here is the code for our alert service
javascript class defaultAlertService implements alertService { showAlert(text: string): void { alert(text); } } interface alertService { showAlert(text: string): void; } var servicesModule: ng.IModule = angular.module("services", []); servicesModule.factory("alertService", () => new defaultAlertService()); Our service implements an interface, so we can easily switch it for a service which uses bootstrap for example
Also on line 14 you can see the angular registration
Yes Typescript has lambda expressions! The difference with a normal anonymous function is that the lambda keeps the this pointer of the parent function instead of a new scope
Now the service needs to get injected in our controller
Here is our controller with it’s new showAlert function
It get’s called when someone clicks a button
The html will follow later
javascript /// <reference path="../../scripts/typings/angularjs/angular.d.ts" /> class appController { constructor(private alertService:alertService) { this.name = "Chris"; } name: string sayHello(): string { return "Hello " + this.name; } showAlert(text: string) { this.alertService.showAlert(text); } } var appMpdule: ng.IModule = angular.module("appModule", ["services"]); appMpdule.controller("appController", appController); On line 5 you see a cool Typescript construction
For a constructor parameter that has a modifier Typescript will automatically emit a class variable
If the name of our parameter is the same as the name of the registered service Angular will just inject it
If not, when you use minimizing you have to use another inline annotation for it to work
You can also see the dependency on the services module when loading our appmodule
Keep in mind that this is not a file dependency
That is still up to us
Look in the html next, there we will need to include the scripts in the right order
Angular –> Service –> Controller
It is simple right now, but this can quickly grow and will lead it’s own life
html <html ng-app="appModule" lang="en"> <head> <meta charset="utf-8" /> <title>Typescript HTML App</title> <link rel="stylesheet" href="app.css" type="text/css" /> <script src="Scripts/angular.js"></script> <script src="Services/alertService.js"> </script> <script src="Home/js/appController.js"> </script> </head> <body ng-controller="appController as ourController"> <h1>Typescript Angular App</h1> <div id="content">{{ourController.sayHello()}}</div> <button ng-click="ourController.showAlert('Hello world!!')">Press me</button> </body> </html> You can see, starting on line 6, the script tags
They have to be in this order or the app will break
On top of that, the browser will load all these scripts, even if there are services included that the user does not need because he does not touch the functionality tat requires these services
Enter RequireJS Typescript and RequireJS Let’s take a look at RequireJS
This is a library that adds modularization to JavaScript applications
Uhm wait, modularization
Doesn’t Angular also have modules
Yes it does! But Angular has logical modules
Angular does not modularize the file loading for example, or the handles the file dependencies as you can see in our index.html
Require can work neatly together with Angular modules, keep in mind that both libraries solve different problems
You can find everything about RequireJS here: http://www.requirejs.org/docs/api.html
let’s have a look
I am going to add Require to my application
The first thing that will change is my index.html
Two things that stand out are there is only one script tag left in our html
That is the reference to Require
As Require will manage all our script loading, this is the only thing we need in our main html page
The second thing is the data-main attribute on the script tag
This tells Require where it should start with executing
Very comparable to a main function in C for example
next we have a look at the contents of our main file
javascript require.config({ paths: { 'angular': '/Scripts/Angular/angular' }, shim: { 'angular': { exports: 'angular' } } }); // Start the main app logic
require(['angular', '../Home/js/appController'], function (angular) { var elem = document.getElementsByName("html")[0]; angular.bootstrap(elem, ['appModule']) }); First in our main js file, I call require.config
We can use this function to give require some extra config stuff
Like short names for libraries
My call tells Require that whenever I want to use angular, it can be found by following the path I pass to it
On line 8 you see another cool thing
It’s a so called shim
Require loads JavaScript files and sees them as modules
But those JavaScript files ideally should know that they contain Require modules
When defining an Require module a JavaScript file should tell Require what it is exporting and on what other modules it depends
We shall have a look at that later
The problem with angular is that this is an existing JavaScript library that does not contain Require modules
Angular just populates the global scope with an angular variable
By using a shim, we tell Require that when angular is done loading, it should clean up the global angular variable and instead pass it to the function on line 17
This function get’s executed when angular is done loading, and it’s dependency, our appController is done loading
You can see that this call to require, executes and when all the modules in the parameters are done loading
If you are paying attention you are probably wondering why we don’t have to shim our appController, as I said earlier that JavaScript files should tell require what modules they are exporting
This is because I made a little modification in our appController
Here is the code javascript /// <reference path="../../scripts/typings/angularjs/angularreq.d.ts" /> /// <amd-dependency path="../../Services/alertService"/> import ServicesModule = require("../../Services/alertService"); export class appController { constructor(private alertService:ServicesModule.alertService) { this.name = "Chris"; } name: string sayHello(): string { return "Hello " + this.name; } showAlert(text: string) { this.alertService.showAlert(text); } } import angular = require('angular'); var appMpdule: ng.IModule = angular.module("appModule", ["services"]); appMpdule.controller("appController", appController); Couple of cool things here
On line two you can see a typescript statement that is unknown to us
This is an undocumented feature of typescript
This statement allows us to specify the Require dependencies of our current module
officially, the import statement should also take care of this, but this doesn’t work when I am not using the module in a statement in my code, I only use types from the module to type parameters, these won’t be there in the resulting JavaScript so Typescript decides it does not need my module
the fix is to also specify the <amd thingy.
On line 4 you can see the import statement
This makes sure I can use types from the module and the module itself in my code
On line 6 you can see an export keyword
This instructs the Typescript compiler to define a Require module that exports our controller
On line 16 you see that we need to import angular to make use of Angular
Just as with our alertService we want to make use of the angular module, so we add an import statement
This leads to compiler errors however
Because unlike our alertService, the angular library is no typescript, and the angular.d file does not tells the compiler that angular in our project get’s loaded as a Require module
So we need to write a new definition file that tells typescript that angular is loaded as a Require module, and from that module exports the angular variable
Basically we need to make Typescript aware of our Require configuration in our main.js file
Here is the .d file to do that
javascript /// <reference path="angular.d.ts" /> declare module "angular" { export = angular; } You can see the file getting referenced on line 1 or our controller and also in our alertService
Because this file get’s referenced Typescript knows of the angular module that exports the angular variable
Just as the shim configuration for require in the beginning of this post
The alertService is defined below javascript /// <reference path="../scripts/typings/angularjs/angularreq.d.ts" /> export class defaultAlertService implements alertService { showAlert(text: string): void { alert(text); } } export interface alertService { showAlert(text: string): void; } import angular = require('angular'); var servicesModule: ng.IModule = angular.module("services", []); servicesModule.factory("alertService", () => new defaultAlertService()); Nothing strange here
It is just like our controller
We use export to export the different types from this module
But there is still something strange happening
When we load our application, I get no errors at all, but it also isn’t working correctly
The problem here comes from the fact that Require loads and bootstraps angular before the DOM is ready
What we want, is a way to tell require load the modules as soon as the dom is ready
Fortunately there is a way to this
This is actually a RequireJS plugin called domReady
It is really cool
Just download the domReady.js file
And make the domReady a module dependency of your main module
Here is the modified main.js
javascript require.config({ paths: { 'angular': '/TypeScriptAngular//Scripts/Angular/angular', 'domReady': '/TypeScriptAngular//Scripts/domReady' } , shim: { 'angular': { exports: 'angular' } } }); // Start the main app logic
require(['domReady', 'angular', '../Home/js/appController'], function (domReady, angular) { domReady(function (doc) { angular.bootstrap(doc, ['appModule']); }); }); You can see that when you add domReady as a dependency, it will give you the current document as a parameter to your module function
So now we are done! Everything works, is object oriented by using typescript, is MVC’d by using angular and asynchronously loads our modules and dependencies when we need them
Typescript makes working with require a lot easier, but you do have to know how Typescript accomplishes this, and how to make use of the .d files
Here is the code by the way! See you next time! TypeScriptAngular.zipRecently I had the need to send some custom HTTP headers when a WebView component used in our app performed a GET request
In this post I will show you how this is done
When searching for this functionality you will soon discover the NavigateWithHttpRequestMessage method of the component
But the documentation says that this method only works for POST methods, not for GET
So, some experimenting was in order
Using the XAML below: <span id="lnum1" style="color: #606060;"> 1:</span> <span style="color: #0000ff;">&lt;</span><span style="color: #800000;">Page</span> 1 <span id="lnum1" style="color: #606060;"> 1:</span> <span style="color: #0000ff;">&lt;</span><span style="color: #800000;">Page</span> <span id="lnum2" style="color: #606060;"> 2:</span> <span style="color: #ff0000;">x:Class</span><span style="color: #0000ff;">="App5.MainPage"</span> 1 <span id="lnum2" style="color: #606060;"> 2:</span> <span style="color: #ff0000;">x:Class</span><span style="color: #0000ff;">="App5.MainPage"</span> <span id="lnum3" style="color: #606060;"> 3:</span> <span style="color: #ff0000;">xmlns</span><span style="color: #0000ff;">="http://schemas.microsoft.com/winfx/2006/xaml/presentation"</span> 1 <span id="lnum3" style="color: #606060;"> 3:</span> <span style="color: #ff0000;">xmlns</span><span style="color: #0000ff;">="http://schemas.microsoft.com/winfx/2006/xaml/presentation"</span> <span id="lnum4" style="color: #606060;"> 4:</span> <span style="color: #ff0000;">xmlns:x</span><span style="color: #0000ff;">="http://schemas.microsoft.com/winfx/2006/xaml"</span> 1 <span id="lnum4" style="color: #606060;"> 4:</span> <span style="color: #ff0000;">xmlns:x</span><span style="color: #0000ff;">="http://schemas.microsoft.com/winfx/2006/xaml"</span> <span id="lnum5" style="color: #606060;"> 5:</span> <span style="color: #ff0000;">xmlns:local</span><span style="color: #0000ff;">="using:App5"</span> 1 <span id="lnum5" style="color: #606060;"> 5:</span> <span style="color: #ff0000;">xmlns:local</span><span style="color: #0000ff;">="using:App5"</span> <span id="lnum6" style="color: #606060;"> 6:</span> <span style="color: #ff0000;">xmlns:d</span><span style="color: #0000ff;">="http://schemas.microsoft.com/expression/blend/2008"</span> 1 <span id="lnum6" style="color: #606060;"> 6:</span> <span style="color: #ff0000;">xmlns:d</span><span style="color: #0000ff;">="http://schemas.microsoft.com/expression/blend/2008"</span> <span id="lnum7" style="color: #606060;"> 7:</span> <span style="color: #ff0000;">xmlns:mc</span><span style="color: #0000ff;">="http://schemas.openxmlformats.org/markup-compatibility/2006"</span> 1 <span id="lnum7" style="color: #606060;"> 7:</span> <span style="color: #ff0000;">xmlns:mc</span><span style="color: #0000ff;">="http://schemas.openxmlformats.org/markup-compatibility/2006"</span> <span id="lnum8" style="color: #606060;"> 8:</span> <span style="color: #ff0000;">mc:Ignorable</span><span style="color: #0000ff;">="d"</span><span style="color: #0000ff;">&gt;</span> 1 <span id="lnum8" style="color: #606060;"> 8:</span> <span style="color: #ff0000;">mc:Ignorable</span><span style="color: #0000ff;">="d"</span><span style="color: #0000ff;">&gt;</span> <span id="lnum9" style="color: #606060;"> 9:</span> 1 <span id="lnum9" style="color: #606060;"> 9:</span> <span id="lnum10" style="color: #606060;"> 10:</span> <span style="color: #0000ff;">&lt;</span><span style="color: #800000;">Grid</span> <span style="color: #ff0000;">Background</span><span style="color: #0000ff;">="{ThemeResource ApplicationPageBackgroundThemeBrush}"</span><span style="color: #0000ff;">&gt;</span> 1 <span id="lnum10" style="color: #606060;"> 10:</span> <span style="color: #0000ff;">&lt;</span><span style="color: #800000;">Grid</span> <span style="color: #ff0000;">Background</span><span style="color: #0000ff;">="{ThemeResource ApplicationPageBackgroundThemeBrush}"</span><span style="color: #0000ff;">&gt;</span> <span id="lnum11" style="color: #606060;"> 11:</span> <span style="color: #0000ff;">&lt;</span><span style="color: #800000;">WebView</span> <span style="color: #ff0000;">Name</span><span style="color: #0000ff;">="webview"</span><span style="color: #0000ff;">&gt;&lt;/</span><span style="color: #800000;">WebView</span><span style="color: #0000ff;">&gt;</span> 1 <span id="lnum11" style="color: #606060;"> 11:</span> <span style="color: #0000ff;">&lt;</span><span style="color: #800000;">WebView</span> <span style="color: #ff0000;">Name</span><span style="color: #0000ff;">="webview"</span><span style="color: #0000ff;">&gt;&lt;/</span><span style="color: #800000;">WebView</span><span style="color: #0000ff;">&gt;</span> <span id="lnum12" style="color: #606060;"> 12:</span> <span style="color: #0000ff;">&lt;/</span><span style="color: #800000;">Grid</span><span style="color: #0000ff;">&gt;</span> 1 <span id="lnum12" style="color: #606060;"> 12:</span> <span style="color: #0000ff;">&lt;/</span><span style="color: #800000;">Grid</span><span style="color: #0000ff;">&gt;</span> <span id="lnum13" style="color: #606060;"> 13:</span> <span style="color: #0000ff;">&lt;/</span><span style="color: #800000;">Page</span><span style="color: #0000ff;">&gt;</span> 1 <span id="lnum13" style="color: #606060;"> 13:</span> <span style="color: #0000ff;">&lt;/</span><span style="color: #800000;">Page</span><span style="color: #0000ff;">&gt;</span> And the following C# code: <span id="lnum1" style="color: #606060;"> 1:</span> <span style="color: #0000ff;">using</span> Windows.Foundation; 1 <span id="lnum1" style="color: #606060;"> 1:</span> <span style="color: #0000ff;">using</span> Windows.Foundation; <span id="lnum2" style="color: #606060;"> 2:</span> <span style="color: #0000ff;">using</span> Windows.Foundation.Collections; 1 <span id="lnum2" style="color: #606060;"> 2:</span> <span style="color: #0000ff;">using</span> Windows.Foundation.Collections; <span id="lnum3" style="color: #606060;"> 3:</span> <span style="color: #0000ff;">using</span> Windows.UI.Xaml; 1 <span id="lnum3" style="color: #606060;"> 3:</span> <span style="color: #0000ff;">using</span> Windows.UI.Xaml; <span id="lnum4" style="color: #606060;"> 4:</span> <span style="color: #0000ff;">using</span> Windows.UI.Xaml.Controls; 1 <span id="lnum4" style="color: #606060;"> 4:</span> <span style="color: #0000ff;">using</span> Windows.UI.Xaml.Controls; <span id="lnum5" style="color: #606060;"> 5:</span> <span style="color: #0000ff;">using</span> Windows.UI.Xaml.Controls.Primitives; 1 <span id="lnum5" style="color: #606060;"> 5:</span> <span style="color: #0000ff;">using</span> Windows.UI.Xaml.Controls.Primitives; <span id="lnum6" style="color: #606060;"> 6:</span> <span style="color: #0000ff;">using</span> Windows.UI.Xaml.Data; 1 <span id="lnum6" style="color: #606060;"> 6:</span> <span style="color: #0000ff;">using</span> Windows.UI.Xaml.Data; <span id="lnum7" style="color: #606060;"> 7:</span> <span style="color: #0000ff;">using</span> Windows.UI.Xaml.Input; 1 <span id="lnum7" style="color: #606060;"> 7:</span> <span style="color: #0000ff;">using</span> Windows.UI.Xaml.Input; <span id="lnum8" style="color: #606060;"> 8:</span> <span style="color: #0000ff;">using</span> Windows.UI.Xaml.Media; 1 <span id="lnum8" style="color: #606060;"> 8:</span> <span style="color: #0000ff;">using</span> Windows.UI.Xaml.Media; <span id="lnum9" style="color: #606060;"> 9:</span> <span style="color: #0000ff;">using</span> Windows.UI.Xaml.Navigation; 1 <span id="lnum9" style="color: #606060;"> 9:</span> <span style="color: #0000ff;">using</span> Windows.UI.Xaml.Navigation; <span id="lnum10" style="color: #606060;"> 10:</span> <span style="color: #0000ff;">using</span> Windows.Web.Http; 1 <span id="lnum10" style="color: #606060;"> 10:</span> <span style="color: #0000ff;">using</span> Windows.Web.Http; <span id="lnum11" style="color: #606060;"> 11:</span> 1 <span id="lnum11" style="color: #606060;"> 11:</span> <span id="lnum12" style="color: #606060;"> 12:</span> <span style="color: #008000;">// The Blank Page item template is documented at http://go.microsoft.com/fwlink/?LinkId=234238</span> 1 <span id="lnum12" style="color: #606060;"> 12:</span> <span style="color: #008000;">// The Blank Page item template is documented at http://go.microsoft.com/fwlink/?LinkId=234238</span> <span id="lnum13" style="color: #606060;"> 13:</span> 1 <span id="lnum13" style="color: #606060;"> 13:</span> <span id="lnum14" style="color: #606060;"> 14:</span> <span style="color: #0000ff;">namespace</span> App5 1 <span id="lnum14" style="color: #606060;"> 14:</span> <span style="color: #0000ff;">namespace</span> App5 <span id="lnum15" style="color: #606060;"> 15:</span> { 1 <span id="lnum15" style="color: #606060;"> 15:</span> { <span id="lnum16" style="color: #606060;"> 16:</span> <span style="color: #008000;">/// &lt;summary&gt;</span> 1 <span id="lnum16" style="color: #606060;"> 16:</span> <span style="color: #008000;">/// &lt;summary&gt;</span> <span id="lnum17" style="color: #606060;"> 17:</span> <span style="color: #008000;">/// An empty page that can be used on its own or navigated to within a Frame.</span> 1 <span id="lnum17" style="color: #606060;"> 17:</span> <span style="color: #008000;">/// An empty page that can be used on its own or navigated to within a Frame.</span> <span id="lnum18" style="color: #606060;"> 18:</span> <span style="color: #008000;">/// &lt;/summary&gt;</span> 1 <span id="lnum18" style="color: #606060;"> 18:</span> <span style="color: #008000;">/// &lt;/summary&gt;</span> <span id="lnum19" style="color: #606060;"> 19:</span> <span style="color: #0000ff;">public</span> <span style="color: #0000ff;">sealed</span> <span style="color: #0000ff;">partial</span> <span style="color: #0000ff;">class</span> MainPage : Page 1 <span id="lnum19" style="color: #606060;"> 19:</span> <span style="color: #0000ff;">public</span> <span style="color: #0000ff;">sealed</span> <span style="color: #0000ff;">partial</span> <span style="color: #0000ff;">class</span> MainPage : Page <span id="lnum20" style="color: #606060;"> 20:</span> { 1 <span id="lnum20" style="color: #606060;"> 20:</span> { <span id="lnum21" style="color: #606060;"> 21:</span> <span style="color: #0000ff;">public</span> MainPage() 1 <span id="lnum21" style="color: #606060;"> 21:</span> <span style="color: #0000ff;">public</span> MainPage() <span id="lnum22" style="color: #606060;"> 22:</span> { 1 <span id="lnum22" style="color: #606060;"> 22:</span> { <span id="lnum23" style="color: #606060;"> 23:</span> <span style="color: #0000ff;">this</span>.InitializeComponent(); 1 <span id="lnum23" style="color: #606060;"> 23:</span> <span style="color: #0000ff;">this</span>.InitializeComponent(); <span id="lnum24" style="color: #606060;"> 24:</span> 1 <span id="lnum24" style="color: #606060;"> 24:</span> <span id="lnum25" style="color: #606060;"> 25:</span> HttpRequestMessage request = <span style="color: #0000ff;">new</span> HttpRequestMessage(HttpMethod.Get, <span style="color: #0000ff;">new</span> Uri(<span style="color: #006080;">"http://www.luminis.eu"</span>)); 1 <span id="lnum25" style="color: #606060;"> 25:</span> HttpRequestMessage request = <span style="color: #0000ff;">new</span> HttpRequestMessage(HttpMethod.Get, <span style="color: #0000ff;">new</span> Uri(<span style="color: #006080;">"http://www.luminis.eu"</span>)); <span id="lnum26" style="color: #606060;"> 26:</span> request.Headers.Append(<span style="color: #006080;">"x-alex"</span>, <span style="color: #006080;">"Custom header value."</span>); 1 <span id="lnum26" style="color: #606060;"> 26:</span> request.Headers.Append(<span style="color: #006080;">"x-alex"</span>, <span style="color: #006080;">"Custom header value."</span>); <span id="lnum27" style="color: #606060;"> 27:</span> 1 <span id="lnum27" style="color: #606060;"> 27:</span> <span id="lnum28" style="color: #606060;"> 28:</span> webview.NavigateWithHttpRequestMessage(request); 1 <span id="lnum28" style="color: #606060;"> 28:</span> webview.NavigateWithHttpRequestMessage(request); <span id="lnum29" style="color: #606060;"> 29:</span> } 1 <span id="lnum29" style="color: #606060;"> 29:</span> } <span id="lnum30" style="color: #606060;"> 30:</span> } 1 <span id="lnum30" style="color: #606060;"> 30:</span> } <span id="lnum31" style="color: #606060;"> 31:</span> } 1 <span id="lnum31" style="color: #606060;"> 31:</span> } You can see on lines 25-28 how the call to the NavigateWithHttpRequestMessage method is configured
I was surprised to see that this call just works, even when the documentation is stating that it shouldn’t
A quick look with fiddler: confirms that indeed a GET request is performed and that my custom header is present
The final test for me was to verify that this would also work using HTML/JavaScript: <span id="lnum1" style="color: #606060;"> 1:</span> &lt;!DOCTYPE html&gt; 1 <span id="lnum1" style="color: #606060;"> 1:</span> &lt;!DOCTYPE html&gt; <span id="lnum2" style="color: #606060;"> 2:</span> &lt;html style=<span style="color: #006080;">"height:100%;width:100%"</span>&gt; 1 <span id="lnum2" style="color: #606060;"> 2:</span> &lt;html style=<span style="color: #006080;">"height:100%;width:100%"</span>&gt; <span id="lnum3" style="color: #606060;"> 3:</span> &lt;head&gt; 1 <span id="lnum3" style="color: #606060;"> 3:</span> &lt;head&gt; <span id="lnum4" style="color: #606060;"> 4:</span> &lt;meta charset=<span style="color: #006080;">"utf-8"</span> /&gt; 1 <span id="lnum4" style="color: #606060;"> 4:</span> &lt;meta charset=<span style="color: #006080;">"utf-8"</span> /&gt; <span id="lnum5" style="color: #606060;"> 5:</span> &lt;title&gt;App4&lt;/title&gt; 1 <span id="lnum5" style="color: #606060;"> 5:</span> &lt;title&gt;App4&lt;/title&gt; <span id="lnum6" style="color: #606060;"> 6:</span> 1 <span id="lnum6" style="color: #606060;"> 6:</span> <span id="lnum7" style="color: #606060;"> 7:</span> &lt;!-- WinJS references --&gt; 1 <span id="lnum7" style="color: #606060;"> 7:</span> &lt;!-- WinJS references --&gt; <span id="lnum8" style="color: #606060;"> 8:</span> &lt;link href=<span style="color: #006080;">"//Microsoft.WinJS.2.0/css/ui-dark.css"</span> rel=<span style="color: #006080;">"stylesheet"</span> /&gt; 1 <span id="lnum8" style="color: #606060;"> 8:</span> &lt;link href=<span style="color: #006080;">"//Microsoft.WinJS.2.0/css/ui-dark.css"</span> rel=<span style="color: #006080;">"stylesheet"</span> /&gt; <span id="lnum9" style="color: #606060;"> 9:</span> &lt;script src=<span style="color: #006080;">"//Microsoft.WinJS.2.0/js/base.js"</span>&gt;&lt;/script&gt; 1 <span id="lnum9" style="color: #606060;"> 9:</span> &lt;script src=<span style="color: #006080;">"//Microsoft.WinJS.2.0/js/base.js"</span>&gt;&lt;/script&gt; <span id="lnum10" style="color: #606060;"> 10:</span> &lt;script src=<span style="color: #006080;">"//Microsoft.WinJS.2.0/js/ui.js"</span>&gt;&lt;/script&gt; 1 <span id="lnum10" style="color: #606060;"> 10:</span> &lt;script src=<span style="color: #006080;">"//Microsoft.WinJS.2.0/js/ui.js"</span>&gt;&lt;/script&gt; <span id="lnum11" style="color: #606060;"> 11:</span> 1 <span id="lnum11" style="color: #606060;"> 11:</span> <span id="lnum12" style="color: #606060;"> 12:</span> &lt;!-- App4 references --&gt; 1 <span id="lnum12" style="color: #606060;"> 12:</span> &lt;!-- App4 references --&gt; <span id="lnum13" style="color: #606060;"> 13:</span> &lt;link href=<span style="color: #006080;">"/css/default.css"</span> rel=<span style="color: #006080;">"stylesheet"</span> /&gt; 1 <span id="lnum13" style="color: #606060;"> 13:</span> &lt;link href=<span style="color: #006080;">"/css/default.css"</span> rel=<span style="color: #006080;">"stylesheet"</span> /&gt; <span id="lnum14" style="color: #606060;"> 14:</span> &lt;script src=<span style="color: #006080;">"/js/default.js"</span>&gt;&lt;/script&gt; 1 <span id="lnum14" style="color: #606060;"> 14:</span> &lt;script src=<span style="color: #006080;">"/js/default.js"</span>&gt;&lt;/script&gt; <span id="lnum15" style="color: #606060;"> 15:</span> 1 <span id="lnum15" style="color: #606060;"> 15:</span> <span id="lnum16" style="color: #606060;"> 16:</span> 1 <span id="lnum16" style="color: #606060;"> 16:</span> <span id="lnum17" style="color: #606060;"> 17:</span> &lt;/head&gt; 1 <span id="lnum17" style="color: #606060;"> 17:</span> &lt;/head&gt; <span id="lnum18" style="color: #606060;"> 18:</span> &lt;body style=<span style="color: #006080;">"height:100%;width:100%"</span>&gt; 1 <span id="lnum18" style="color: #606060;"> 18:</span> &lt;body style=<span style="color: #006080;">"height:100%;width:100%"</span>&gt; <span id="lnum19" style="color: #606060;"> 19:</span> &lt;x-ms-webview style=<span style="color: #006080;">"height:100%;width:100%"</span> id=<span style="color: #006080;">"webview"</span>&gt; 1 <span id="lnum19" style="color: #606060;"> 19:</span> &lt;x-ms-webview style=<span style="color: #006080;">"height:100%;width:100%"</span> id=<span style="color: #006080;">"webview"</span>&gt; <span id="lnum20" style="color: #606060;"> 20:</span> &lt;/x-ms-webview&gt; 1 <span id="lnum20" style="color: #606060;"> 20:</span> &lt;/x-ms-webview&gt; <span id="lnum21" style="color: #606060;"> 21:</span> &lt;script&gt; 1 <span id="lnum21" style="color: #606060;"> 21:</span> &lt;script&gt; <span id="lnum22" style="color: #606060;"> 22:</span> (<span style="color: #0000ff;">function</span> () { 1 <span id="lnum22" style="color: #606060;"> 22:</span> (<span style="color: #0000ff;">function</span> () { <span id="lnum23" style="color: #606060;"> 23:</span> <span style="color: #0000ff;">var</span> request = <span style="color: #0000ff;">new</span> Windows.Web.Http.HttpRequestMessage(Windows.Web.Http.HttpMethod.get, <span style="color: #0000ff;">new</span> Windows.Foundation.Uri(<span style="color: #006080;">"http://www.luminis.eu"</span>)); 1 <span id="lnum23" style="color: #606060;"> 23:</span> <span style="color: #0000ff;">var</span> request = <span style="color: #0000ff;">new</span> Windows.Web.Http.HttpRequestMessage(Windows.Web.Http.HttpMethod.get, <span style="color: #0000ff;">new</span> Windows.Foundation.Uri(<span style="color: #006080;">"http://www.luminis.eu"</span>)); <span id="lnum24" style="color: #606060;"> 24:</span> request.headers.append(<span style="color: #006080;">"x-alex"</span>, <span style="color: #006080;">"Custom value"</span>); 1 <span id="lnum24" style="color: #606060;"> 24:</span> request.headers.append(<span style="color: #006080;">"x-alex"</span>, <span style="color: #006080;">"Custom value"</span>); <span id="lnum25" style="color: #606060;"> 25:</span> webview.navigateWithHttpRequestMessage(request); 1 <span id="lnum25" style="color: #606060;"> 25:</span> webview.navigateWithHttpRequestMessage(request); <span id="lnum26" style="color: #606060;"> 26:</span> })(); 1 <span id="lnum26" style="color: #606060;"> 26:</span> })(); <span id="lnum27" style="color: #606060;"> 27:</span> &lt;/script&gt; 1 <span id="lnum27" style="color: #606060;"> 27:</span> &lt;/script&gt; <span id="lnum28" style="color: #606060;"> 28:</span> &lt;/body&gt; 1 <span id="lnum28" style="color: #606060;"> 28:</span> &lt;/body&gt; <span id="lnum29" style="color: #606060;"> 29:</span> 1 <span id="lnum29" style="color: #606060;"> 29:</span> <span id="lnum30" style="color: #606060;"> 30:</span> &lt;/html&gt; 1 <span id="lnum30" style="color: #606060;"> 30:</span> &lt;/html&gt; <span id="lnum31" style="color: #606060;"> 31:</span> 1 <span id="lnum31" style="color: #606060;"> 31:</span> On lines 22 –26 you can find the JavaScript performing the call to the NavigateWithHttpRequestMessage method
Another look with fiddler: confirms that this also works
Conclusion You can use the NavigateWithHttpRequestMessage method to send custom headers with both POST and GET requests
Note that you can’t set a cookie this way
By the way, one of the best posts about the new features in the WebView component in Windows 8.1 is this oneOSGi Configuration Admin command line client restored It has been lost for a long time, but i finally restored the Luminis OSGi Config Admin command line client! I can here you think
“Restored
Lost
What was this guy doing?” It’s a long story, and I don’t even know all the ugly details myself, but we (Luminis) used to host our open source projects on a separate server, with all that nice Atlassian collaboration tools like Confluence and Jira
It was a hosted environment, so we didn’t have to manage anything
Well, that’s what we thought
One of my favorite oneliners is: “the fact that the chances are low, doesn’t means it’s never happening”
How true
The server crashed and it turned out there was no backup
Oops, that hurts
Of course, i’m lazy, so the latest source was still on my laptop
But only the latest
Not a big deal, but I hate losing history
Worse: i never checked-in the documentation, because I’d written it right into Confluence; after all, that’s what a wiki is for, right
You know the feeling when your word processor crashes and makes you lose work: yes, you can recreate it, but you always end up with the nasty feeling that the original version was better written
I hate that too
It took me a few hours of digging the internet, but finally i found some traces of my docs in Google’s cache; enough to restore it quite easily
The source history is lost forever, but well, what the hack
I swore this will never happen again, so i made two promises
One: i’ll never use subversion again
I was already using Git now and then, but I never realized that having the whole repository on each system has more advantages than just speed and being able to work offline
Two: i’d better keep the documentation with the source
You can find the source and the renewed docs on bitbucket
All in one git repository
Please clone it, so I’ll have one more backup when something terrible happensWhen working with large (server side) java application, sometimes it would be nice if you could look inside, to see what thread is taking up so much cpu time, and why
Something similar to the Unix top command, but then showing all threads in one (java) application, instead of all processes in the system
When I was looking for such a monitoring application, I came accross the 2.0 version of MC4J that provides a “Thread Info” panel that displays threads together with CPU usage; exactly what I needed
Unfortunately, there is only an alpha release of this MC4J version, that is not yet perfectly stable
Moreover, the thread info panel doesn’t handle applications with large amounts of threads very well
As the source code of this version of MC4J is not (yet) publically available, this option turned out to be a dead end
To my surprise, other applications with such functionality are hard to find
There are probably enough profiling applications that can do the job, but I wanted something simple, something JMX-based, that can used also to monitor applications running in production
There is however something called JTop, which is a plugin for JConsole
It’s actually a demo for the new (since Java 6) JConsole plugin API, that does show CPU usage per thread
It’s fairly basic and only shows total CPU usage, which is not very usefull
You would expect that (after a year), somebody would have extended the demo to something more useful, but as I couldn’t find anything like that, I thought I should give it a try myself
The result is a JConsole plugin that displays the top threads, sorted by cpu usage in the last sampling period
It also displays cpu usage history, and an average over the last 10 sampling periods
To avoid ending up with an unresponsive user interface when monitoring applications with large number of threads, I took a few precautions
First of all, the plugin has it’s own refresh rate
It’s independent from the JConsole poll interval, which is 4 seconds by default
For applications with large amounts of threads, this is way too short: only retrieving all thread information can already take 4 or 5 seconds! Although you can change the JConsole poll interval with a command line option, I thought it would be more convenient to be able to change it from the monitoring panel
It’s default set to 10 seconds, which I think is reasonable in most cases
If you notice that cpu usage measurement takes too much of the application under test, just increase the sample period and see the RMI Connection thread that processes these request, sink in the list
Another precaution was not to list all threads in the table
Displaying thousands of rows in a table is quite useless in any case, and I was afraid it would seriously harm performance
Eventually, diplaying that many rows turned out to be not much of a problem; I guess I still suffer from an prejudice with respect to Swing performance… Using MX4J also showed me that in a continuously refreshing table, it’s hard to select a thread in order to see it’s stacktrace
Therefore, in this plugin, tracing a thread is “sticky”: when you click a row in the table, the stacktrace of that thread is shown immediately and is refreshed with each new sample, until you deselect it or select another thread
Even though having threads sorted by cpu usage is the logical thing to do, it’s not always convenient when you’re studying the results, as rows keeping moving with each refresh
To lock the rows to there current position, click the “fix order” button
The topmost rows (actually all rows with a non-zero cpu usage), will stay where they are
Rows that had a cpu usage of zero, but have a non-zero value in the next sampling periods, will appear just below these rows, to avoid that you oversee any thread that suddenly takes a large amount of cpu time
You can run the plugin by downloading the jar-file and passing it to JConsole with the plugin option: jconsole -pluginpath topthreads.jar
When JConsole connects, it should have a seventh tab named “top threads”
update: there’s a new version, see https://arnhem.luminis.eu/new_version_topthreads_jconsole_plugin